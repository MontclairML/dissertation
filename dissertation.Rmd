---
title             : 
shorttitle        : "Influence of Parsimony"
author: 
  - name          : "Diego Figueiras"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Dickson Hall 226"
    email         : "figueirasd1@montclair.edu"
affiliation:
  - id            : "1"
    institution   : "Montclai State University"

keywords          : "Employee turnover, machine learning"
wordcount         : "X"
bibliography      : ["articles.bib", "references.bib", "r-references.bib"]
floatsintext      : no
figurelist        : yes
tablelist         : yes
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no
csl               : "apa7.csl"
documentclass     : "apa7"
classoption       : "man"
output            : papaja::apa6_pdf
editor_options: 
  markdown: 
    wrap: sentence
params:
  title: "Influence of Parsimony and Work-related Psychological Constructs in Predicting Turnover Intention when Using Machine Learning VS Regression"
  author: 'Diego Figueiras'
always_allow_html: true
header-includes:
  - \setlength{\parskip}{0pt}
  - \newcommand{\bcenter}{\begin{center}}
  - \newcommand{\ecenter}{\end{center}}
  - \raggedbottom
#  - \pagenumbering{gobble}                                     ## removes page numbers
#  - \usepackage{sidecap}                                       ## float figures in .pdf
  - \usepackage{wrapfig}                                       ## wrap figures in .pdf
  #- \usepackage[normalem]{ulem}                                           ## strikeout
  #- \usepackage{graphicx}
#  - \graphicspath{ {./images/} }
  - \usepackage{multicol}
  - \setlength{\columnsep}{5cm}
  - \usepackage{fancyhdr}
  - \usepackage{setspace}                                      ## header/footer logo
  - \pagestyle{fancy}
  #- \setlength{\headheight}{40pt}                        ## More space between line and text
  #- \addtolength{\topmargin}{-20pt}                      ## trying to pull text up (vertically)
  #- \rhead{\includegraphics[width = .1\textwidth]{erg2.png}}                  ## header
  #- \lfoot{\includegraphics[width = .2\textwidth]{clientlogo2.png}}            ## footer
  #- \usepackage{enumitem}                                ## customized bullet symbols
  #- \usepackage{amsfonts}                                ## picking symbols for bullets
  #- \setlist[itemize,1]{label=$\bullet$}
  #- \setlist[itemize,2]{label=$\checkmark$}                   ## ams symbols
  #- \setlist[itemize,3]{label=$\circ$}              ## http://www.personal.psu.edu/dpl14/latex/symbols.pdf
  #- \setlist[itemize,4]{label=$\maltese$}

---

\thispagestyle{empty}
\newpage

\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
\pagenumbering{gobble}
\phantom{i'm a ghost}
\vskip 1.0in
\bcenter


\textbf{`r params$title`} 

\vskip 0.8in

A DISSERTATION

\vskip 0.8in

Submitted to the Faculty of  

Montclair State University in partial fulfillment

of the requirements

for the degree of Doctor of Philosophy

\vskip 0.4in

by

`r params$author`

Montclair State University

Montclair, NJ

May 2024

\ecenter

\vskip 1.0in


Dissertation Chair: Dr. Michael Bixter

\newpage

\section{}
\pagenumbering{roman}
\fancyhead[LO, LE]{INFLUENCE OF PARSIMONY}
\cfoot{}
\rhead{\thepage}

\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

\onehalfspacing 

\bcenter

MONTCLAIR STATE UNIVERSITY  

THE GRADUATE SCHOOL  

DISSERTATION APPROVAL  

\vskip 0.2in

We hereby approve the Dissertation 

\vskip 0.2in  

\textbf{Influence of Parsimony and Work-related Psychological Constructs in Predicting Turnover Intention when Using Machine Learning VS Linear Regression}  

of  

Diego Figueiras  

Candidate for the Degree:  

Doctor of Philosophy



\ecenter  


\begin{multicols}{2}  


\noindent{Graduate Programs:}
\newline
Counseling

\vskip 0.6in

\noindent{Certified by:}

\vskip 0.6in  

\linespread{1.0}\selectfont
\noindent\rule{6cm}{0.2mm}
\newline
Dr. Kenneth Sumner 
\newline
Associate Provost for Academic Affairs and
\newline
Acting Dean of the Graduate School

\vskip 0.3in

\noindent\rule{6cm}{0.2mm}
\newline
Date

\columnbreak

\noindent{Dissertation Committee:}

\vskip 0.6in

\noindent\rule{6cm}{0.2mm}
\newline
Dr. Michael Bixter
\newline
Dissertation Chair

\vskip 0.6in

\noindent\rule{6cm}{0.2mm}
\newline
Dr. Kevin Askew

\vskip 0.6in

\noindent\rule{6cm}{0.2mm}
\newline
Dr. John T. Kulas


\end{multicols}  

\newpage
\fancyhead[LO, LE]{INFLUENCE OF PARSIMONY}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

\bcenter

\phantom{i'm a ghost}

\vskip 1.6in

Copyright@2024 by Diego Figueiras. All rights reserved.

\ecenter

\newpage
\fancyhead[LO, LE]{INFLUENCE OF PARSIMONY}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

\bcenter

\textbf{Abstract}

\ecenter

This dissertation explores the ongoing debate between traditional statistical regression models and machine learning (ML) algorithms in predictive modeling, focusing on the impact of sample size and the number of variables. Study 1 investigates the relationship between sample size and predictive accuracy, proposing hypotheses regarding the advantages of ML over regression as sample size increases. Additionally, the study examines the influence of the number of variables on predictive accuracy, emphasizing the trade-off between ML and regression models. Using data from the Federal Employee Viewpoint Survey, the research aims to contribute insights into the conditions favoring each modeling approach. Study 2 shifts the focus to incremental validity, exploring whether work-related psychological constructs enhance ML models' predictive accuracy in turnover intention compared to biodata alone. The proposed hypotheses suggest that incorporating psychological constructs will improve predictive accuracy, addressing the "garbage in garbage out" concern prevalent in ML applications. The methods involve diverse datasets, including responses from federal employees and an online survey through Amazon's MTurk, with machine learning algorithms such as Gradient Boosting Trees, Random Forest, Neural Networks, and Support Vector Machines being compared to linear and logistic regressions. The dissertation seeks to advance understanding in the field, offering practical insights for researchers and practitioners navigating the dynamic landscape of predictive modeling.


\newpage
\fancyhead[LO, LE]{INFLUENCE OF PARSIMONY}
\pagenumbering{arabic}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

\tableofcontents

\section{Chapter 1 (Heading 1)}
\newpage

```{r setup, include = FALSE}
library("papaja")
r_refs("articles.bib")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

# Introduction

Employee turnover is a critical concern for organizations, as it impacts productivity, performance, and overall organizational effectiveness [@griffeth2000meta].
Accurate prediction of turnover is crucial for proactive human resource management and the implementation of effective retention strategies [@mitchell2001people; @mobley1979review].
In recent years, the application of predictive modeling techniques has gained prominence in addressing this challenge.
The debate arises as to whether regression-based models or machine learning models are more effective in predicting turnover, particularly when working with small sample sizes.

The high amount of computer power in the cloud environment nowadays and the developments in the field of machine learning are providing easy access to high-performance services.
Machine learning-supported tools are enabling companies to analyze and evaluate information in a quick and effective way [@tambe2019artificial].
We see this in the form of applications, software, and solutions that are common in business or that automate the different decision-making processes, such as programs that create and post job descriptions, application tracking systems that identify key words to place candidates in the right openings, tools for scheduling interviews with your online calendar, chatbots for screening, etc [@rkab2019recruitment].
Human resources practices are not being oblivious to these developments.
Experts in these practices are realizing the advantages of data-driven decision making [@fallucchi2020predicting].
Large amounts of human resources data can be analyzed in a short time and empirical inferences can be made, enabling experts to better understand employees and help anticipate issues and patterns [@macijauskieneartificial].
Being able to predict the best suited personnel for positioning or that will turn over is of particular interest to human resource departments and companies in general.
Making the wrong decision when giving a promotion or demotion can cause waste of time and energy, as well as compromise the perceived organizational justice and support, resulting in more turnover.
This is why personnel placement processes are some of the most pivotal in human resources [@macijauskieneartificial].

When making decisions for placement using traditional regression-based methods, there is a high probability of this being affected by clinical rather than mechanical factors that can cause biased choices from time to time [@fallucchi2020predicting].
With machine learning, on the other hand, these decisions are based on a bit more mechanical foundation than most other recommended methods, since it is based solely on the patterns the algorithm finds on the data (though there can still be bias in the development or implementation of particular algorithms, this is minimized in comparison with traditional methods) [@zou2018ai; @raji2019actionable].
Not only this, but the decisions made in the personnel placement process can be explained to the candidates with their reasons, providing them with confidence in the results and diminishing the chances of low perceived organizational justice/support and high turnover.

The aim of this dissertation is to investigate and compare the predictive capabilities of regression-based models and machine learning models in the context of turnover prediction, focusing specifically on sample sizes and number of variables.
By examining the strengths and limitations of these modeling approaches, this study seeks to shed light on which combination of methods and data characteristics offers greater accuracy and reliability in predicting turnover within resource-constrained environments. Hopefully, this will inform practices in the decision-making process when using these algorithms. 

Regression-based models, including linear regression, logistic regression, and Cox proportional hazards regression, have long been established as prominent tools in predictive modeling [@hosmer2013applied; @cox1972regression].
These models are characterized by their simplicity, interpretability, and assumption of linearity between predictors and the outcome variable (or the logit of the outcome variable in the case of binary logistic regression).
The straightforward nature of regression-based models allows for the identification of significant predictors and estimation of their individual effects, facilitating an understanding of the underlying mechanisms driving turnover [@meyer2004employee; @hom2009explaining].

Contrarily, machine learning models have garnered significant attention due to their ability to handle complex relationships and patterns in large datasets [@breiman2001random; @hastie2009elements].
Algorithms such as random forests, support vector machines, and artificial neural networks offer the potential to capture non-linear and interactive effects without having to specify a theoretical model, making them valuable tools in predictive modeling [@niculescu2005predicting; @kermany2018identifying].
Machine learning models have been increasingly applied to turnover prediction, displaying promising results in various studies [@biswas2020prediction].

While the application of machine learning models has gained momentum, their performance in the context of small sample sizes remains an open question.
The literature suggests that machine learning models may face challenges, such as overfitting, when trained on limited data [@xu2020deep; @varoquaux2018cross].
Consequently, the predictive performance of these models might be compromised when sample sizes are below a certain threshold.
Thus, it becomes imperative to evaluate whether regression-based models, with their simplicity and interpretability, outperform machine learning models when the sample size is small enough.

Additionally, the performance of machine learning models may be affected when the number of independent variables is small.
In such scenarios, these models may face challenges such as overfitting or difficulty in identifying meaningful patterns [@varoquaux2018cross; @guyon2003introduction].
On the other hand, regression-based models may offer advantages in situations where the number of independent variables is limited, as they are less prone to overfitting and can provide transparent insights into the relationships between predictors and turnover [@hosmer2013applied].

Traditional variables in the demographic and biodata domain, such as age, gender, and education, have been commonly used in turnover prediction models [@hom2012reviewing; @meyer2004employee].
However, the utilization of antecedent variables typically studied in the field of I-O psychology, such as job satisfaction, organizational commitment, and conscientiousness, may provide deeper insights into the underlying factors contributing to turnover [@lee2021overlooked; @hom2012reviewing].

The application of work-related psychological antecedent variables in machine learning-based models holds promise for improving turnover prediction accuracy.
These variables capture psychological and organizational aspects that directly impact employees' turnover intentions and behaviors [@lee2021overlooked; @meyer2004employee].
By considering these variables in predictive models, organizations can gain a more comprehensive understanding of the complex dynamics that drive turnover and develop targeted interventions to mitigate it [@griffeth2000meta; @hom2012reviewing].

Contrarily, models relying solely on demographics and biodata may overlook critical factors contributing to turnover.
While these variables provide basic demographic information, they may lack the depth and specificity necessary to capture the nuances and complexities of turnover behavior [@hom2012reviewing].
Incorporating work-related psychological antecedent variables can offer a more nuanced and accurate prediction by considering individual attitudes, perceptions, and experiences within the organizational context, as has been demonstrated before [@meyer2004employee; @lee2021overlooked].


This second study aims to address this research question by employing survey data.
By leveraging turnover intention data and a restricted set of independent variables, along with relevant predictor variables such as demographics/biodata, job characteristics, employee engagement, and other work-related psychological antecedent variables of turnover, a comparative analysis will be conducted.
The performance of regression-based models and machine learning models will be assessed using various metrics, including accuracy, precision, recall, and the area under the receiver operating characteristic curve (AUC-ROC) [@davis2006relationship].

The findings from this research will contribute to the existing literature on turnover prediction and provide valuable insights for practitioners and researchers alike.
Understanding the relative performance of regression-based models and machine learning models when dealing with small sample sizes can guide decision-making regarding the choice of modeling techniques in resource-limited scenarios.
Ultimately, this research aims to enhance our understanding of turnover prediction and inform effective retention strategies to mitigate the negative consequences of employee turnover.

## What Constitutes Machine Learning

The delineation between machine learning (ML) and statistical modeling has been extensively explored in the literature [@breiman2001statistical].
However, the distinction remains ambiguous, as noted by @collins2014external.
Breiman's seminal work on the "two cultures" serves as a key reference in this discussion [@breiman2001statistical].
Breiman highlights the contrast between theory-driven models like regression and empirical algorithms such as decision trees, artificial neural networks, support vector machines, and random forests.

## Theory-based models

Theory-based models are models that are based on theory and assumptions, such as traditional linear regression, and benefit from human intervention and subject knowledge for model specification.
The analysis in this approach starts with assuming a stochastic data model for the inside of the black box.
Usually, research that uses this approach starts by assuming that the data are generated by a particular model.
This model is used as a template for statistical analysis.
When faced with an applied problem, researchers that use this approach come up with a data model by looking at the literature developed by previous scholars or by their own theorizing, or some combination of both.
This enables them to develop a reasonably good parametric class of models for a complex mechanism devised by nature, and then parameters are estimated and conclusions are drawn.
However, these conclusions are about the model's mechanism, not about nature's mechanism, and therefore if the model is a poor emulation of nature, the conclusion could be wrong.
Breiman (2001) criticized this approach, pointing out that: "A few decades ago, the commitment to data models was such that even simple precaution such as residual analysis or goodness-of-fit tests were not used. The belief in the infallibility of the data models was almost religious. It is a strange phenomenon -- once a model is made, then it becomes truth and the conclusions from it are infallible (p. 202)." He concludes by using the following old saying: "If all a man has is a hammer, then every problem looks like a nail (p. 202)." To solve a wide range of problems, such as is the case in the social sciences with the abundance of variables, a larger set of tools is needed.
The rapidly increasing ability of computers to store and manipulate data can provide us with more varied tools.

## Empirical-based models

In the mid-1980s, with the development of neural networks and decision trees, a new community of researchers appeared focused on predictive accuracy [@cristianini2002support].
They began using these algorithms on working in complex prediction problems where it was obvious that data models were not applicable, such as speech recognition, image recognition, handwriting recognition, times series analysis, or financial market analysis.
The approach is that nature produces data in a black box whose insides are complex and partly unknowable.
The goal is not to explain the patterns in this data, but to predict them based on input; not to focus on data models, but on the characteristics of the algorithms [@breiman2001statistical].
Within psychology, this is the same approach as dustbowl empiricism [@schoenfeldt1999dust].

An effective characterization of machine learning involves its emphasis on models that autonomously glean insights from data [@mitchell1997machine].
Unlike regression, machine learning exhibits a higher degree of automation in modeling, particularly in handling nonlinear relationships and interaction terms, as exemplified by Boulesteix [@boulesteix2014machine].
Achieving this automation often entails the use of highly adaptable algorithms in machine learning, which may necessitate penalization techniques to prevent overfitting [@deo2016learning].
Some scholars depict the relationship between statistical modeling and machine learning as a continuous spectrum [@beam2018big].
Meanwhile, others categorize any approach deviating from basic regression models as machine learning, encompassing methods like penalized regression (e.g., LASSO, elastic net) or generalized additive models (GAM) [@he2009learning].
It is essential to highlight that, according to the "automatic learning from data" definition, these methods do not strictly fall under machine learning.
In this study, we refrain from classifying them as such.

## Common Terms in Machine Learning

-   Supervised learning: these are algorithms that have a dependent variable they are trying to predict.
    If the dependent variable is discrete, it would be a classifier; if it is continuous, it would be regression-based [@hastie2009elements].
    For the purposes of this study we will be focusing on this type of machine learning.

-   Unsupervised learning: these are algorithms that do not have a dependent variable that they are trying to predict, but rather look for associations and clusters among the independent variables [@hastie2009elements]

-   Cross-validation: this is a technique used to assess the performance and generalizability of a predictive model.
    It involves partitioning the dataset into multiple subsets, training the model on some of these subsets, and evaluating its performance on the remaining data.
    This process is repeated multiple times, with different subsets used for training and testing, allowing for a more robust evaluation of the model's effectiveness across various data scenarios [@douglass2020book].

-   Regularization: in machine learning, this is a technique employed to prevent overfitting and enhance the generalization performance of a predictive model by adding a penalty term to the cost function.
    This penalty discourages the model from fitting the training data too closely and helps to control the complexity of the model, preventing it from becoming too intricate and specialized to the training set [@hastie2009elements].

-   Ensemble: in machine learning, this involves combining predictions from multiple individual models to create a more robust and accurate overall prediction [@dietterich2000ensemble].
    By leveraging the diversity among the constituent models, ensemble methods aim to improve generalization performance, mitigate overfitting, and enhance predictive accuracy across a variety of scenarios.

## Data Hungriness

The concept of data hungriness refers to the sample size needed for a method to generate a prediction model with a good predictive accuracy [@van2014modern].
The data hungriness of a predictive modeling technique is defined as the minimum number of events per variable at which the optimism of the generated model is less than 0.01.
Optimism is defined as the difference between error on the sample data and the error when applying the model to another dataset.
Every machine learning model has some amount of error in its predictions.
This error usually comes from two different sources: bias and variance.
Bias is the tendency of the model to underfit, and variance is the tendency to overfit.
The relationship between these two sources of error is known as the bias-variance tradeoff, and developers of machine learning models have to find the balance between the two.

To test if this trade-off has been done in a way that minimizes error, it is a good idea to measure performance using data that the model has never seen before.
The performance of the model on this "test data" will be a more accurate predictor of the model's performance in the real world, which is the fundamental basis for cross-validation .
The model's optimism, therefore, is the difference between the training error estimated from the data used to build the model and the test error estimated by applying the model into out-of-sample data.
The sample size needed to minimize this difference is what is referred to as data hungriness.
Machine learning algorithms generally require big sample sizes to minimize this difference [@van2014modern].

## Parsimony

Parsimony is defined as the sample size and number of variables that a dataset must have in order to maximize the predictive accuracy of a model [@sanchez2018big].
A model is considered parsimonious when it both uses the least amount of variables possible (sparsity) and has good prediction accuracy.
Typically, parsimony is reported as the performance metric of models that are sparse.

# Algorithms

In this dissertation, various machine learning algorithms are described and assessed in their ability to predict dependent variables of interest in the field of I/O psychology, specifically turnover intention. 
This section provides a general overview of the underlying mechanicsms behind these algorithms.

## Decision Trees

Decision trees are a type of supervised machine learning method.
They create classification or regression models following a tree-like structure [@mahesh2020machine].
Each fork is a split in a predictor variables, and in which each end node contains a prediction for the outcome variable [@marsland2011machine].
They are used to explain how the target variable's values can be predicted based on other variables.
Nodes are split into sub-nodes based on a threshold value of a variable [@mahesh2020machine].
Decision trees are capable of handling missing values and mixed features, as well as to select variables automatically.
On the downside, their predictive power is not as high as other algorithms and they are not stable with high model variance and small variations in the data.
This could result in a large effect on the tree structure that is not meaningful.
However, understanding how they operate becomes important in order to understand other algorithms, such as random forest and gradient boosting trees [@alpaydin2020introduction; @marsland2011machine].

## Classification and Regression Trees (CART)

A Classification and Regression Tree is an umbrella term that refers to the use of either classification and/or regression trees [@marsland2011machine].
We already explained classification trees, so lets now explain regression.
Regression trees are a type of decision tree.
They are different from classification trees in that each leaf represents a numeric value, while classification trees have "true" or "false" in their leaves, or some other discrete category.
The roots and branches of regression trees are typically ranges of the IVs, while the leaves are typically the average value of the DV in those ranges.
The structure of these trees typically starts with the root being the lower threshold of the IVs, and its leaf the average DV at this range.
Then the first branch is the upper threshold, with the leaf being the average DV at that range.
The second branch is the middle threshold of the IVs, with the leaves being the average DV between the middle and the lower threshold and the average DV between the middle and the upper threshold.
Because of this structure, regression trees are better at capturing non-linear associations than simple linear regression [@mahesh2020machine].

## Random Forests (RF)

Random forest are machine learning algorithms that take an ensemble approach that provides an improvement over decision trees [@mahesh2020machine].
They are built by taking a random sample of the data and then building an ongoing series of decision trees on the subsets.
They create many decision trees (hence "forest") to improve predictive accuracy and if one or more of these smaller decision trees are not relevant, they get ignored in favor of the better ones.
In other words, they combine a group of weak learners to form a stronger learner.

The way this algorithm works is that a number of decision trees are built on bootstrapped training sets and a random sample of IVs are chosen at each step as split variables from the full set of predictions in each decision tree.
In this way, it is unlikely that all of the individual trees will be influenced by a few "noisy" predictors.
Hundreds of decision trees are built this way, which is why these algorithms can be very slow and take a lot of computational memory.
The algorithm keeps track of the outcome predicted by every decision tree and picks the most frequent one; this is called "bagging".
Variance is reduced by taking the average of the uncorrelated trees ("out of bag" samples), making the final result more reliable and less variable.
They greatly help reduce overfitting and bias because of this, which are two of the greatest limitations of regular decision trees.
Similarly, they can fill in missing data by computing the weighted averages across the hundreds of samples, in the case of continuous data, and frequencies, in the case of categorical data.
[@alpaydin2020introduction].

## Gradient Boosting Trees (GBT)

Gradient boosting trees are very similar to random forests but with the difference that the former learn sequentially instead of randomly.
The ways the individual trees are built and their results are combined is different.
Random forest builds independent decision trees and combines them in parallel, while gradient boosting trees use a method called "boosting", which combines each learner sequentially so that each new tree corrects the errors of the previous one.
A weak learner would be a decision tree with only one split, which is also called a "stump".
To evaluate how well each tree does, the algorithm uses a loss function (such as cross entropy, to name one).
In classification trees, when the DV label and predictor do not agree, the loss function is close to 1; when they are in perfect agreement, the loss function is 0.
In GBT, a series of trees are created and each of them tries to lower the loss function of the previous tree in the series.
Trees are constantly added in this way until no further enhancement can be achieved.
This makes predictions with GBT fast and memory-efficient, although they are hard to visualize and interpret.
Compared to random forest, they have a lot of model capacity that enables them to model complex relationships and decision boundaries.
As with previous decision trees methods, there is also the danger of overfitting [@mahesh2020machine].

## Bayesian Additive Regression Trees (BART)

Bayesian Additive Regression Trees are also an ensemble method that uses many decision trees as its building blocks, much like RF or GBT.
It could be said that BART tries to capture the best of RF and GBT; they construct the trees by sampling randomly, much like bagging in RF, and they also try to capture signal not yet accounted for by the current model, much like boosting in GBT.
It is considered a nonparametric function approach used to predict using regression trees.
They rely on recursive binary partitioning of predictor space into hyperrectangles.
Hyperrectangles are cubes used to classify data, since putting data in 3d space is a better way to classify mixed data.
Once this is done, the trees are summed and regularized to avoid overfitting [@chipman2010bart; @marsland2011machine].

## Neural Networks (NN)

Neural networks were built with the intention of emulating the human nervous system. The input is associated with weights, bias, activation functions, and a computed output from all of this. Much like decision trees, NNs consist of nodes and connections between the nodes, with input nodes, output nodes, and different layers of nodes in between. A neural network may contain more than one layer between input and output to handle complex problems. These layers are referred to as hidden layers [@alpaydin2020introduction]. 
	The way the algorithm works is that the input node moves to the hidden layer by being multiplied by the weight and having the bias added, each of which is associated with the input. This is done for every single value in the input (also called IV), and the results are put into the activation function formula associated to hidden layers. The activation function is what decides the predicted output, and the formula varies depending on which function is being used. Common activation functions are the sigmoid, soft plus, hyperbolic tangent, and rectified linear units (ReLu). The weights and bias come from fitting the NN to the data beforehand using a method called “backpropagation.” After scaling the portion of the activation function estimated, the resulting functions are aggregated, then multiplied by the weights associated with the output and the bias also associated with the output is later added, resulting in a function that fits the data and that is used to make predictions based on the input.
To model very complex neural networks with high predictive power, one can extend the model by adding more hidden layers, which is what we know as deep learning. Due to the rapid development of hardware and continuous research on backpropagation techniques, NNs are a well studied area in machine learning [@mahesh2020machine; @marsland2011machine; @alpaydin2020introduction]. 

## Support Vector Machines (SVM) classifiers

Support Vector Machines (SVMs) represent supervised machine learning algorithms frequently employed for classification tasks.
These algorithms operate on the principle of identifying a hyperplane that optimally segregates the dataset into two distinct classes.
The assurance of correct classification increases as data points move farther away from this hyperplane.
The pivotal data points nearest to the hyperplane, known as support vectors, play a crucial role; their removal would impact the position of the dividing hyperplane, designating them as vital components of the dataset.
The margin, denoting the distance between the hyperplane and the support vectors, is a key measure.
The objective of SVM is to select a hyperplane with the widest possible margin, enhancing the likelihood of accurately classifying new data.
Cross-validation is employed to determine this margin.
SVM finds extensive application in tasks such as text classification and image recognition, excelling in managing outliers by permitting misclassifications and overlapping classifications [@mahesh2020machine; @marsland2011machine; @alpaydin2020introduction]

# Evaluation metrics

Performance metrics evaluate a model's predictive performance and tell you how good or poor the performance of the model is.
In this study, various performance metrics will be used.
This section provides a general description of them.

## Accuracy

Accuracy is a prediction performance metric used for classification machine learning algorithms (those that predict a dependent variable that has categorical data rather than continuous).
It is perhaps the simplest metric to use and implemented, and it's defined as the number of correct predictions divided by the total number of predictions.
This ratio is usually stated as a percentage [@marsland2011machine].

## Precision

Precision is also a ratio for classification algorithms, much like accuracy, but between true positives and total amount of positives predicted.
This metric focuses on Type-I errors [@alpaydin2020introduction]; in other words, incorrectly labeling the dependent variable during prediction.
A precision score towards 1 means that the model did not miss any true positives and is able to classify well between correct and incorrect labeling of the dependent variable.
A low precision score of less than 0.5 means that the classifier has a high number of false positives, which can be an outcome of imbalance class.
This metric cannot measure the existence of Type-II error [@marsland2011machine].

## Recall/Sensitivity

Recall, also called sensitivity, focuses on Type-II errors [@alpaydin2020introduction].
It is similar to precision but with a different denominator, which would be the number of true positives plus the number of false negatives.
Recall towards 1 will mean that the model did not miss any true positives and is able to classify well between correctly and incorrectly labeling the dependent variable.
A low recall score of less than 0.5 would mean that the classifier has a high number of false negatives, which can be an outcome of imbalance class.
This metric cannot measure the existence of Type-I error [@marsland2011machine; @alpaydin2020introduction].

## F1-score

This metric combines precision and recall, since it is the harmonic mean between the two [@alpaydin2020introduction].
A high F1 score means that there is high precision and high recall.
A low F1 score has little to no meaning, since although it tells us that the precision and recall were low, it does not tell us which cases were incorrectly classified as true positives or false negatives.
However, it is still useful in deducing the performance of the model [@marsland2011machine].

## Area Under the Receiver Operating Characteristics Curve (AUC-ROC)

AUC-ROC is a useful metric to compare different machine learning classifiers.
The true positives ratio (TPR) and false positives ratio (FPR) are plotted in a graph under different thresholds.
This resulting curve is called the Receiver Operating Characteristics Curve (ROC).
The area under this curve is what we call AUC, which is equivalent to the probability that a randomly chosen true positive case is deemed to have a higher probability of being positive than negative than a randomly chosen negative case.
In other words, a high AUC means that the probability of a randomly chosen positive example is indeed positive.
This tells that the model does a good job at discriminating negatives and positives, with most true positives at one end and positives in the other [@alpaydin2020introduction; @marsland2011machine].

## Root Mean Square Error (RMSE)

The root mean square error is another standard way of measuring the accuracy of a predictive model.
It shows how far predictions fall from measured true values using Euclidean distance [@myers2013research].
In other words, it is simply the standard deviation of the residuals.
It tells you how spread out the errors are and how concentrated the data is around the line of best fit.
It indicates the absolute fit of the model to the data and the average model prediction error in units of the dependent variable.
The lower the RMSE, the better the predictive accuracy.

## Log-loss

Log-loss is another important metric in assessing the performance of machine learning algorithms, especially classifiers.
It indicates how close the prediction probability is to the true value, and the more the predicted probability is different from the actual value, the higher the log-loss [@marsland2011machine; @alpaydin2020introduction].

## Kappa Statistic

Kappa statistic, also called Cohen's kappa, is a metric used to assess the performance of a classification model.
It is the same metric to compute inter-rater agreement, but in the context of machine learning, it can be used to compare the algorithm's predictions on different classes or labels of a dependent variable [@marsland2011machine; @alpaydin2020introduction].

## Geometric mean score (G-mean)

The geometric mean is another useful metric in assessing accuracy of performance.
It is the root of the product of a class-wise sensitivity.
It tried to maximize the accuracy on each of the classes and keep them balanced at the same time.
Mathematically, it is basically the square root of the product of sensitivity and specificity [@kubat1997addressing; @barandela2003new].

# Rationale

Predictive modeling has gained significant attention in recent years across various disciplines due to its potential to make data-driven decisions.
One of the fundamental questions that researchers and practitioners encounter in predictive modeling is whether traditional statistical regression models or machine learning (ML) algorithms are more effective in producing accurate predictions.
This debate is crucial as it guides the selection of appropriate modeling techniques for specific datasets, which can vary in terms of sample size and the number of variables.
In this section, we will review the literature and rationale leading to the hypotheses for Study 1 of the current dissertation proposal.

## Sample Size and Predictive Modeling

Predictive modeling techniques, such as regression models and ML algorithms, differ in their ability to handle different sample sizes effectively.
Owing to its flexibility, ML methods are claimed to have better performance over traditional statistical modeling, and to better handle a larger number of potential predictors [@vijayakumar2019replicability].
Many of these methods have the ability to capture non-linear associations without a theoretical specification by the researcher, as has been shown by simulation studies done by @miller2016finding, in which they simulated datasets with prescribed multivariate dependencies and non-linear effects, and they found out that random forest and boosting performed better than regression in selecting variables relevant to fit the model.
Multiple linear regression models also have the inconvenience that they use assumptions when fitting the data.
Namely, that there is statistical independence of the errors, a linear relationship between dependent and independent variables, homoscedasticity of the error, and normality of the error distribution [@myers2013research].
This is refered to as inconvenient because these assumptions may not always be met, which can lead to biased results that jeopardize predictive accuracy.
Machine learning methods, on the other hand, are quite effective at handling non-linear and complex data without having to make any assumptions, even if datasets are noisy [@craninx2008artificial].
@alzate2022predictions did a study that showed evidence of classification trees outperforming logistic regression when predicting attrition in the U.S.
Marine Corps.
@froud2021relative found that machine learning outperformed regression for simulated non-linear heteroscedastic variables when predicting quality of life and academic performance of school children in Norway.
@luu2020machine found that machine learning (specifically, gradient boosting trees) outperforms logistic regression in predicting next-season NHL player injury.

It is worth noting, however, that there are cases where regression models outperform machine learning in predictive accuracy [@sanchez2018big; @sanchez2018comparison]. It has been shown that machine learning algorithms also require more data than traditional data modeling, since their data hungriness is so demanding [@van2014modern].
Furthermore, machine learning models are typically assessed in terms of discrimination performance (e.g., accuracy, area under the receiver operating characteristic [ROC] curve [AUC]), while the reliability predictions (calibration) are often not assessed (Van Calster et al., 2016).

In a preliminary systematic literature review conducted by the author of this dissertation, when looking at ML-based predictive models built to predict turnover, among other relevant dependent variables for personnel selection, only two articles out of the twenty-eight reviewed had data with sample sizes above 269,999, which is the sample size in @sanchez2018comparison's study at which machine learning algorithms outperform traditional data modeling such as regression.
Without meeting the data hungriness of machine learning algorithms, not only is the accuracy of the models jeopardized, but the outcomes might not be any better than regression, raising the question of why should the parsimony of these traditional models be sacrificed for more complex models if the results might be similar? Knowing the threshold at which regression-based methods are a better alternative is of interest when creating predictive models of turnover intention. 

In the same literature review, when it comes to parsimony, the article with the lowest events per variable (EPV) was 1.62 and the highest was 22267.42.
The median EPV was 44, which is small and more appropriate for predictive studies using regression, as stated by @sanchez2018comparison, since their research shows that machine learning algorithms perform better than regression at a EPV above 200.
Only ten out of the twenty-eight articles in this literature review had a EPV above 87.
Using machine learning algorithms with such small number of variables could compromise the bias-variance tradeoff, overfitting the data.

# Hypotheses

## Study 1: Sample Size and Number of Variables

This study is based on prior research [@sanchez2018comparison; @kirasich2018random], which suggests that ML algorithms outperform regression models consistently when the sample size is large.
In their study, they compared machine learning methods with regression methods for variable selection for two differently sized datasets, focusing on parsimony in predictor selection, which balances predictive accuracy with the sparsity of predictors.
Their findings were that for the smaller dataset (N=6,565), regression achieved the best predictive accuracy, while machine learning methods faring better only in the larger dataset (N=269,999).
However, the study's approach of using only two distinct sample sizes—one relatively small and the other significantly large—presents certain limitations. Specifically, this binary distinction may not sufficiently capture the nuances that could emerge with intermediate sample sizes. A broader range of dataset sizes might provide a more comprehensive understanding of how the performance of these methods scales with sample size. Including multiple datasets that vary incrementally in size could help delineate more precisely the thresholds at which machine learning begins to outperform traditional regression, or vice versa, thus offering deeper insights into the relative advantages of each analytical method across a spectrum of conditions.
Since there is some recognition that regression models perform well with smaller datasets, where the number of observations is limited, and that ML algorithms tend to outperform when the dataset is very large, the following hypotheses for Study 1 are proposed:

Hypothesis 1: As sample size increases, the relative advantage of ML algorithms over regression-based alogirithms in predictive accuracy of turnover intention will increase.

In addition to sample size, the number of variables in a dataset is another critical factor affecting the choice between regression models and ML algorithms.
Going back to the study done by @sanchez2018comparison, predictive accuracy decreased for machine learning when the dataset had fewer variables compared to regression models, whose predictive accuracy remained the same or improved.
Specifically, this happened when the EPV were below 200.
This leads to the proposal of Hypothesis 2.

Hypothesis 2: As the number of variables increases, the relative advantage of ML algorithms over regression-based alogirithms in predictive accuracy of turnover intention will increase.

In conclusion, Study 1 aims to investigate the interplay between sample size and the number of variables in predictive modeling by testing the hypotheses outlined above.
Through a systematic exploration of these factors, we hope to contribute to the understanding of when regression models or ML algorithms are more suitable for predictive tasks in real-world scenarios.
Both simulated and real-world data will be used to test these two hypotheses.

## Study 2: Incremental Validity

The literature review done on 28 articles that studied the predictive accuracy of machine learning models when predicting relevant variables from a job selection context, such as turnover or job satisfaction, noted that only 9% of the studies looked at independent variables that I/O psychologists typically use in predicting job performance in selection contexts. The literature has proved that there variables are relevant in predicting job performance/turnover, such as job satisfaction, commitment, conscientiousness, organizational identification, perceived organization support and justice, involvement, or engagement [@ertas2015turnover; @kim2017employee; @pitts2011so; @liss2015loving; @maertz2007effects; @smith2005achieving; @liss2015loving; @choi2011organizational; @schaufeli2017ultra; @reeve2001refining; @meyer1991three; @shamir2004single; @shore1991construct; @franzen2022developing]. None of the articles that were predicting turnover or turnover intention used any measure of any of these psychological constructs. 
The literature in the field of I/O psychology has much to contribute in using the appropriate predictors for building machine learning algorithms that want to predict job performance and turnover.
This would help the "garbage in garbage out" criterion problem that is so common with ML.

The application of work-related psychological antecedent variables in machine learning-based models holds promise for improving turnover prediction accuracy.
These variables capture psychological and organizational aspects that directly impact employees' turnover intentions and behaviors [@lee2021overlooked; @meyer2004employee]. However, when building machine learning models to predict turnover, most studies use data that could better be described as biodata. Biodata in the context of job selection refers to a comprehensive collection of an individual's personal, educational, and professional information used for employment assessment and decision-making processes. This information typically includes details such as educational qualifications, work experience, skills, achievements, and other relevant data that assists employers in evaluating a candidate's suitability for a specific position.
Assessing whether work-related psychological contructs show incremental validity over biodata when establishing criterion-related validity with turnover intention is of interest in the study of machine learning in the I/O psychology field.
The proposed hypotheses are the following:

Hypothesis 3: machine learning models trained with work-related psychological constructs will show better predictive accuracy metrics when predicting turnover intention than models trained using biodata.


# Study 1

## Methods

## Procedure and Participants

The data that will be used for this study are responses to the Federal Employee Viewpoint Survey (FEVS) publicly available at the website of the Office of Personnel Management here: https://www.opm.gov/fevs/public-data-file/.

This survey is administered annually by the Office of Personnel Management (OPM). It is administered to employees of Departments and large agencies and the small/independent agencies that accept an invitation to participate in the survey. The survey is conducted electronically, with eligible employees invited to participate by email notification. To encourage higher response rates, OPM sends follow-up or reminder emails to potential participants. More information on the OPM FEVS can be found in the official technical report they upload to their website here: https://www.opm.gov/fevs/reports/technical-reports/technical-report/technical-report/2022/2022-technical-report.pdf

The dataset consists of 105 variables and a sample size of 557,779 federal employees that took the survey in 2022. To ensure data anonymity, no identifying information was collected. Different samples will be randomly taken from this dataset to test for hypothesis 1. These will each have a sample of 100, 1,000, 10,000, 100,000.  
Additionally, to test for hypothesis 2, samples with varying number of predictors will be randomly selected from the FAVS. These will be with 40, 75, and the full 105. 


## Data


```{r fevsitems, message=FALSE, warning=FALSE}
library(tidyverse)
library(readxl)

data_fevs <- read_excel("2022_OPM_FEVS_PRDF_Codebook_r2.xlsx")
data_fevs$`RESPONSE VALUE`<-as.numeric(data_fevs$`RESPONSE VALUE`)
data_fevs<- data_fevs%>%group_by(VARIABLE, `ITEM TEXT`, `SURVEY SECTION`)%>%summarise(n=mean(`RESPONSE VALUE`))
data_fevs<-data_fevs%>%select(VARIABLE, `SURVEY SECTION`, `ITEM TEXT`)

data_fevs <- data_fevs[order(data_fevs$`SURVEY SECTION`), ]


apa_table(
  data_fevs
  , caption = "FEVS Items"
  #, note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  , longtable = TRUE
  , landscape = TRUE
  , font_size = 'tiny'
  #, escape = FALSE
)
```


All items used in the FEVS can be found in table \@ref(tab:fevsitems). The variables used in the FEVS are the following:

## Independent variables:

The 2022 OPM FEVS was conducted via the Web. The 122-item survey included 20 demographic questions and 104 items that were grouped into twelve topic headings intended to organize the instruments and facilitate respondent comprehension. Below is a summary of the questions within topics. See table \@ref(tab:fevsitems) for the actual items.

2022 OPM FEVS topic areas:
-    My Work Experience: Items 1–13 addressed employees’ personal work experiences and opinions. Response scales ranged from "Strongly Agree" to "Strongly Disagree". 

-    My Work Unit: Items 14–34 addressed employees’ opinions regarding cooperation, recruitment, quality, and performance management in their work unit. Items 14-18 and 23-34 had response scales ranging from "Strongly Agree" to "Strongly Disagree". Items 19-22 had response scales ranging from "Always" to "Never". 

-    My Organization: Items 35–44 covered agency policies and practices related to job performance, performance appraisals, workplace diversity and fairness, as well as perceptions of employees’ personal empowerment, safety, and preparedness. This section also addressed employees’ views of their agency. Response scales ranged from "Strongly Agree" to "Strongly Disagree". 

-    My Supervisor: Items 45–54 addressed employees’ perceptions of their supervisor. For instance, this section asked whether supervisors support work-life balance, provide opportunities to demonstrate leadership skills, and promote a workplace culture that supports staff development. Items 45-54 and 53-54 had a response scale that ranged from "Strongly Agree" to "Strongly Disagree". Item 52 had a response scale that ranged from "Very Good" to "Very Poor". 

-    Leadership: Items 55–64 asked about the effectiveness of the agency’s senior leaders and managers, overall, and in motivating employees, maintaining high ethical standards, communicating organizational policies, and generating respect. Items 55-58 and 60-64 had a response scale that ranged from "Strongly Agree" to "Strongly Disagree". Item 59 had a response scale from "Very Good" to "Very Poor". 

-    My Satisfaction: Items 65–70 addressed employee satisfaction with various aspects of their jobs, including pay, job training, opportunities for advancement, recognition for work well done, and the policies and practices of senior leaders. Response scales ranged from "Very Satisfied" to "Very Dissatisfied". 

-    Diversity, Equity, Inclusion, and Accessibility: Items 71–84 addressed employees perceptions of policies and practices related to diversity, equity, and inclusion in their agency and also meeting accessibility needs. Response scales ranged from "Strongly Agree" to "Strongly Disagree".


-    Employee Experience: Items 85–89 askes about employees’ experience of they experience their work and what motivates them. Response scales ranged from "Strongly Agree" to "Strongly Disagree".

-    Pandemic, Transition to the Worksite, Workplace Flexibilities: Items 90–99 addressed the continuing impact of the COVID-19 pandemic and decisions related to returning to the worksite.

-    Paid Parental Leave: Items 100–104 asked about the experiences of using the new paid parental leave benefit for employees who indicated they had used it.



## Dependent variable:

## Turnover Intention

Some experts advise against using turnover intention as a proxy for actual turnover in federal government agencies.
Analyzing age and experience as the unit of analysis revealed strong correlations between turnover intention and actual turnover.
Conversely, at the organizational level, turnover intention exhibited a negative correlation with actual turnover [@cho2012turnover].
@cohen2016does also observed similar findings, noting that the turnover intention rate is not significantly associated with actual turnover at the organizational level.
Despite this caution, organizational-level research on turnover is recommended to utilize actual turnover, while individual-level studies should employ turnover intention at the employee level.
Given that this study investigates employees' perceptions of organizational workplace environments and their relationship to turnover intention at the individual level, the use of turnover intention is deemed appropriate for the study's objectives.

Turnover intention is assessed using a single survey item: "Are you contemplating leaving your organization within the next year, and if so, why?" Response options include "No," "Yes, to take another job within the Federal Government," "Yes, to take another job outside the Federal Government," and "Yes, other." Turnover intention was recoded as a binary outcome, with "0" indicating "No" and "1" indicating "Yes, to take another job within the Federal Government," "Yes, to take another job outside the Federal Government," and "Yes, other."

## Data analysis

Several machine learning models were trained, and the predictive accuracy measures of Turnover intention were estimated for each combination of data, sample sizes, number of variables, and model.
The algorithms being used were determined by a prior literature review conducted by the author in which the most frequent and accurate algorithms used in predicting turnover were the following: Gradient Boosting Trees (GBT), Random Forest (RF), Neural Networks (NN), and Support Vector Machines (SVM).
Additionally, a logistic regression was used as a comparison.

For all of the analyses, the scikit-learn library in Python was used to train the models and optimize performance. Whenever hyperparameter tunning was being done, each configuration was assessed using a 10-fold cross-validation scheme. 

For each XGB model, the xgboost package in Python was used and hyperparameter tuning was conducted using the GridSearchCV method from the scikit-learn library. This tuning process explored a combination of three hyperparameters: max_depth, with values ranging from 1 to 4, and learning_rate, tested at three levels (0.005, 0.05, 0.5). 

Each neural network was developed and evaluated using TensorFlow's Keras API in Python. The architecture of the networks consisted of an input layer with 128 neurons, a hidden layer with 64 neurons, and an output layer with a single neuron using a sigmoid activation function. The models were compiled with the Adam optimizer and the training process was conducted over 10 epochs with a batch size of 32, ensuring that the model had multiple iterations to learn from the dataset.

For each SVM, hyperparameter tuning was executed through RandomizedSearchCV. The norm ('l1', 'l2') was used in the error term, the maximum number of iterations were (1000, 2000, 3000), and the factor to multiply the intercept were (1, 10, 100). 

For each random forest, the grid search focused on two key parameters: the number of trees in the forest, with tested values of 10, 50, 100, and 200, and the maximum depth of each tree, with values ranging from 1 to 4.

```{r}
# library(tidyverse)
# data<-read.csv("2022_OPM_FEVS_PRDF.csv")
# data<- mutate_all(data[3:104], function(x) as.numeric(as.character(x)))
# data<-na.omit(data)
# library(psych)
# fa.parallel(data)

```



# Results

## Sample Size 100,000 

```{r logitable100k, message=FALSE, warning=FALSE}
library(tidyverse)
# Load necessary packages or install if not already installed
# if (!require(knitr)) {
#     install.packages("knitr")
#     library(knitr)
# }
# library(kableExtra)
# Create a data frame with the classification report metrics
# metrics <- data.frame(
#   Class = c('0', '1', 'Accuracy', 'Macro Avg', 'Weighted Avg'),
#   Precision = c(0.71, 0.78, NA, 0.74, 0.75),
#   Recall = c(0.45, 0.91, NA, 0.68, 0.76),
#   `F1-Score` = c(0.55, 0.84, 0.76, 0.69, 0.75),
#   Support = c(11812, 24862, 36674, 36674, 36674)
# )

metrics<- read.csv("logi100k.csv")
logi100k<-data.frame(
  Algorithm = "logi100k",
  Model = "Logistic Regression",
  `Sample Size`= 100000,
  `Number of Variables` = 105,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsLOGI <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsLOGI[2:7]<- mutate_all(metricsLOGI[2:7], function(x) as.numeric(as.character(x)))
metricsLOGI <- data.frame(lapply(metricsLOGI, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metrics)[1] <- ""

# Use kable to create a nicer table
apa_table(
  metrics
  , caption = "Logistic Regression Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  #, longtable = TRUE
  , landscape = FALSE
  #, escape = FALSE
)


# kable(metrics, caption = "Logistic Regression Predictive Metrics", 
#       col.names = c('Class', 'Precision', 'Recall', 'F1-Score', 'Support'), 
#       align = 'c') %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
#                 full_width = F, font_size = 12)


```



```{r xgbtable100k, message=FALSE, warning=FALSE}


metrics<- read.csv("xgb100k.csv")
xgb100k<-data.frame(
  Algorithm = "xgb100k",
  Model = "xgboosting",
  `Sample Size`= 100000,
  `Number of Variables` = 105,
  `AUC Score` = round(metrics$AUC[1], 3)
)


# Convert numeric values to text to handle NA values
metricsXGB <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsXGB[2:7]<- mutate_all(metricsXGB[2:7], function(x) as.numeric(as.character(x)))
metricsXGB <- data.frame(lapply(metricsXGB, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsXGB)[1] <- ""

apa_table(
  metricsXGB
  , caption = "xgboosting Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  #, longtable = TRUE
  , landscape = FALSE
  #, escape = FALSE
)




```



```{r nn100k, message=FALSE, warning=FALSE}


metrics<- read.csv("nn100k.csv")
nn100k<-data.frame(
  Algorithm = "nn100k",
  Model = "Neural Network",
  `Sample Size`= 100000,
  `Number of Variables` = 105,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsNN <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsNN[2:7]<- mutate_all(metricsNN[2:7], function(x) as.numeric(as.character(x)))
metricsNN <- data.frame(lapply(metricsNN, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsNN)[1] <- ""

apa_table(
  metricsNN
  , caption = "Neural Network Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  #, longtable = TRUE
  , landscape = FALSE
  #, escape = FALSE
)



```



```{r svm100k, message=FALSE, warning=FALSE}


metrics<- read.csv("svm100k.csv")
svm100k<-data.frame(
  Algorithm = "svm100k",
  Model = "Support Vector Machines",
  `Sample Size`= 100000,
  `Number of Variables` = 105,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsSVM <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsSVM[2:7]<- mutate_all(metricsSVM[2:7], function(x) as.numeric(as.character(x)))
metricsSVM <- data.frame(lapply(metricsSVM, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsSVM)[1] <- ""

apa_table(
  metricsSVM
  , caption = "SVM Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  #, longtable = TRUE
  , landscape = FALSE
  #, escape = FALSE
)




```



```{r rf100k, message=FALSE, warning=FALSE}



metrics<- read.csv("rf100k.csv")
rf100k<-data.frame(
  Algorithm = "rf100k",
  Model = "Random Forest",
  `Sample Size`= 100000,
  `Number of Variables` = 105,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsRF <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsRF[2:7]<- mutate_all(metricsRF[2:7], function(x) as.numeric(as.character(x)))
metricsRF <- data.frame(lapply(metricsRF, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsRF)[1] <- ""

apa_table(
  metricsRF
  , caption = "Random Forest Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  #, longtable = TRUE
  , landscape = FALSE
  #, escape = FALSE
)



```

For all of these analyses, the dataset was split into a training dataset of 80,000 and a testing dataset of 20,000. All predictive performance metrics were obtained through cross-validation on the testing dataset. A logistic regression analysis was conducted to predict turnover intention based on the set of 104 FEVS predictors. Overall accuracy across categories was `r metricsLOGI$Accuracy[3]`, as can be seen in table \@ref(tab:logitable100k). The weighted average precision was `r metricsLOGI$Precision[4]`, with a recall of `r metricsLOGI$Recall[4]` and an F1 score of `r metricsLOGI$F1.score[4]`, suggesting a moderate level of prediction consistency across categories. 
The performance of a xgboosting model across categories was `r metricsXGB$Accuracy[3]`, as can be seen in table \@ref(tab:xgbtable100k). The weighted average precision was `r metricsXGB$Precision[4]`, with a recall of `r metricsXGB$Recall[4]` and an F1 score of `r metricsXGB$F1.score[4]`, suggesting moderate levels of predictive accuracy. 
The performance of a neural network model across categories was `r metricsNN$F1.score[3]`, as can be seen in table \@ref(tab:nn100k). The weighted average precision was `r metricsNN$Precision[4]`, with a recall of `r metricsNN$Recall[4]` and an F1 score of `r metricsNN$F1.score[4]`, suggesting moderate levels of predictive accuracy. 
The performance of a SVM model across categories was `r metricsSVM$F1.score[3]`, as can be seen in table \@ref(tab:svm100k). The weighted average precision was `r metricsSVM$Precision[4]`, with a recall of `r metricsSVM$Recall[4]` and an F1 score of `r metricsSVM$F1.score[4]`, suggesting moderate levels of predictive accuracy.
The performance of a random forest model across categories was `r metricsRF$F1.score[3]`, as can be seen in table \@ref(tab:rf100k). The weighted average precision was `r metricsRF$Precision[4]`, with a recall of `r metricsRF$Recall[4]` and an F1 score of `r metricsRF$F1.score[4]`, suggesting moderate levels of predictive accuracy. 

## Sample Size 10,000

```{r logitable10k, message=FALSE, warning=FALSE}


metrics<- read.csv("logi10k.csv")
logi10k<-data.frame(
  Algorithm = "logi10k",
  Model = "Logistic Regression",
  `Sample Size`= 10000,
  `Number of Variables` = 105,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsLOGI <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsLOGI[2:7]<- mutate_all(metricsLOGI[2:7], function(x) as.numeric(as.character(x)))
metricsLOGI <- data.frame(lapply(metricsLOGI, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsLOGI)[1] <- ""

apa_table(
  metricsLOGI
  , caption = "Logistic Regression Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  #, longtable = TRUE
  , landscape = FALSE
  #, escape = FALSE
)




```


```{r xgbtable10k, message=FALSE, warning=FALSE}


metrics<- read.csv("xgb10k.csv")
xgb10k<-data.frame(
  Algorithm = "xgb10k",
  Model = "xgboosting",
  `Sample Size`= 10000,
  `Number of Variables` = 105,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsXGB <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsXGB[2:7]<- mutate_all(metricsXGB[2:7], function(x) as.numeric(as.character(x)))
metricsXGB <- data.frame(lapply(metricsXGB, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsXGB)[1] <- ""

apa_table(
  metricsXGB
  , caption = "xgboosting Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  #, longtable = TRUE
  , landscape = FALSE
  #, escape = FALSE
)



```


```{r nn10k, message=FALSE, warning=FALSE}



metrics<- read.csv("nn10k.csv")
nn10k<-data.frame(
  Algorithm = "nn10k",
  Model = "Neural Network",
  `Sample Size`= 10000,
  `Number of Variables` = 105,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsNN <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsNN[2:7]<- mutate_all(metricsNN[2:7], function(x) as.numeric(as.character(x)))
metricsNN <- data.frame(lapply(metricsNN, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsNN)[1] <- ""

apa_table(
  metricsNN
  , caption = "Neural Network Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  #, longtable = TRUE
  , landscape = FALSE
  #, escape = FALSE
)



```

Note: nn10k was a bit different from nn100k

```{r svm10k, message=FALSE, warning=FALSE}


metrics<- read.csv("svm10k.csv")
svm10k<-data.frame(
  Algorithm = "svm10k",
  Model = "Support Vector Machines",
  `Sample Size`= 10000,
  `Number of Variables` = 105,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsSVM <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsSVM[2:7]<- mutate_all(metricsSVM[2:7], function(x) as.numeric(as.character(x)))
metricsSVM <- data.frame(lapply(metricsSVM, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsSVM)[1] <- ""

apa_table(
  metricsSVM
  , caption = "SVM Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  #, longtable = TRUE
  , landscape = FALSE
  #, escape = FALSE
)


```

```{r rf10k, message=FALSE, warning=FALSE}



metrics<- read.csv("rf10k.csv")
rf10k<-data.frame(
  Algorithm = "rf10k",
  Model = "Random Forest",
  `Sample Size`= 10000,
  `Number of Variables` = 105,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsRF <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsRF[2:7]<- mutate_all(metricsRF[2:7], function(x) as.numeric(as.character(x)))
metricsRF <- data.frame(lapply(metricsRF, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsRF)[1] <- ""

apa_table(
  metricsRF
  , caption = "Random Forest Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  #, longtable = TRUE
  , landscape = FALSE
  #, escape = FALSE
)



```
For all of these analyses, the dataset was split into a training dataset of 8,000 and a testing dataset of 2,000. All predictive performance metrics were obtained through cross-validation on the testing dataset. A logistic regression analysis was conducted to predict turnover intention based on the set of 104 FEVS predictors. Overall accuracy across categories was `r metricsLOGI$Accuracy[3]`, as can be seen in Table \@ref(tab:logitable10k). The weighted average precision was `r metricsLOGI$Precision[4]`, with a recall of `r metricsLOGI$Recall[4]` and an F1 score of `r metricsLOGI$F1.score[4]`, suggesting a moderate level of prediction consistency across categories. 
The performance of a xgboosting model across categories was `r metricsXGB$Accuracy[3]`, as can be seen in table \@ref(tab:xgbtable10k). The weighted average precision was `r metricsXGB$Precision[4]`, with a recall of `r metricsXGB$Recall[4]` and an F1 score of `r metricsXGB$F1.score[4]`, suggesting moderate levels of predictive accuracy. 
The performance of a neural network model across categories was `r metricsNN$F1.score[3]`, as can be seen in table \@ref(tab:nn10k). The weighted average precision was `r metricsNN$Precision[4]`, with a recall of `r metricsNN$Recall[4]` and an F1 score of `r metricsNN$F1.score[4]`, suggesting moderate levels of predictive accuracy. 
The performance of a SVM model across categories was `r metricsSVM$F1.score[3]`, as can be seen in table \@ref(tab:svm10k). The weighted average precision was `r metricsSVM$Precision[4]`, with a recall of `r metricsSVM$Recall[4]` and an F1 score of `r metricsSVM$F1.score[4]`, suggesting moderate levels of predictive accuracy.
The performance of a random forest model across categories was `r metricsRF$F1.score[3]`, as can be seen in table \@ref(tab:rf10k). The weighted average precision was `r metricsRF$Precision[4]`, with a recall of `r metricsRF$Recall[4]` and an F1 score of `r metricsRF$F1.score[4]`, suggesting moderate levels of predictive accuracy.


## Sample Size 1,000

```{r logitable1k, message=FALSE, warning=FALSE}


metrics<- read.csv("logi1k.csv")
logi1k<-data.frame(
  Algorithm = "logi1k",
  Model = "Logistic Regression",
  `Sample Size`= 1000,
  `Number of Variables` = 105,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsLOGI <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsLOGI[2:7]<- mutate_all(metricsLOGI[2:7], function(x) as.numeric(as.character(x)))
metricsLOGI <- data.frame(lapply(metricsLOGI, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsLOGI)[1] <- ""

apa_table(
  metricsLOGI
  , caption = "Logistic Regression Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  #, longtable = TRUE
  , landscape = FALSE
  #, escape = FALSE
)



```

```{r xgbtable1k, message=FALSE, warning=FALSE}



metrics<- read.csv("xgb1k.csv")
xgb1k<-data.frame(
  Algorithm = "xgb1k",
  Model = "xgboosting",
  `Sample Size`= 1000,
  `Number of Variables` = 105,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsXGB <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsXGB[2:7]<- mutate_all(metricsXGB[2:7], function(x) as.numeric(as.character(x)))
metricsXGB <- data.frame(lapply(metricsXGB, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsXGB)[1] <- ""

apa_table(
  metricsXGB
  , caption = "xgboosting Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  #, longtable = TRUE
  , landscape = FALSE
  #, escape = FALSE
)




```

```{r nn1k, message=FALSE, warning=FALSE}



metrics<- read.csv("nn1k.csv")
nn1k<-data.frame(
  Algorithm = "nn1k",
  Model = "Neural Network",
  `Sample Size`= 1000,
  `Number of Variables` = 105,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsNN <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsNN[2:7]<- mutate_all(metricsNN[2:7], function(x) as.numeric(as.character(x)))
metricsNN <- data.frame(lapply(metricsNN, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsNN)[1] <- ""

apa_table(
  metricsNN
  , caption = "Neural Network Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  #, longtable = TRUE
  , landscape = FALSE
  #, escape = FALSE
)




```

```{r svm1k, message=FALSE, warning=FALSE}


metrics<- read.csv("svm1k.csv")
svm1k<-data.frame(
  Algorithm = "svm1k",
  Model = "Support Vector Machines",
  `Sample Size`= 1000,
  `Number of Variables` = 105,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsSVM <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsSVM[2:7]<- mutate_all(metricsSVM[2:7], function(x) as.numeric(as.character(x)))
metricsSVM <- data.frame(lapply(metricsSVM, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsSVM)[1] <- ""

apa_table(
  metricsSVM
  , caption = "SVM Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  #, longtable = TRUE
  , landscape = FALSE
  #, escape = FALSE
)




```

```{r rf1k, message=FALSE, warning=FALSE}


metrics<- read.csv("rf1k.csv")
rf1k<-data.frame(
  Algorithm = "rf1k",
  Model = "Random Forest",
  `Sample Size`= 1000,
  `Number of Variables` = 105,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsNN <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsNN[2:7]<- mutate_all(metricsNN[2:7], function(x) as.numeric(as.character(x)))
metricsNN <- data.frame(lapply(metricsNN, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsRF)[1] <- ""

apa_table(
  metricsRF
  , caption = "Random Forest Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  #, longtable = TRUE
  , landscape = FALSE
  #, escape = FALSE
)



```


For all of these analyses, the dataset was split into a training dataset of 800 and a testing dataset of 200. All predictive performance metrics were obtained through cross-validation on the testing dataset. A logistic regression analysis was conducted to predict turnover intention based on the set of 104 FEVS predictors. Overall accuracy across categories was `r metricsLOGI$Accuracy[3]`, as can be seen in Table \@ref(tab:logitable1k). The weighted average precision was `r metricsLOGI$Precision[4]`, with a recall of `r metricsLOGI$Recall[4]` and an F1 score of `r metricsLOGI$F1.score[4]`, suggesting a moderate level of prediction consistency across categories. 
The performance of a xgboosting model across categories was `r metricsXGB$Accuracy[3]`, as can be seen in table \@ref(tab:xgbtable1k). The weighted average precision was `r metricsXGB$Precision[4]`, with a recall of `r metricsXGB$Recall[4]` and an F1 score of `r metricsXGB$F1.score[4]`, suggesting moderate levels of predictive accuracy. 
The performance of a neural network model across categories was `r metricsNN$F1.score[3]`, as can be seen in table \@ref(tab:nn1k). The weighted average precision was `r metricsNN$Precision[4]`, with a recall of `r metricsNN$Recall[4]` and an F1 score of `r metricsNN$F1.score[4]`, suggesting moderate levels of predictive accuracy. 
The performance of a SVM model across categories was `r metricsSVM$F1.score[3]`, as can be seen in table \@ref(tab:svm1k). The weighted average precision was `r metricsSVM$Precision[4]`, with a recall of `r metricsSVM$Recall[4]` and an F1 score of `r metricsSVM$F1.score[4]`, suggesting moderate levels of predictive accuracy.
The performance of a random forest model across categories was `r metricsRF$F1.score[3]`, as can be seen in table \@ref(tab:rf1k). The weighted average precision was `r metricsRF$Precision[4]`, with a recall of `r metricsRF$Recall[4]` and an F1 score of `r metricsRF$F1.score[4]`, suggesting moderate levels of predictive accuracy.


## Sample Size 100


```{r logitable200, message=FALSE, warning=FALSE}


metrics<- read.csv("logi100.csv")
logi100<-data.frame(
  Algorithm = "logi100",
  Model = "Logistic Regression",
  `Sample Size`= 100,
  `Number of Variables` = 105,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsLOGI <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsLOGI[2:7]<- mutate_all(metricsLOGI[2:7], function(x) as.numeric(as.character(x)))
metricsLOGI <- data.frame(lapply(metricsLOGI, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsLOGI)[1] <- ""

apa_table(
  metricsLOGI
  , caption = "Logistic Regression Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  #, longtable = TRUE
  , landscape = FALSE
  #, escape = FALSE
)




```

```{r xgbtable200, message=FALSE, warning=FALSE}


metrics<- read.csv("xgb100.csv")
xgb100<-data.frame(
  Algorithm = "xgb100",
  Model = "xgboosting",
  `Sample Size`= 100,
  `Number of Variables` = 105,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsXGB <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsXGB[2:7]<- mutate_all(metricsXGB[2:7], function(x) as.numeric(as.character(x)))
metricsXGB <- data.frame(lapply(metricsXGB, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsXGB)[1] <- ""

apa_table(
  metricsXGB
  , caption = "xgboosting Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  #, longtable = TRUE
  , landscape = FALSE
  #, escape = FALSE
)



```

```{r nn200, message=FALSE, warning=FALSE}


metrics<- read.csv("nn100.csv")
nn100<-data.frame(
  Algorithm = "nn100",
  Model = "Neural Network",
  `Sample Size`= 100,
  `Number of Variables` = 105,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsNN <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsNN[2:7]<- mutate_all(metricsNN[2:7], function(x) as.numeric(as.character(x)))
metricsNN <- data.frame(lapply(metricsNN, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsNN)[1] <- ""

apa_table(
  metricsNN
  , caption = "Neural Network Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  #, longtable = TRUE
  , landscape = FALSE
  #, escape = FALSE
)




```

```{r svm200, message=FALSE, warning=FALSE}



metrics<- read.csv("svm100.csv")
svm100<-data.frame(
  Algorithm = "svm100",
  Model = "Support Vector Machines",
  `Sample Size`= 100,
  `Number of Variables` = 105,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsSVM <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsSVM[2:7]<- mutate_all(metricsSVM[2:7], function(x) as.numeric(as.character(x)))
metricsSVM <- data.frame(lapply(metricsSVM, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsSVM)[1] <- ""

apa_table(
  metricsSVM
  , caption = "SVM Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  #, longtable = TRUE
  , landscape = FALSE
  #, escape = FALSE
)




```

```{r rf200, message=FALSE, warning=FALSE}



metrics<- read.csv("rf100.csv")
rf100<-data.frame(
  Algorithm = "rf100",
  Model = "Random Forest",
  `Sample Size`= 100,
  `Number of Variables` = 105,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsRF <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsRF[2:7]<- mutate_all(metricsRF[2:7], function(x) as.numeric(as.character(x)))
metricsRF <- data.frame(lapply(metricsRF, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsRF)[1] <- ""


apa_table(
  metricsRF
  , caption = "Random Forest Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  #, longtable = TRUE
  , landscape = FALSE
  #, escape = FALSE
)




```


For all of these analyses, the dataset was split into a training dataset of 80 and a testing dataset of 20. All predictive performance metrics were obtained through cross-validation on the testing dataset. A logistic regression analysis was conducted to predict turnover intention based on the set of 104 FEVS predictors. Overall accuracy across categories was `r metricsLOGI$Accuracy[3]`, as can be seen in Table \@ref(tab:logitable200). The weighted average precision was `r metricsLOGI$Precision[4]`, with a recall of `r metricsLOGI$Recall[4]` and an F1 score of `r metricsLOGI$F1.score[4]`, suggesting a moderate level of prediction consistency across categories. 
The performance of a xgboosting model across categories was `r metricsXGB$Accuracy[3]`, as can be seen in table \@ref(tab:xgbtable200). The weighted average precision was `r metricsXGB$Precision[4]`, with a recall of `r metricsXGB$Recall[4]` and an F1 score of `r metricsXGB$F1.score[4]`, suggesting moderate levels of predictive accuracy. 
The performance of a neural network model across categories was `r metricsNN$F1.score[3]`, as can be seen in table \@ref(tab:nn200). The weighted average precision was `r metricsNN$Precision[4]`, with a recall of `r metricsNN$Recall[4]` and an F1 score of `r metricsNN$F1.score[4]`, suggesting moderate levels of predictive accuracy. 
The performance of a SVM model across categories was `r metricsSVM$F1.score[3]`, as can be seen in table \@ref(tab:svm200). The weighted average precision was `r metricsSVM$Precision[4]`, with a recall of `r metricsSVM$Recall[4]` and an F1 score of `r metricsSVM$F1.score[4]`, suggesting moderate levels of predictive accuracy.
The performance of a random forest model across categories was `r metricsRF$F1.score[3]`, as can be seen in table \@ref(tab:rf200). The weighted average precision was `r metricsRF$Precision[4]`, with a recall of `r metricsRF$Recall[4]` and an F1 score of `r metricsRF$F1.score[4]`, suggesting moderate levels of predictive accuracy.




## Number of Variables 75

```{r logitable75, message=FALSE, warning=FALSE}


metrics<- read.csv("logi75.csv")
logi75<-data.frame(
  Algorithm = "logi75",
  Model = "Logistic Regression",
  `Sample Size`= 100000,
  `Number of Variables` = 75,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsLOGI <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsLOGI[2:7]<- mutate_all(metricsLOGI[2:7], function(x) as.numeric(as.character(x)))
metricsLOGI <- data.frame(lapply(metricsLOGI, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsLOGI)[1] <- ""

apa_table(
  metricsLOGI
  , caption = "Logistic Regression Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  #, longtable = TRUE
  , landscape = FALSE
  #, escape = FALSE
)




```

```{r xgbtable75, message=FALSE, warning=FALSE}



metrics<- read.csv("xgb75.csv")
xgb75<-data.frame(
  Algorithm = "xgb75",
  Model = "xgboosting",
  `Sample Size`= 100000,
  `Number of Variables` = 75,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsXGB <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsXGB[2:7]<- mutate_all(metricsXGB[2:7], function(x) as.numeric(as.character(x)))
metricsXGB <- data.frame(lapply(metricsXGB, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsXGB)[1] <- ""

apa_table(
  metricsXGB
  , caption = "xgboosting Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  #, longtable = TRUE
  , landscape = FALSE
  #, escape = FALSE
)



```

```{r nn75, message=FALSE, warning=FALSE}



metrics<- read.csv("nn75.csv")
nn75<-data.frame(
  Algorithm = "nn75",
  Model = "Neural Network",
  `Sample Size`= 100000,
  `Number of Variables` = 75,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsNN <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsNN[2:7]<- mutate_all(metricsNN[2:7], function(x) as.numeric(as.character(x)))
metricsNN <- data.frame(lapply(metricsNN, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsNN)[1] <- ""

apa_table(
  metricsNN
  , caption = "Neural Network Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  #, longtable = TRUE
  , landscape = FALSE
  #, escape = FALSE
)




```

```{r svm75, message=FALSE, warning=FALSE}



metrics<- read.csv("svm75.csv")
svm75<-data.frame(
  Algorithm = "svm75",
  Model = "Support Vector Machines",
  `Sample Size`= 100000,
  `Number of Variables` = 75,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsSVM <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsSVM[2:7]<- mutate_all(metricsSVM[2:7], function(x) as.numeric(as.character(x)))
metricsSVM <- data.frame(lapply(metricsSVM, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsSVM)[1] <- ""

apa_table(
  metricsSVM
  , caption = "SVM Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  #, longtable = TRUE
  , landscape = FALSE
  #, escape = FALSE
)




```

```{r rf75, message=FALSE, warning=FALSE}



metrics<- read.csv("rf75.csv")
rf75<-data.frame(
  Algorithm = "rf75",
  Model = "Random Forest",
  `Sample Size`= 100000,
  `Number of Variables` = 75,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsRF <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsRF[2:7]<- mutate_all(metricsRF[2:7], function(x) as.numeric(as.character(x)))
metricsRF <- data.frame(lapply(metricsRF, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsRF)[1] <- ""


apa_table(
  metricsRF
  , caption = "Random Forest Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  #, longtable = TRUE
  , landscape = FALSE
  #, escape = FALSE
)




```


For all of these analyses, the dataset was split into a training dataset of 80,000 and a testing dataset of 20,000. All predictive performance metrics were obtained through cross-validation on the testing dataset. A logistic regression analysis was conducted to predict turnover intention based on the set of FEVS predictors. From these predictors, 75 were randomly chosen to conduct the analyses. Overall accuracy across categories was `r metricsLOGI$Accuracy[3]`, as can be seen in Table \@ref(tab:logitable75). The weighted average precision was `r metricsLOGI$Precision[4]`, with a recall of `r metricsLOGI$Recall[4]` and an F1 score of `r metricsLOGI$F1.score[4]`, suggesting a moderate level of prediction consistency across categories. 
The performance of a xgboosting model across categories was `r metricsXGB$Accuracy[3]`, as can be seen in table \@ref(tab:xgbtable75). The weighted average precision was `r metricsXGB$Precision[4]`, with a recall of `r metricsXGB$Recall[4]` and an F1 score of `r metricsXGB$F1.score[4]`, suggesting moderate levels of predictive accuracy. 
The performance of a neural network model across categories was `r metricsNN$F1.score[3]`, as can be seen in table \@ref(tab:nn75). The weighted average precision was `r metricsNN$Precision[4]`, with a recall of `r metricsNN$Recall[4]` and an F1 score of `r metricsNN$F1.score[4]`, suggesting moderate levels of predictive accuracy. 
The performance of a SVM model across categories was `r metricsSVM$F1.score[3]`, as can be seen in table \@ref(tab:svm75). The weighted average precision was `r metricsSVM$Precision[4]`, with a recall of `r metricsSVM$Recall[4]` and an F1 score of `r metricsSVM$F1.score[4]`, suggesting moderate levels of predictive accuracy.
The performance of a random forest model across categories was `r metricsRF$F1.score[3]`, as can be seen in table \@ref(tab:rf75). The weighted average precision was `r metricsRF$Precision[4]`, with a recall of `r metricsRF$Recall[4]` and an F1 score of `r metricsRF$F1.score[4]`, suggesting moderate levels of predictive accuracy.




## Number of Variables 15

```{r logitable40, message=FALSE, warning=FALSE}


metrics<- read.csv("logi40.csv")
logi40<-data.frame(
  Algorithm = "logi15",
  Model = "Logistic Regression",
  `Sample Size`= 100000,
  `Number of Variables` = 15,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsLOGI <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsLOGI[2:7]<- mutate_all(metricsLOGI[2:7], function(x) as.numeric(as.character(x)))
metricsLOGI <- data.frame(lapply(metricsLOGI, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsLOGI)[1] <- ""

apa_table(
  metricsLOGI
  , caption = "Logistic Regression Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  #, longtable = TRUE
  , landscape = FALSE
  #, escape = FALSE
)




```

```{r xgbtable40, message=FALSE, warning=FALSE}


metrics<- read.csv("xgb40.csv")
xgb40<-data.frame(
  Algorithm = "xgb15",
  Model = "xgboosting",
  `Sample Size`= 100000,
  `Number of Variables` = 15,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsXGB <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsXGB[2:7]<- mutate_all(metricsXGB[2:7], function(x) as.numeric(as.character(x)))
metricsXGB <- data.frame(lapply(metricsXGB, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsXGB)[1] <- ""

apa_table(
  metricsXGB
  , caption = "xgboosting Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  #, longtable = TRUE
  , landscape = FALSE
  #, escape = FALSE
)




```

```{r nn40, message=FALSE, warning=FALSE}



metrics<- read.csv("nn40.csv")
nn40<-data.frame(
  Algorithm = "nn15",
  Model = "Neural Network",
  `Sample Size`= 100000,
  `Number of Variables` = 15,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsNN <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsNN[2:7]<- mutate_all(metricsNN[2:7], function(x) as.numeric(as.character(x)))
metricsNN <- data.frame(lapply(metricsNN, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsNN)[1] <- ""

apa_table(
  metricsNN
  , caption = "Neural Network Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  #, longtable = TRUE
  , landscape = FALSE
  #, escape = FALSE
)



```

```{r svm40, message=FALSE, warning=FALSE}



metrics<- read.csv("svm40.csv")
svm40<-data.frame(
  Algorithm = "svm15",
  Model = "Support Vector Machines",
  `Sample Size`= 100000,
  `Number of Variables` = 15,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsSVM <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsSVM[2:7]<- mutate_all(metricsSVM[2:7], function(x) as.numeric(as.character(x)))
metricsSVM <- data.frame(lapply(metricsSVM, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsSVM)[1] <- ""

apa_table(
  metricsSVM
  , caption = "SVM Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  #, longtable = TRUE
  , landscape = FALSE
  #, escape = FALSE
)



```

```{r rf40, message=FALSE, warning=FALSE}


metrics<- read.csv("rf40.csv")
rf40<-data.frame(
  Algorithm = "rf15",
  Model = "Random Forest",
  `Sample Size`= 100000,
  `Number of Variables` = 15,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsRF <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsRF[2:7]<- mutate_all(metricsRF[2:7], function(x) as.numeric(as.character(x)))
metricsRF <- data.frame(lapply(metricsRF, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsRF)[1] <- ""


apa_table(
  metricsRF
  , caption = "Random Forest Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  #, longtable = TRUE
  , landscape = FALSE
  #, escape = FALSE
)




```



For all of these analyses, the dataset was split into a training dataset of 80,000 and a testing dataset of 20,000. All predictive performance metrics were obtained through cross-validation on the testing dataset. A logistic regression analysis was conducted to predict turnover intention based on the set of FEVS predictors. From these predictors, 40 were randomly chosen to conduct the analyses. Overall accuracy across categories was `r metricsLOGI$Accuracy[3]`, as can be seen in Table \@ref(tab:logitable40). The weighted average precision was `r metricsLOGI$Precision[4]`, with a recall of `r metricsLOGI$Recall[4]` and an F1 score of `r metricsLOGI$F1.score[4]`, suggesting a moderate level of prediction consistency across categories. 
The performance of a xgboosting model across categories was `r metricsXGB$Accuracy[3]`, as can be seen in table \@ref(tab:xgbtable40). The weighted average precision was `r metricsXGB$Precision[4]`, with a recall of `r metricsXGB$Recall[4]` and an F1 score of `r metricsXGB$F1.score[4]`, suggesting moderate levels of predictive accuracy. 
The performance of a neural network model across categories was `r metricsNN$F1.score[3]`, as can be seen in table \@ref(tab:nn40). The weighted average precision was `r metricsNN$Precision[4]`, with a recall of `r metricsNN$Recall[4]` and an F1 score of `r metricsNN$F1.score[4]`, suggesting moderate levels of predictive accuracy. 
The performance of a SVM model across categories was `r metricsSVM$F1.score[3]`, as can be seen in table \@ref(tab:svm40). The weighted average precision was `r metricsSVM$Precision[4]`, with a recall of `r metricsSVM$Recall[4]` and an F1 score of `r metricsSVM$F1.score[4]`, suggesting moderate levels of predictive accuracy.
The performance of a random forest model across categories was `r metricsRF$F1.score[3]`, as can be seen in table \@ref(tab:rf40). The weighted average precision was `r metricsRF$Precision[4]`, with a recall of `r metricsRF$Recall[4]` and an F1 score of `r metricsRF$F1.score[4]`, suggesting moderate levels of predictive accuracy.


## Summary Results

```{r summarytable}
options(scipen = 999)


############### 75_10k ######################################
metrics<- read.csv("logi75_10k.csv")
logi75_10k<-data.frame(
  Algorithm = "logi75_10k",
  Model = "Logistic Regression",
  `Sample Size`= 10000,
  `Number of Variables` = 75,
  `AUC Score` = round(metrics$AUC[1], 3)
)


metrics<- read.csv("xgb75_10k.csv")
xgb75_10k<-data.frame(
  Algorithm = "xgb75_10k",
  Model = "xgboosting",
  `Sample Size`= 10000,
  `Number of Variables` = 75,
  `AUC Score` = round(metrics$AUC[1], 3)
)


metrics<- read.csv("nn75_10k.csv")
nn75_10k<-data.frame(
  Algorithm = "nn75_10k",
  Model = "Neural Network",
  `Sample Size`= 10000,
  `Number of Variables` = 75,
  `AUC Score` = round(metrics$AUC[1], 3)
)


metrics<- read.csv("svm75_10k.csv")
svm75_10k<-data.frame(
  Algorithm = "svm75_10k",
  Model = "Support Vector Machines",
  `Sample Size`= 10000,
  `Number of Variables` = 75,
  `AUC Score` = round(metrics$AUC[1], 3)
)


metrics<- read.csv("rf75_10k.csv")
rf75_10k<-data.frame(
  Algorithm = "rf75_10k",
  Model = "Random Forest",
  `Sample Size`= 10000,
  `Number of Variables` = 75,
  `AUC Score` = round(metrics$AUC[1], 3)
)

############### 75_1k ######################################
metrics<- read.csv("logi75_1k.csv")
logi75_1k<-data.frame(
  Algorithm = "logi75_1k",
  Model = "Logistic Regression",
  `Sample Size`= 1000,
  `Number of Variables` = 75,
  `AUC Score` = round(metrics$AUC[1], 3)
)


metrics<- read.csv("xgb75_1k.csv")
xgb75_1k<-data.frame(
  Algorithm = "xgb75_1k",
  Model = "xgboosting",
  `Sample Size`= 1000,
  `Number of Variables` = 75,
  `AUC Score` = round(metrics$AUC[1], 3)
)


metrics<- read.csv("nn75_1k.csv")
nn75_1k<-data.frame(
  Algorithm = "nn75_1k",
  Model = "Neural Network",
  `Sample Size`= 1000,
  `Number of Variables` = 75,
  `AUC Score` = round(metrics$AUC[1], 3)
)


metrics<- read.csv("svm75_1k.csv")
svm75_1k<-data.frame(
  Algorithm = "svm75_1k",
  Model = "Support Vector Machines",
  `Sample Size`= 1000,
  `Number of Variables` = 75,
  `AUC Score` = round(metrics$AUC[1], 3)
)


metrics<- read.csv("rf75_1k.csv")
rf75_1k<-data.frame(
  Algorithm = "rf75_1k",
  Model = "Random Forest",
  `Sample Size`= 1000,
  `Number of Variables` = 75,
  `AUC Score` = round(metrics$AUC[1], 3)
)


############### 75_100 ######################################
metrics<- read.csv("logi75_100.csv")
logi75_100<-data.frame(
  Algorithm = "logi75_100",
  Model = "Logistic Regression",
  `Sample Size`= 100,
  `Number of Variables` = 75,
  `AUC Score` = round(metrics$AUC[1], 3)
)


metrics<- read.csv("xgb75_100.csv")
xgb75_100<-data.frame(
  Algorithm = "xgb75_100",
  Model = "xgboosting",
  `Sample Size`= 100,
  `Number of Variables` = 75,
  `AUC Score` = round(metrics$AUC[1], 3)
)


metrics<- read.csv("nn75_100.csv")
nn75_100<-data.frame(
  Algorithm = "nn75_100",
  Model = "Neural Network",
  `Sample Size`= 100,
  `Number of Variables` = 75,
  `AUC Score` = round(metrics$AUC[1], 3)
)


metrics<- read.csv("svm75_100.csv")
svm75_100<-data.frame(
  Algorithm = "svm75_100",
  Model = "Support Vector Machines",
  `Sample Size`= 100,
  `Number of Variables` = 75,
  `AUC Score` = round(metrics$AUC[1], 3)
)


metrics<- read.csv("rf75_100.csv")
rf75_100<-data.frame(
  Algorithm = "rf75_100",
  Model = "Random Forest",
  `Sample Size`= 100,
  `Number of Variables` = 75,
  `AUC Score` = round(metrics$AUC[1], 3)
)


############### 15_10k ######################################
metrics<- read.csv("logi15_10k.csv")
logi15_10k<-data.frame(
  Algorithm = "logi15_10k",
  Model = "Logistic Regression",
  `Sample Size`= 10000,
  `Number of Variables` = 15,
  `AUC Score` = round(metrics$AUC[1], 3)
)


metrics<- read.csv("xgb15_10k.csv")
xgb15_10k<-data.frame(
  Algorithm = "xgb15_10k",
  Model = "xgboosting",
  `Sample Size`= 10000,
  `Number of Variables` = 15,
  `AUC Score` = round(metrics$AUC[1], 3)
)


metrics<- read.csv("nn15_10k.csv")
nn15_10k<-data.frame(
  Algorithm = "nn15_10k",
  Model = "Neural Network",
  `Sample Size`= 10000,
  `Number of Variables` = 15,
  `AUC Score` = round(metrics$AUC[1], 3)
)


metrics<- read.csv("svm15_10k.csv")
svm15_10k<-data.frame(
  Algorithm = "svm15_10k",
  Model = "Support Vector Machines",
  `Sample Size`= 10000,
  `Number of Variables` = 15,
  `AUC Score` = round(metrics$AUC[1], 3)
)


metrics<- read.csv("rf15_10k.csv")
rf15_10k<-data.frame(
  Algorithm = "rf15_10k",
  Model = "Random Forest",
  `Sample Size`= 10000,
  `Number of Variables` = 15,
  `AUC Score` = round(metrics$AUC[1], 3)
)

############### 15_1k ######################################
metrics<- read.csv("logi15_1k.csv")
logi15_1k<-data.frame(
  Algorithm = "logi15_1k",
  Model = "Logistic Regression",
  `Sample Size`= 1000,
  `Number of Variables` = 15,
  `AUC Score` = round(metrics$AUC[1], 3)
)


metrics<- read.csv("xgb15_1k.csv")
xgb15_1k<-data.frame(
  Algorithm = "xgb15_1k",
  Model = "xgboosting",
  `Sample Size`= 1000,
  `Number of Variables` = 15,
  `AUC Score` = round(metrics$AUC[1], 3)
)


metrics<- read.csv("nn15_1k.csv")
nn15_1k<-data.frame(
  Algorithm = "nn15_1k",
  Model = "Neural Network",
  `Sample Size`= 1000,
  `Number of Variables` = 15,
  `AUC Score` = round(metrics$AUC[1], 3)
)


metrics<- read.csv("svm15_1k.csv")
svm15_1k<-data.frame(
  Algorithm = "svm15_1k",
  Model = "Support Vector Machines",
  `Sample Size`= 1000,
  `Number of Variables` = 15,
  `AUC Score` = round(metrics$AUC[1], 3)
)


metrics<- read.csv("rf15_1k.csv")
rf15_1k<-data.frame(
  Algorithm = "rf15_1k",
  Model = "Random Forest",
  `Sample Size`= 1000,
  `Number of Variables` = 15,
  `AUC Score` = round(metrics$AUC[1], 3)
)


############### 15_100 ######################################
metrics<- read.csv("logi15_100.csv")
logi15_100<-data.frame(
  Algorithm = "logi15_100",
  Model = "Logistic Regression",
  `Sample Size`= 100,
  `Number of Variables` = 15,
  `AUC Score` = round(metrics$AUC[1], 3)
)


metrics<- read.csv("xgb15_100.csv")
xgb15_100<-data.frame(
  Algorithm = "xgb15_100",
  Model = "xgboosting",
  `Sample Size`= 100,
  `Number of Variables` = 15,
  `AUC Score` = round(metrics$AUC[1], 3)
)


metrics<- read.csv("nn15_100.csv")
nn15_100<-data.frame(
  Algorithm = "nn15_100",
  Model = "Neural Network",
  `Sample Size`= 100,
  `Number of Variables` = 15,
  `AUC Score` = round(metrics$AUC[1], 3)
)


metrics<- read.csv("svm15_100.csv")
svm15_100<-data.frame(
  Algorithm = "svm15_100",
  Model = "Support Vector Machines",
  `Sample Size`= 100,
  `Number of Variables` = 15,
  `AUC Score` = round(metrics$AUC[1], 3)
)


metrics<- read.csv("rf15_100.csv")
rf15_100<-data.frame(
  Algorithm = "rf15_100",
  Model = "Random Forest",
  `Sample Size`= 100,
  `Number of Variables` = 15,
  `AUC Score` = round(metrics$AUC[1], 3)
)

data_sum<- rbind(
  logi100k,
  logi10k,
  logi1k,
  logi100,
  logi75,
  logi75_10k,
  logi75_1k,
  logi75_100,
  logi40,
  logi15_10k,
  logi15_1k,
  logi15_100,
  xgb100k,
  xgb10k,
  xgb1k,
  xgb100,
  xgb75,
  xgb75_10k,
  xgb75_1k,
  xgb75_100,
  xgb40,
  xgb15_10k,
  xgb15_1k,
  xgb15_100,
  nn100k, 
  nn10k,
  nn1k,
  nn100,
  nn75,
  nn75_10k,
  nn75_1k,
  nn75_100,
  nn40,
  nn15_10k,
  nn15_1k,
  nn15_100,
  svm100k,
  svm10k,
  svm1k,
  svm100,
  svm75,
  svm75_10k,
  svm75_1k,
  svm75_100,
  svm40,
  svm15_10k,
  svm15_1k,
  svm15_100,
  rf100k, 
  rf10k,
  rf1k,
  rf100, 
  rf75,
  rf75_10k,
  rf75_1k,
  rf75_100,
  rf40,
  rf15_10k,
  rf15_1k,
  rf15_100
  
)

data_sum <- data_sum[order(data_sum$AUC.Score, decreasing = TRUE), ]
row.names(data_sum) <- NULL

#write.csv(data_sum, "summary_table.csv")

apa_table(
  data_sum
  , caption = "AUC scores for all models"
  #, note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  , longtable = TRUE
  , landscape = FALSE
  #, escape = FALSE
)


```

The AUC score for all combination of models, sample sizes and number of variables has been summarized in table \@ref(tab:summarytable). For the models with datasets of 75 and 15 variables, analysis under samples sizes of 100,000, 10,000, 1,000 and 100 were done very much the same as with the datasets that had 105 variables. The same pattern of predictive performance was observed.  

When sorting by sample size, ML models with 100,000 respondents did show slightly different AUC scores than logistic regression, with ML models having an AUC score 0.01 higher when the number of variables was 105 and 0.01 when the number of variables was 75. When the number of variables was 15 and the sample size 100,000, there was a small difference in performance, with ML algorithms giving an AUC score of 0.79 while logistic regression was 0.76. 

Performance does improve for ML algorithms compared to logistic regression as number of variables decreases, although these differences are small, with an average delta of 0.02. 

Overall, the best performing algorithm is an xgboosting decision tree with a sample size of 100 and 105 number of variables, with an AUC score of 0.98. The next 12 best performing models were also ML-based. The 13th best performing model was a logistic regression, with an AUC of 0.88 and a delta of 0.1 when compared to ML models. 


# Study 2

## Methods

## Procedure and Participants

A Qualtrics survey including  were sent out via Amazon's MTurk. Participants ere sourced to create a sample that closely mirrors the diverse working population across various industries. Quality assurance measures, including attention check items and tracking response times, were used. The Cloud Research platform was used to help ensure the quality of the data. The following MTurk and Cloud Research inclusion criteria were used for the online survey: approved 95% HIT (task) rate on MTurk, 100+ HITs approved, geographical locations restricted to the United States, Cloud Research approved participants, duplicate IP addresses blocked, and suspicious geocodes blocked. Additionally, participants were compensated for their participation with $1.50 per assessment. The sample size was 241 after exclusions. 

## Data

All survey items can be found in Appendix 2.

## Independent Variables

## Work-related psychological constructs

The following publicly available instruments were added as independent variables: the 3-items Utrecht Work Engagement Scale (UWES-3) [@schaufeli2017ultra] and the refined 9-items Lodahl and Kejner's Job Involvement Scale (JI) [@reeve2001refining].  The affective (ACS) and normative (NCS) components of the revised Three-Component Model (TCM) Employee Commitment Survey  [@meyer1991three], will be used to measure commitment, and the 1-item Graphic Scale of Organizational Identification (GSOI) [@shamir2004single] will be used to measure organizational identification. The 8-item Survey of Perceived Organizational Support (SPOS) [@shore1991construct] will be used to measure organizational support. Conscientiousness will be measured using the 6-items Big Five Inventory – Short Form (BFI-2-S) [@franzen2022developing]. All of these instruments are available in Appendix 2.       

## Biodata

Biodata, or biographical data, is a method commonly used in industrial and organizational (I/O) psychology to predict future job performance and turnover based on past life experiences and behaviors. Here are several articles that have discussed or utilized biodata in I/O academic research:

- Biodata has been effectively used to predict job performance across various roles, demonstrating significant predictive validity [@mumford1992developmental].
- In their comprehensive review, @reilly1982validity highlighted the utility of biodata for personnel selection and its predictive effectiveness regarding job performance and turnover.
- @robertson2001personnel integrated biodata variables into their research on employee selection, finding correlations with job success.
- @owens1969personnel discussed the applications of biodata in understanding career progression and job performance, suggesting its effectiveness in capturing relevant life experiences that translate into work settings.
- Biodata's role in predicting managerial potential was explored by @judge2002personality, who validated its use in forecasting leadership effectiveness.
- @mount2000incremental incorporated biodata in their research on predicting job performance and retention, further supporting its relevance in I/O psychology. 

The biodata inventory consists of a series of structured biographical items carefully designed to capture a broad spectrum of life experiences, educational background, and behaviors that are empirically linked to work outcomes. As can be seen in Appendix 2, the inventory includes items such as educational and job-related experiences (BIO1, BIO10), training and skills (BIO2), historical performance indicators (BIO3, BIO5), job engagement metrics (BIO6), involvement in organized activities (BIO7), and recognition for achievements (BIO8). Additionally, items assessing general satisfaction with pay, benefits, and job location (BIO11-BIO13) are included to gauge overall job satisfaction. The response format for these items varied, with most employing a Likert scale ranging from 1 to 5, and others requiring specific factual responses (e.g., GPA in BIO9).

## Dependent Variables

## Turnover Intention

To understand why these work-related psychological constructs are being used it is important to understand the constructs that influence an employee's intention to leave their organization. It is crucial for developing a effective turnover intention model. This section outlines a theoretical model of turnover intention, emphasizing the role of specific work-related psychological constructs and attitudes. These include job involvement, commitment, perceived organizational support, employee engagement, and job identification, each of which has been shown to significantly impact an employee’s likelihood to remain with their employer.
Job involvement refers to the degree to which an individual psychologically identifies with their job and perceives their job performance as integral to their self-worth [@kanungo1982measurement]. Employees with high job involvement are less likely to exhibit turnover intentions as their jobs constitute a vital part of their self-identity [@tm1965definition]. Organizational commitment is characterized by an employee's emotional attachment to, identification with, and involvement in the organization [@meyer1991three]. Affective commitment, where employees feel an emotional attachment to their organization, is inversely related to turnover intentions. Employees who are emotionally invested are less likely to leave [@meyer1991three]. Perceived organizational support reflects the extent to which employees believe that their organization values their contributions and cares about their well-being [@eisenberger1986perceived]. Employees who perceive a high level of organizational support feel an obligation to reciprocate and are less likely to consider leaving [@rhoades2002perceived]. Employee engagement is a robust indicator of an employee's emotional and cognitive connection to their work, characterized by vigor, dedication, and absorption [@schaufeli2002measurement]. Engaged employees are more likely to stay with the organization as they experience positive emotions towards their work and a sense of challenge and significance in their roles [@schaufeli2004job]. Job identification describes the alignment of job roles with the employee’s self-concept [@ashforth1989social]. When employees identify strongly with their job roles, they are more committed to their responsibilities and less likely to leave [@van2001identification].


To measured turnover intention a one item Turnover Intention (TI) scale was used, which asks the following: “Are you contemplating leaving your organization within the next year?” 

## Job Satisfaction 

To measure job satisfaction the Michigan Organizational Assessment Questionnaire (MOAQ) short form was used. This is a streamlined version of the original instrument, designed to efficiently assess aspects of job satisfaction and organizational climate. This condensed form retains the core elements of the original MOAQ but reduces the number of items to facilitate quicker administration and analysis while maintaining robust psychometric properties [@lawler1979michigan]. The short form typically includes items that measure three key dimensions: job satisfaction, organizational commitment, and perceived organizational support. Each item is rated on a Likert scale, allowing respondents to express varying degrees of agreement or disagreement, providing a quantitative measure of an employee's satisfaction and engagement with their organization [@lawler1979michigan; @seashore1983assessing] .

## Data Analysis  
Several machine learning algorithms were used to train a series of models using biodata features only. These models include GBT, RF, NN, and SVM. A multiple  and logistic regressions were compared to these machine learning models. A second series of models that include work-related psychological constructs only in the data were trained. Performance metrics of each model were evaluated using accuracy, precision, recall, F1-scores, and AUC-ROC. Deltas between the AUC score of each set of models before and after adding work-related psychological constructs were computed to see whether there was a change in performance. This was done to test for hypothesis 3.  


## Data Description

```{r study2descr, message=FALSE, warning=FALSE}
data_descr_study1<-read.csv("descriptives_study2.csv")

apa_table(
  data_descr_study1
  , caption = "Descriptive Statistics"
  #, note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  #, longtable = TRUE
  , landscape = FALSE
  #, escape = FALSE
)

```

The dataset used in this study consists of 241 observations, each represented by 22 variables. These variables encompass a mixture of continuous and categorical data types. Turnover Intention is the target variable, measured as a binary indicator (0 or 1). Job satisfaction, measures by the MOAQ, is also a target variable, and unlike Turnover Intention it is a continuous Likert-scale ranging from "Strongly agree" to "Strongly disagree". The work-related psychological constructs predictor variables were also continuous Likert-scales ranging from "Strongly agree" to "Strongly disagree". Biodata variables were continuous and varied in scale. Summary descriptive statistics for each variable in the dataset can be found in table \@ref(tab:study2descr). 

# Results

## Predicting Turnover Intention

```{r biotable, message=FALSE, warning=FALSE}

data_bio<-read.csv("model_auc_scores.csv")

colnames(data_bio)<- c("Model", "AUC Score Biodata")

apa_table(
  data_bio
  , caption = "Biodata only model"
  
  #, note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  #, longtable = TRUE
  , landscape = FALSE
  #, escape = FALSE
)

```


```{r wrctable, message=FALSE, warning=FALSE}

data_wrc<-read.csv("addedWRC_model_auc_scores.csv")
colnames(data_wrc)<- c("Model", "AUC Score WRPC")

apa_table(
  data_wrc
  , caption = "Work-related psychological constructs only model"
  
  #, note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  #, longtable = TRUE
  , landscape = FALSE
  #, escape = FALSE
)

```


```{r deltas}
data_ALL<-read.csv("ALL_model_auc_scores.csv")
colnames(data_ALL)<- c("Model", "AUC Score when adding WRPC")
data_del<-left_join(data_bio, data_ALL, by="Model")

data_del$Delta <- data_del$`AUC Score when adding WRPC` - data_del$`AUC Score Biodata`

apa_table(
  data_del
  , caption = "Work-related psychological constructs added to the models"
  
  #, note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  #, longtable = TRUE
  , landscape = FALSE
  #, escape = FALSE
)

```


```{r coefbg, fig.cap="Logits of the Logistic Regression", message=FALSE, warning=FALSE}

library(ggplot2)


data <- read.csv("lr_coeff_importance.csv")


ggplot(data, aes(x=reorder(Feature, -Coefficient), y=Coefficient)) +
  geom_bar(stat="identity", fill="gray") +
  coord_flip() +  # Flips the axes to make labels readable
  labs(title="Figure 1. Feature Importance in Logistic Regression Model",
       x="Features",
       y="Logit") +
  theme_minimal() +
  theme(legend.position="none") 


```

```{r featuresbg,fig.cap="Features Importance of a Random Forest Model", message=FALSE, warning=FALSE}

library(ggplot2)


data <- read.csv("rf_feature_importance.csv")


ggplot(data, aes(x=reorder(Feature, -Importance), y=Importance)) +
  geom_bar(stat="identity", fill="gray") +
  coord_flip() +  # Flips the axes to make labels readable
  labs(title="Figure 2. Feature Importance in Random Forest Model",
       x="Feature",
       y="Importance") +
  theme_minimal() +
  theme(legend.position="none") 


```



The objective of this study was to evaluate the impact of incorporating work-related psychological constructs into models trained with biodata features for predicting turnover intention. To this end, machine learning algorithms including Gradient Boosting Trees (GBT), Random Forest (RF), Neural Networks (NN) and Support Vector Machines (SVM) were employed. A logistic regression was also employed. Models were initially trained using only biodata features. Subsequently, a second series of models was developed, integrating work-related psychological constructs.

The analysis revealed a consistent enhancement in model performance upon the addition of work-related psychological constructs across all algorithms. Specifically, models incorporating work-related psychological constructs demonstrated a notable improvement in AUC-ROC scores (table \@ref(tab:wrctable)) compared to those based solely on biodata features (table \@ref(tab:biotable)). As can be seen in table \@ref(tab:deltas), the deltas between the AUC scores with Biodata and the AUC scores when adding WRPC to the models are on average `r mean(data_del$Delta)`.  

The results substantiate hypothesis 3, demonstrating that models trained with both biodata features and work-related psychological constructs outperform those trained with biodata features alone. These findings underscore the value of incorporating psychological constructs into predictive models within the domain of work-related outcomes.

Additionally, as can be seen in figure \@ref(fig:coefbg) and figure \@ref(fig:featuresbg), the comparative analysis of the two bar graphs, which depict feature importances derived from logistic regression and Random Forest models respectively, reveals distinct patterns in how each model values the same set of features.

In the logistic regression model, the feature "ACS" displayed the most significant negative coefficient, suggesting a strong inverse relationship with the target variable. In stark contrast, "MOAQ" and "SPOS" showed slightly less negative coefficients, indicating their substantial but lesser influence compared to "ACS". On the other hand, "BIO4" and "JIMS" had positive coefficients, albeit smaller in magnitude, pointing to their positive but modest impact on the model’s predictions.

The Random Forest model presented a different hierarchy of feature importances. "SPOS" emerged as the most critical predictor with the highest importance score, indicating its predominant influence in the model. This was followed by "MOAQ" and "ACS", which also held significant importance but to a lesser extent than "SPOS". The features "BFI" and "EES", while contributing to the model, had relatively lower importance scores, suggesting their roles were not as decisive in the Random Forest model as in the logistic regression model.


## Predicting Job Satisfaction


```{r biotable2, message=FALSE, warning=FALSE}

data_bio2<-read.csv("base_model_rmse_scores.csv")

colnames(data_bio2)<- c("Model", "RMSE Biodata")

apa_table(
  data_bio2
  , caption = "Biodata only model"
  
  #, note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  #, longtable = TRUE
  , landscape = FALSE
  #, escape = FALSE
)

```


```{r wrctable2, message=FALSE, warning=FALSE}

data_wrc2<-read.csv("addedWRC_model_rmse_scores.csv")
colnames(data_wrc2)<- c("Model", "RMSE WRPC")

apa_table(
  data_wrc2
  , caption = "Work-related psychological constructs only model"
  
  #, note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  #, longtable = TRUE
  , landscape = FALSE
  #, escape = FALSE
)

```


```{r deltas2}
data_ALL2<-read.csv("ALL_model_rmse_scores.csv")
data_ALL2$Model[1]<-'Linear Regression'
colnames(data_ALL2)<- c("Model", "RMSE when adding WRPC")
data_del2<-left_join(data_bio2, data_ALL2, by="Model")

data_del2$Delta <- data_del2$`RMSE when adding WRPC` - data_del2$`RMSE Biodata`

apa_table(
  data_del2
  , caption = "Work-related psychological constructs added to the models"
  
  #, note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  #, longtable = TRUE
  , landscape = FALSE
  #, escape = FALSE
)

```


```{r coefbg2, fig.cap="Betas waves of the Multiple Regression", message=FALSE, warning=FALSE}


library(ggplot2)


data <- read.csv("lm_coeff_importance.csv")

data <- data[order(data$Coefficient, decreasing=FALSE), ]


ggplot(data, aes(x=reorder(Feature, -Coefficient), y=Coefficient)) +
  geom_bar(stat="identity", fill="gray") +  # Gray bars
  coord_flip() +  # Flips the axes to make labels readable
  labs(title="Figure 3. Feature Importance in Multiple Linear Regression Model",
       x="Feature",
       y="Beta waves") +
  theme_minimal() +
  theme(legend.position="none")  

```


```{r featuresbg2, fig.cap="Feature Importance of a Random Forest Model", message=FALSE, warning=FALSE}


library(ggplot2)

data <- read.csv("rfjs_coeff_importance.csv")

ggplot(data, aes(x=reorder(Feature, -Importance), y=Importance)) +
  geom_bar(stat="identity", fill="gray") +  # Gray bars
  coord_flip() +  # Flips the axes to make labels readable
  labs(title="Figure 4. Feature Importance in Random Forest Model",
       x="Feature",
       y="Importance") +
  theme_minimal() +
  theme(legend.position="none")  

```

The analysis revealed a consistent enhancement in model performance upon the addition of work-related psychological constructs across all algorithms. Specifically, models incorporating work-related psychological constructs demonstrated a notable improvement in RMSE scores (table \@ref(tab:wrctable2)) compared to those based solely on biodata features (table \@ref(tab:biotable2)). As can be seen in table \@ref(tab:deltas2), the deltas between the RMSE scores with Biodata and the RMSE scores when adding WRPC to the models are on average `r mean(data_del2$Delta, na.rm=TRUE)`.  

Additionally, when it comes to feature importante, both linear regression and Random Forest show similar results. As we can see in figure \@ref(fig:coefbg2), which is a bargraph showing the beta coefficients of the linear regression model, there is a balanced distribution of both positive and negative coefficients, indicating varied influences on the dependent variable. The most prominent feature in this model is "SPOS," which has a positive coefficient of approximately 0.356, suggesting a strong positive relationship with the target outcome. As can be expected, "TI" exhibits a significant negative coefficient of -0.327. Other features like "ACS" and "BIO9_1" also show positive influences, albeit to a lesser extent.

Conversely, figure \@ref(fig:featuresbg2), which stems from a Random Forest model, exclusively presents positive importance scores, focusing solely on the magnitude of influence rather than direction. In this model, "SPOS" also emerges as the most significant feature, with an importance score markedly higher at 0.470, emphasizing its critical role in model predictions. Following "SPOS," "ACS" and "EES" hold substantial importance scores of 0.211 and 0.109 respectively, demonstrating their essential contributions to the model’s performance. 

This comparative evaluation reveals that "SPOS" and "ACS" are consistently recognized as the most influential features across both models when predicting job satisfaction. 

The results substantiate hypothesis 3, demonstrating that models trained with both biodata features and work-related psychological constructs outperform those trained with biodata features alone. .

# Discussion

Hypothesis 1 anticipated that as sample size increases, ML algorithms would exhibit a growing superiority over regression models in terms of predictive accuracy for turnover intentions. This hypothesis was predicated on the assertion from previous studies that ML algorithms tend to outperform regression models in larger datasets due to their ability to handle high-dimensional data and complex model interactions (Sanchez, 2018; Kirasich et al., 2018). 

Hypothesis 2 was based on the observation that ML algorithms perform less effectively in terms of predictive accuracy when the number of predictors in a dataset is limited, particularly when the events per variable (EPV) are below 200. This hypothesis did find support in the current study’s results, suggesting that the number of variables may be a determinant factor in the efficacy of ML algorithms compared to regression models. 

The findings from the present study build on the earlier research by Kirasich et al. (2018) and Sanchez-Pinto, Venable, et al. (2018), and provide a more nuanced understanding of the comparative performance of machine learning (ML) algorithms and logistic regression models across varying sample sizes and numbers of variables. This study aimed to evaluate how these factors influence the predictive accuracy of turnover intentions, offering insight into the optimal conditions for each modeling approach.

## Sample Size Effects
Consistent with Hypothesis 1, the results suggest that ML algorithms tend to have a slight advantage over logistic regression as sample size increases. This aligns with prior findings indicating that ML algorithms generally outperform regression models in larger datasets. However, the marginal difference in AUC scores between ML algorithms and logistic regression (an average delta of 0.02 to 0.03) suggests that while ML models might start to outperform logistic regression as data quantity increases, the extent of this advantage is not as pronounced as might be expected from previous literature. This could indicate that ML algorithms are sensitive to the specific characteristics of data (like noise and data distribution) even in larger datasets, which could diminish their relative performance advantage.

## Variable Number Effects
In support of Hypothesis 2, the data indicate that the performance of ML algorithms improves relative to logistic regression as the number of variables increases. This finding is particularly significant when considering the performance consistency of logistic regression across different numbers of predictors. The study corroborates the assertion from Sanchez-Pinto, Venable, et al. (2018) that ML algorithms require a sufficient number of variables to optimize their predictive capabilities. However, the small delta in AUC scores suggests that while the number of variables is an important factor, the quality or relevance of these variables to the outcome variable (turnover intention) might be equally critical.

## Enhancement of Predictive Accuracy
Study 2 findings substantiate the significant role of incorporating work-related psychological constructs (WRPC) into predictive models for turnover intention and job satisfaction, offering compelling evidence in support of Hypothesis 3. The enhanced predictive accuracy of models integrating WRPC alongside biodata features, as demonstrated by higher AUC-ROC and lower RMSE scores, underscores the value of these constructs in modeling work-related outcomes.

The results clearly indicate that models trained with both biodata and psychological constructs outperform those trained solely with biodata. The addition of WRPC yielded an average increase in AUC scores of 0.17 for predicting turnover intention, and a decrease in RMSE scores by 0.14 for predicting job satisfaction. These improvements are not merely statistical but are likely to be practically significant, enhancing the models' utility in organizational settings. This finding aligns with previous research emphasizing the importance of psychological factors in understanding employee behavior (Lee et al., 2021; Meyer et al., 2004).

## Feature Importance and Model Interpretation
The comparative analysis of feature importances in logistic regression and Random Forest models reveals similarities in how each model values the same set of features. For instance, in both a logistic regression and a random forest, SPOS, ACS, and MOAQ were identified as relevant predictors of TI. Similarly, SPOS, ACS, and EES were identified as relevant features by both multiple regression and random forest when predicting job satisfaction. These similarities highlight that ML doesn't need to remain a mere predictive tool when the sample size is relatively small (as it is usually the case in academic settings), and can also be used for theory development to identify relevant features of a model.

## Implications and Future Directions
These findings have several important implications for predictive modeling in organizational settings. First, the choice between using ML algorithms and logistic regression should consider the size and complexity of the dataset. For applications where data are abundant and variables are numerous and well-selected, ML algorithms may offer a slight predictive edge. However, in scenarios where data are limited or the number of predictors is constrained, logistic regression remains a robust choice.

The modest differences observed also suggest that the absolute superiority of one model type over another cannot be assumed. Instead, model selection should be guided by specific project needs, including considerations like interpretability, computational resources, and the stakes of predictive errors.

Future research should explore these dynamics across a broader array of conditions, including incremental changes in sample size and variable count, to better delineate the thresholds at which ML algorithms significantly outperform logistic regression. Moreover, studies should incorporate cross-validation and external validation datasets to verify the generalizability of the observed trends.

Future studies should also explore the incremental validity of adding various types of psychological constructs and examine whether certain constructs are more influential than others. Additionally, exploring other outcomes relevant to organizational settings, such as employee engagement or performance, could further validate the utility of WRPC in predictive modeling. It would also be beneficial to investigate the impact of combining traditional statistical approaches with machine learning to leverage the strengths of both methodologies.

## Limitations
The study's approach, while more granular than previous efforts, still faces limitations. The binary categorization of model types (ML vs. logistic regression) might oversimplify the diverse capabilities and configurations of specific algorithms within these categories. Additionally, the performance metrics (AUC scores) do not capture other critical aspects of model performance such as precision, recall, or the trade-offs between them.

## Conclusion
While the findings suggest specific trends in the relative performance of ML algorithms and logistic regression, they also highlight the complexity of predictive modeling and the need for careful, context-specific selection of analytical strategies. This study contributes to a more detailed understanding of these models' operationalization in real-world scenarios, paving the way for more informed and effective applications in predictive analytics.

As Richard Landers said in an interview with SIOP, "Let's blend I-O psychology's tried-and-true practices where we know what we're measuring and we're very confident in the kinds of recommendations we're giving, and let's figure out where the intersections are with some of the new stuff coming out, to figure out what is truly new and useful and what is just a faddish waste of time." (Landers, 2019).
Many of the new technological developments in the fields of data science and computers science are not backed by the expertise that I/O psychology has been developing over the years, yet they appear more attractive for many HR practices.
It is important that we blend with these communities and contribute with our knowledge, otherwise, as Landers puts it, "they're going to run away with the farm," (Landers, 2019).
A way in which the field of I/O psychology could contribute in the development of machine learning solutions for HR practices would be to build algorithms that use antecedents of job performance commonly studied by I/O psychology researchers as input variables.
These could be variables such as organizational commitment, organizational engagement, organizational identification, perceived organizational support, perceived organizational justice, etc.
Showing evidence of incremental predictive accuracy over algorithms built with other types of input would encourage better practices.
Although machine learning algorithms have the advantage that they can learn from the data and look at associations between variables that would need to be specified if a regression technique was used instead, they are also very data hungry.
This makes them viable for fortune 500 companies, for instance, but in most contexts there's the possibility that regression methods will produce the same predictive accuracy (or perhaps even better) without having to sacrifice parsimony and sparsity.
It is also important to be selective on the variables that are being fed to the algorithms, since using all and any data that gives good predictive performance could lead to the aforementioned GIGO issue ("garbage in, garbage out").
Input may be subjective (and/or biased), particularly if there are IVs such as "previous performance".
This GIGO issue might be exacerbated by peoples' awe and wonder of ML.
Using variables supported by previous research would be best practice in building these ML solutions.

\newpage

# References
```{r}
r_refs("r-references.bib", append=FALSE)
```

<!-- ```{=tex} -->
<!-- \begingroup -->
<!-- \setlength{\parindent}{-0.5in} -->
<!-- \setlength{\leftskip}{0.5in} -->

<!-- ``` -->
::: {#refs custom-style="Bibliography"}
:::

<!-- ```{=tex} -->
<!-- \endgroup -->
<!-- ``` -->

\newpage


\appendix


# 2 {#aptables}



