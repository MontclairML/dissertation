---
title             : "Influence of Parsimony and Work-related Psychological Constructs in Predicting Turnover Intention when Using Machine Learning VS Regression"
shorttitle        : "Influence of Parsimony"
author: 
  - name          : "Diego Figueiras"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Dickson Hall 226"
    email         : "figueirasd1@montclair.edu"
affiliation:
  - id            : "1"
    institution   : "Montclai State University"

keywords          : "Employee turnover, machine learning"
wordcount         : "X"
bibliography      : ["articles.bib", "references.bib", "r-references.bib"]
floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no
csl               : "apa7.csl"
documentclass     : "apa7"
classoption       : "man"
output            : papaja::apa6_pdf
editor_options: 
  markdown: 
    wrap: sentence
params:
  title: "Influence of Parsimony and Work-related Psychological Constructs in Predicting Turnover Intention when Using Machine Learning VS Regression"
  author: 'Diego Figueiras'
always_allow_html: true
header-includes:
  - \newcommand{\bcenter}{\begin{center}}
  - \newcommand{\ecenter}{\end{center}}
#  - \pagenumbering{gobble}                                     ## removes page numbers
#  - \usepackage{sidecap}                                       ## float figures in .pdf
  - \usepackage{wrapfig}                                       ## wrap figures in .pdf
  #- \usepackage[normalem]{ulem}                                           ## strikeout
  #- \usepackage{graphicx}
#  - \graphicspath{ {./images/} }
  - \usepackage{multicol}
  - \setlength{\columnsep}{5cm}
  - \usepackage{fancyhdr}
  - \usepackage{setspace}                                      ## header/footer logo
  - \pagestyle{fancy}
  #- \setlength{\headheight}{40pt}                        ## More space between line and text
  #- \addtolength{\topmargin}{-20pt}                      ## trying to pull text up (vertically)
  #- \rhead{\includegraphics[width = .1\textwidth]{erg2.png}}                  ## header
  #- \lfoot{\includegraphics[width = .2\textwidth]{clientlogo2.png}}            ## footer
  #- \usepackage{enumitem}                                ## customized bullet symbols
  #- \usepackage{amsfonts}                                ## picking symbols for bullets
  #- \setlist[itemize,1]{label=$\bullet$}
  #- \setlist[itemize,2]{label=$\checkmark$}                   ## ams symbols
  #- \setlist[itemize,3]{label=$\circ$}              ## http://www.personal.psu.edu/dpl14/latex/symbols.pdf
  #- \setlist[itemize,4]{label=$\maltese$}

---


\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
\pagenumbering{gobble}
\phantom{i'm a ghost}
\vskip 1.0in
\bcenter


\textbf{`r params$title`} 

\vskip 0.8in

A DISSERTATION

\vskip 0.8in

Submitted to the Faculty of  

Montclair State University in partial fulfillment

of the requirements

for the degree of Doctor of Philosophy

\vskip 0.4in

by

`r params$author`

Montclair State University

Montclair, NJ

May 2024

\ecenter

\vskip 1.0in


Dissertation Chair: Dr. Michael Bixter

\newpage

\section{}
\pagenumbering{roman}
\fancyhead[LO, LE]{INFLUENCE OF PARSIMONY}
\cfoot{}
\rhead{\thepage}

\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

\onehalfspacing 

\bcenter

MONTCLAIR STATE UNIVERSITY  

THE GRADUATE SCHOOL  

DISSERTATION APPROVAL  

\vskip 0.2in

We hereby approve the Dissertation 

\vskip 0.2in  

\textbf{Influence of Parsimony and Work-related Psychological Constructs in Predicting Turnover Intention when Using Machine Learning VS Linear Regression}  

of  

Diego Figueiras  

Candidate for the Degree:  

Doctor of Philosophy



\ecenter  


\begin{multicols}{2}  

Graduate Programs:
\newline
Counseling

\vskip 0.6in

Certified by:

\vskip 0.6in  

\linespread{1.0}\selectfont
\rule{6cm}{0.2mm}
\newline
Dr. Scott Herness
\newline
Vice Provost for Research and
\newline
Dean of the Graduate School

\vskip 0.3in

\rule{6cm}{0.2mm}
\newline
Date

\columnbreak

Dissertation Committee:

\vskip 0.6in

\rule{6cm}{0.2mm}
\newline
Dr. Michael Bixter
\newline
Dissertation Chair

\vskip 0.6in

\rule{6cm}{0.2mm}
\newline
Dr. Kevin Askew

\vskip 0.6in

\rule{6cm}{0.2mm}
\newline
Dr. John T. Kulas


\end{multicols}  

\newpage
\fancyhead[LO, LE]{INFLUENCE OF PARSIMONY}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

\bcenter

\phantom{i'm a ghost}

\vskip 1.6in

Copyright@2024 by Diego Figueiras. All rights reserved.

\ecenter

\newpage
\fancyhead[LO, LE]{INFLUENCE OF PARSIMONY}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

\bcenter

\textbf{Abstract}

\ecenter

This dissertation explores the ongoing debate between traditional statistical regression models and machine learning (ML) algorithms in predictive modeling, focusing on the impact of sample size and the number of variables. Study 1 investigates the relationship between sample size and predictive accuracy, proposing hypotheses regarding the advantages of ML over regression as sample size increases. Additionally, the study examines the influence of the number of variables on predictive accuracy, emphasizing the trade-off between ML and regression models. Using data from the Federal Employee Viewpoint Survey, the research aims to contribute insights into the conditions favoring each modeling approach. Study 2 shifts the focus to incremental validity, exploring whether work-related psychological constructs enhance ML models' predictive accuracy in turnover intention compared to biodata alone. The proposed hypotheses suggest that incorporating psychological constructs will improve predictive accuracy, addressing the "garbage in garbage out" concern prevalent in ML applications. The methods involve diverse datasets, including responses from federal employees and an online survey through Amazon's MTurk, with machine learning algorithms such as Gradient Boosting Trees, Random Forest, Neural Networks, Support Vector Machines, and logistic regression. The dissertation seeks to advance understanding in the field, offering practical insights for researchers and practitioners navigating the dynamic landscape of predictive modeling.


\newpage
\fancyhead[LO, LE]{INFLUENCE OF PARSIMONY}
\pagenumbering{arabic}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

\tableofcontents

\section{Chapter 1 (Heading 1)}
\newpage

```{r setup, include = FALSE}
library("papaja")
r_refs("articles.bib")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

# Introduction

Employee turnover is a critical concern for organizations, as it impacts productivity, performance, and overall organizational effectiveness [@griffeth2000meta].
Accurate prediction of turnover is crucial for proactive human resource management and the implementation of effective retention strategies [@mitchell2001people; @mobley1979review].
In recent years, the application of predictive modeling techniques has gained prominence in addressing this challenge.
The debate arises as to whether regression-based models or machine learning models are more effective in predicting turnover, particularly when working with small sample sizes.

The high amount of computer power in the cloud environment nowadays and the developments in the field of machine learning are providing easy access to high-performance services.
Machine learning-supported tools are enabling companies to analyze and evaluate information in a quick and effective way [@tambe2019artificial].
We see this in the form of applications, software, and solutions that are common in business or that automate the different decision-making processes, such as programs that create and post job descriptions, application tracking systems that identify key words to place candidates in the right openings, tools for scheduling interviews with your online calendar, chatbots for screening, etc [@rkab2019recruitment].
Human resources practices are not being oblivious to these developments.
Experts in these practices are realizing the advantages of data-driven decision making [@fallucchi2020predicting].
Large amounts of human resources data can be analyzed in a short time and empirical inferences can be made, enabling experts to better understand employees and help anticipate issues and patterns [@macijauskieneartificial].
Being able to predict the best suited personnel for positioning or that will turn over is of particular interest to human resource departments and companies in general.
Making the wrong decision when giving a promotion or demotion can cause waste of time and energy, as well as compromise the perceived organizational justice and support, resulting in more turnover.
This is why personnel placement processes are some of the most pivotal in human resources [@macijauskieneartificial].

When making decisions for placement using traditional regression-based methods, there is a high probability of this being affected by subjective factors that can cause biased choices from time to time [@fallucchi2020predicting].
With machine learning, on the other hand, these decisions are based on a bit more objective foundation than most other recommended methods, since it is based solely on the patterns the algorithm finds on the data (though there can still be bias in the development or implementation of particular algorithms, this is minimized in comparison with traditional methods).
Not only this, but the decisions made in the personnel placement process can be explained to the candidates with their reasons, providing them with confidence in the results and diminishing the chances of low perceived organizational justice/support and high turnover.

The aim of this dissertation is to investigate and compare the predictive capabilities of regression-based models and machine learning models in the context of turnover prediction, focusing specifically on sample sizes and number of variables.
By examining the strengths and limitations of these modeling approaches, this study seeks to shed light on which combination of methods and data characteristics offers greater accuracy and reliability in predicting turnover within resource-constrained environments. Hopefully, this will inform practices in the decision-making process when using these algorithms. 

Regression-based models, including linear regression, logistic regression, and Cox proportional hazards regression, have long been established as prominent tools in predictive modeling [@hosmer2013applied; @cox1972regression].
These models are characterized by their simplicity, interpretability, and assumption of linearity between predictors and the outcome variable (or the logit of the outcome variable in the case of binary logistic regression).
The straightforward nature of regression-based models allows for the identification of significant predictors and estimation of their individual effects, facilitating an understanding of the underlying mechanisms driving turnover [@meyer2004employee; @hom2009explaining].

Contrarily, machine learning models have garnered significant attention due to their ability to handle complex relationships and patterns in large datasets [@breiman2001random; @hastie2009elements].
Algorithms such as random forests, support vector machines, and artificial neural networks offer the potential to capture non-linear and interactive effects without having to specify a theoretical model, making them valuable tools in predictive modeling [@niculescu2005predicting; @kermany2018identifying].
Machine learning models have been increasingly applied to turnover prediction, displaying promising results in various studies [@biswas2020prediction].

While the application of machine learning models has gained momentum, their performance in the context of small sample sizes remains an open question.
The literature suggests that machine learning models may face challenges, such as overfitting, when trained on limited data [@xu2020deep; @varoquaux2018cross].
Consequently, the predictive performance of these models might be compromised when sample sizes are below a certain threshold.
Thus, it becomes imperative to evaluate whether regression-based models, with their simplicity and interpretability, outperform machine learning models when the sample size is small enough.

Additionally, the performance of machine learning models may be affected when the number of independent variables is small.
In such scenarios, these models may face challenges such as overfitting or difficulty in identifying meaningful patterns [@varoquaux2018cross; @guyon2003introduction].
On the other hand, regression-based models may offer advantages in situations where the number of independent variables is limited, as they are less prone to overfitting and can provide transparent insights into the relationships between predictors and turnover [@hosmer2013applied].

Traditional variables in the demographic and biodata domain, such as age, gender, and education, have been commonly used in turnover prediction models [@hom2012reviewing; @meyer2004employee].
However, the utilization of antecedent variables typically studied in the field of I-O psychology, such as job satisfaction, organizational commitment, and conscientiousness, may provide deeper insights into the underlying factors contributing to turnover [@lee2021overlooked; @hom2012reviewing].

The application of work-related psychological antecedent variables in machine learning-based models holds promise for improving turnover prediction accuracy.
These variables capture psychological and organizational aspects that directly impact employees' turnover intentions and behaviors [@lee2021overlooked; @meyer2004employee].
By considering these variables in predictive models, organizations can gain a more comprehensive understanding of the complex dynamics that drive turnover and develop targeted interventions to mitigate it [@griffeth2000meta; @hom2012reviewing].

Contrarily, models relying solely on demographics and biodata may overlook critical factors contributing to turnover.
While these variables provide basic demographic information, they may lack the depth and specificity necessary to capture the nuances and complexities of turnover behavior [@hom2012reviewing].
Incorporating work-related psychological antecedent variables can offer a more nuanced and accurate prediction by considering individual attitudes, perceptions, and experiences within the organizational context, as has been demonstrated before [@meyer2004employee; @lee2021overlooked].


This second study aims to address this research question by employing comprehensive data.
By leveraging turnover intention data and a restricted set of independent variables, along with relevant predictor variables such as demographics/biodata, job characteristics, employee engagement, and other work-related psychological antecedent variables of turnover, a comparative analysis will be conducted.
The performance of regression-based models and machine learning models will be assessed using various metrics, including accuracy, precision, recall, and the area under the receiver operating characteristic curve (AUC-ROC) [@davis2006relationship].

The findings from this research will contribute to the existing literature on turnover prediction and provide valuable insights for practitioners and researchers alike.
Understanding the relative performance of regression-based models and machine learning models when dealing with small sample sizes can guide decision-making regarding the choice of modeling techniques in resource-limited scenarios.
Ultimately, this research aims to enhance our understanding of turnover prediction and inform effective retention strategies to mitigate the negative consequences of employee turnover.

## What Constitutes Machine Learning

The delineation between machine learning (ML) and statistical modeling has been extensively explored in the literature [@breiman2001statistical].
However, the distinction remains ambiguous, as noted by @collins2014external.
Breiman's seminal work on the "two cultures" serves as a key reference in this discussion [@breiman2001statistical].
Breiman highlights the contrast between theory-driven models like regression and empirical algorithms such as decision trees, artificial neural networks, support vector machines, and random forests.

## Theory-based models

Theory-based models are models that are based on theory and assumptions, such as traditional linear regression, and benefit from human intervention and subject knowledge for model specification.
The analysis in this approach starts with assuming a stochastic data model for the inside of the black box.
Usually, research that uses this approach starts by assuming that the data are generated by a particular model.
This model is used as a template for statistical analysis.
When faced with an applied problem, researchers that use this approach come up with a data model by looking at the literature developed by previous scholars or by their own theorizing, or some combination of both.
This enables them to develop a reasonably good parametric class of models for a complex mechanism devised by nature, and then parameters are estimated and conclusions are drawn.
However, these conclusions are about the model's mechanism, not about nature's mechanism, and therefore if the model is a poor emulation of nature, the conclusion could be wrong.
Breiman (2001) criticized this approach, pointing out that: "A few decades ago, the commitment to data models was such that even simple precaution such as residual analysis or goodness-of-fit tests were not used. The belief in the infallibility of the data models was almost religious. It is a strange phenomenon -- once a model is made, then it becomes truth and the conclusions from it are infallible (p. 202)." He concludes by using the following old saying: "If all a man has is a hammer, then every problem looks like a nail (p. 202)." To solve a wide range of problems, such as is the case in the social sciences with the abundance of variables, a larger set of tools is needed.
The rapidly increasing ability of computers to store and manipulate data can provide us with more varied tools.

## Empirical-based models

In the mid-1980s, with the development of neural networks and decision trees, a new community of researchers appeared focused on predictive accuracy [@cristianini2002support].
They began using these algorithms on working in complex prediction problems where it was obvious that data models were not applicable, such as speech recognition, image recognition, handwriting recognition, times series analysis, or financial market analysis.
The approach is that nature produces data in a black box whose insides are complex and partly unknowable.
The goal is not to explain the patterns in this data, but to predict them based on input; not to focus on data models, but on the characteristics of the algorithms [@breiman2001statistical].
Within psychology, this is the same approach as dustbowl empiricism [@schoenfeldt1999dust].

An effective characterization of machine learning involves its emphasis on models that autonomously glean insights from data [@mitchell1997machine].
Unlike regression, machine learning exhibits a higher degree of automation in modeling, particularly in handling nonlinear relationships and interaction terms, as exemplified by Boulesteix [@boulesteix2014machine].
Achieving this automation often entails the use of highly adaptable algorithms in machine learning, which may necessitate penalization techniques to prevent overfitting [@deo2016learning].
Some scholars depict the relationship between statistical modeling and machine learning as a continuous spectrum [@beam2018big].
Meanwhile, others categorize any approach deviating from basic regression models as machine learning, encompassing methods like penalized regression (e.g., LASSO, elastic net) or generalized additive models (GAM) [@he2009learning].
It is essential to highlight that, according to the "automatic learning from data" definition, these methods do not strictly fall under machine learning.
In this study, we refrain from classifying them as such.

## Common Terms in Machine Learning

-   Supervised learning: these are algorithms that have a dependent variable they are trying to predict.
    If the dependent variable is discrete, it would be a classifier; if it is continuous, it would be regression-based [@hastie2009elements].
    For the purposes of this study we will be focusing on this type of machine learning.

-   Unsupervised learning: these are algorithms that do not have a dependent variable that they are trying to predict, but rather look for associations and clusters among the independent variables [@hastie2009elements]

-   Cross-validation: this is a technique used to assess the performance and generalizability of a predictive model.
    It involves partitioning the dataset into multiple subsets, training the model on some of these subsets, and evaluating its performance on the remaining data.
    This process is repeated multiple times, with different subsets used for training and testing, allowing for a more robust evaluation of the model's effectiveness across various data scenarios [@douglass2020book].

-   Regularization: in machine learning, this is a technique employed to prevent overfitting and enhance the generalization performance of a predictive model by adding a penalty term to the cost function.
    This penalty discourages the model from fitting the training data too closely and helps to control the complexity of the model, preventing it from becoming too intricate and specialized to the training set [@hastie2009elements].

-   Ensemble: in machine learning, this involves combining predictions from multiple individual models to create a more robust and accurate overall prediction [@dietterich2000ensemble].
    By leveraging the diversity among the constituent models, ensemble methods aim to improve generalization performance, mitigate overfitting, and enhance predictive accuracy across a variety of scenarios.

## Data Hungriness

The concept of data hungriness refers to the sample size needed for a method to generate a prediction model with a good predictive accuracy [@van2014modern].
The data hungriness of a predictive modeling technique is defined as the minimum number of events per variable at which the optimism of the generated model is less than 0.01.
Optimism is defined as the difference between error on the sample data and the error when applying the model to another dataset.
Every machine learning model has some amount of error in its predictions.
This error usually comes from two different sources: bias and variance.
Bias is the tendency of the model to underfit, and variance is the tendency to overfit.
The relationship between these two sources of error is known as the bias-variance tradeoff, and developers of machine learning models have to find the balance between the two.

To test if this trade-off has been done in a way that minimizes error, it is a good idea to measure performance using data that the model has never seen before.
The performance of the model on this "test data" will be a more accurate predictor of the model's performance in the real world, which is the fundamental basis for cross-validation .
The model's optimism, therefore, is the difference between the training error estimated from the data used to build the model and the test error estimated by applying the model into out-of-sample data.
The sample size needed to minimize this difference is what is referred to as data hungriness.
Machine learning algorithms generally require big sample sizes to minimize this difference [@van2014modern].

## Parsimony

Parsimony is defined as the sample size and number of variables that a dataset must have in order to maximize the predictive accuracy of a model [@sanchez2018big].
A model is considered parsimonious when it both uses the least amount of variables possible (sparsity) and has good prediction accuracy.
Typically, parsimony is reported as the performance metric of models that are sparse.

# Algorithms

In this study, various machine learning algorithms are described and assessed in their ability to predict dependent variables of interest in the field of I/O psychology, specifically turnover intention. 
This section provides a general overview of the theory behind these algorithms.

## Decision Trees

Decision trees are a type of supervised machine learning method.
They create classification or regression models following a tree-like structure [@mahesh2020machine].
Each fork is a split in a predictor variables, and in which each end node contains a prediction for the outcome variable [@marsland2011machine].
They are used to explain how the target variable's values can be predicted based on other variables.
Nodes are split into sub-nodes based on a threshold value of a variable [@mahesh2020machine].
Decision trees are capable of handling missing values and mixed features, as well as to select variables automatically.
On the downside, their predictive power is not as high as other algorithms and they are not stable with high model variance and small variations in the data.
This could result in a large effect on the tree structure that is not meaningful.
Understanding how they operate becomes important in order to understand other algorithms, such as random forest and gradient boosting trees [@alpaydin2020introduction; @marsland2011machine].

## Classification and Regression Trees (CART)

A Classification and Regression Tree is an umbrella term that refers to the use of either classification and/or regression trees [@marsland2011machine].
We already explained classification trees, so lets now explain regression.
Regression trees are a type of decision tree.
They are different from classification trees in that each leaf represents a numeric value, while classification trees have "true" or "false" in their leaves, or some other discrete category.
The roots and branches of regression trees are typically ranges of the IVs, while the leaves are typically the average value of the DV in those ranges.
The structure of these trees typically starts with the root being the lower threshold of the IVs, and its leaf the average DV at this range.
Then the first branch is the upper threshold, with the leaf being the average DV at that range.
The second branch is the middle threshold of the IVs, with the leaves being the average DV between the middle and the lower threshold and the average DV between the middle and the upper threshold.
Because of this structure, regression trees are better at capturing non-linear associations than simple linear regression [@mahesh2020machine].

## Random Forests (RF)

Random forest are machine learning algorithms that take an ensemble approach that provides an improvement over decision trees [@mahesh2020machine].
They are built by taking a random sample of the data and then building an ongoing series of decision trees on the subsets.
They create many decision trees (hence "forest") to improve predictive accuracy and if one or more of these smaller decision trees are not relevant, they get ignored in favor of the better ones.
In other words, they combine a group of weak learners to form a stronger learner.

The way this algorithm works is that a number of decision trees are built on bootstrapped training sets and a random sample of IVs are chosen at each step as split variables from the full set of predictions in each decision tree.
In this way, it is unlikely that all of the individual trees will be influenced by a few "noisy" predictors.
Hundreds of decision trees are built this way, which is why these algorithms can be very slow and take a lot of computational memory.
The algorithm keeps track of the outcome predicted by every decision tree and picks the most frequent one; this is called "bagging".
Variance is reduced by taking the average of the uncorrelated trees ("out of bag" samples), making the final result more reliable and less variable.
They greatly help reduce overfitting and bias because of this, which are two of the greatest limitations of regular decision trees.
Similarly, they can fill in missing data by computing the weighted averages across the hundreds of samples, in the case of continuous data, and frequencies, in the case of categorical data.
[@alpaydin2020introduction].

## Gradient Boosting Trees (GBT)

Gradient boosting trees are very similar to random forests but with the difference that the former learn sequentially instead of randomly.
The ways the individual trees are built and their results are combined is different.
Random forest builds independent decision trees and combines them in parallel, while gradient boosting trees use a method called "boosting", which combines each learner sequentially so that each new tree corrects the errors of the previous one.
A weak learner would be a decision tree with only one split, which is also called a "stump".
To evaluate how well each tree does, the algorithm uses a loss function (such as cross entropy, to name one).
In classification trees, when the DV label and predictor do not agree, the loss function is close to 1; when they are in perfect agreement, the loss function is 0.
In GBT, a series of trees are created and each of them tries to lower the loss function of the previous tree in the series.
Trees are constantly added in this way until no further enhancement can be achieved.
This makes predictions with GBT fast and memory-efficient, although they are hard to visualize and interpret.
Compared to random forest, they have a lot of model capacity that enables them to model complex relationships and decision boundaries.
As with previous decision trees methods, there is also the danger of overfitting [@mahesh2020machine].

## Bayesian Additive Regression Trees (BART)

Bayesian Additive Regression Trees are also an ensemble method that uses many decision trees as its building blocks, much like RF or GBT.
It could be said that BART tries to capture the best of RF and GBT; they construct the trees by sampling randomly, much like bagging in RF, and they also try to capture signal not yet accounted for by the current model, much like boosting in GBT.
It is considered a nonparametric function approach used to predict using regression trees.
They rely on recursive binary partitioning of predictor space into hyperrectangles.
Hyperrectangles are cubes used to classify data, since putting data in 3d space is a better way to classify mixed data.
Once this is done, the trees are summed and regularized to avoid overfitting [@chipman2010bart; @marsland2011machine].

## Neural Networks (NN)

Neural networks were built with the intention of emulating the human nervous system. The input is associated with weights, bias, activation functions, and a computed output from all of this. Much like decision trees, NNs consist of nodes and connections between the nodes, with input nodes, output nodes, and different layers of nodes in between. A neural network may contain more than one layer between input and output to handle complex problems. These layers are referred to as hidden layers [@alpaydin2020introduction]. 
	The way the algorithm works is that the input node moves to the hidden layer by being multiplied by the weight and having the bias added, each of which is associated with the input. This is done for every single value in the input (also called IV), and the results are put into the activation function formula associated to hidden layers. The activation function is what decides the predicted output, and the formula varies depending on which function is being used. Common activation functions are the sigmoid, soft plus, hyperbolic tangent, and rectified linear units (ReLu). The weights and bias come from fitting the NN to the data beforehand using a method called “backpropagation.” After scaling the portion of the activation function estimated, the resulting functions are aggregated, then multiplied by the weights associated with the output and the bias also associated with the output is later added, resulting in a function that fits the data and that is used to make predictions based on the input.
To model very complex neural networks with high predictive power, one can extend the model by adding more hidden layers, which is what we know as deep learning. Due to the rapid development of hardware and continuous research on backpropagation techniques, NNs are the most studied area in machine learning [@mahesh2020machine; @marsland2011machine; @alpaydin2020introduction]. 

## Support Vector Machines (SVM) classifiers

Support Vector Machines (SVMs) represent supervised machine learning algorithms frequently employed for classification tasks.
These algorithms operate on the principle of identifying a hyperplane that optimally segregates the dataset into two distinct classes.
The assurance of correct classification increases as data points move farther away from this hyperplane.
The pivotal data points nearest to the hyperplane, known as support vectors, play a crucial role; their removal would impact the position of the dividing hyperplane, designating them as vital components of the dataset.
The margin, denoting the distance between the hyperplane and the support vectors, is a key measure.
The objective of SVM is to select a hyperplane with the widest possible margin, enhancing the likelihood of accurately classifying new data.
Cross-validation is employed to determine this margin.
SVM finds extensive application in tasks such as text classification and image recognition, excelling in managing outliers by permitting misclassifications and overlapping classifications [@mahesh2020machine; @marsland2011machine; @alpaydin2020introduction]

# Evaluation metrics

Performance metrics evaluate a model's predictive performance and tell you how good or poor the performance of the model is.
In this study, various performance metrics will be used.
This section provides a general description of them.

## Accuracy

Accuracy is a prediction performance metric used for classification machine learning algorithms (those that predict a dependent variable that has categorical data rather than continuous).
It is perhaps the simplest metric to use and implemented, and it's defined as the number of correct predictions divided by the total number of predictions.
This ratio is usually stated as a percentage [@marsland2011machine].

## Precision

Precision is also a ratio for classification algorithms, much like accuracy, but between true positives and total amount of positives predicted.
This metric focuses on Type-I errors [@alpaydin2020introduction]; in other words, incorrectly labeling the dependent variable during prediction.
A precision score towards 1 means that the model did not miss any true positives and is able to classify well between correct and incorrect labeling of the dependent variable.
A low precision score of less than 0.5 means that the classifier has a high number of false positives, which can be an outcome of imbalance class.
This metric cannot measure the existence of Type-II error [@marsland2011machine].

## Recall/Sensitivity

Recall, also called sensitivity, focuses on Type-II errors [@alpaydin2020introduction].
It is similar to precision but with a different denominator, which would be the number of true positives plus the number of false negatives.
Recall towards 1 will mean that the model did not miss any true positives and is able to classify well between correctly and incorrectly labeling the dependent variable.
A low recall score of less than 0.5 would mean that the classifier has a high number of false negatives, which can be an outcome of imbalance class.
This metric cannot measure the existence of Type-I error [@marsland2011machine; @alpaydin2020introduction].

## F1-score

This metric combines precision and recall, since it is the harmonic mean between the two [@alpaydin2020introduction].
A high F1 score means that there is high precision and high recall.
A low F1 score has little to no meaning, since although it tells us that the precision and recall were low, it does not tell us which cases were incorrectly classified as true positives or false negatives.
However, it is still useful in deducing the performance of the model [@marsland2011machine].

## Area Under the Receiver Operating Characteristics Curve (AUC-ROC)

AUC-ROC is a useful metric to compare different machine learning classifiers.
The true positives ratio (TPR) and false positives ratio (FPR) are plotted in a graph under different thresholds.
This resulting curve is called the Receiver Operating Characteristics Curve (ROC).
The area under this curve is what we call AUC, which is equivalent to the probability that a randomly chosen true positive case is deemed to have a higher probability of being positive than negative than a randomly chosen negative case.
In other words, a high AUC means that the probability of a randomly chosen positive example is indeed positive.
This tells that the model does a good job at discriminating negatives and positives, with most true positives at one end and positives in the other [@alpaydin2020introduction; @marsland2011machine].

## Root Mean Square Error (RMSE)

The root mean square error is another standard way of measuring the accuracy of a predictive model.
It shows how far predictions fall from measured true values using Euclidean distance [@myers2013research].
In other words, it is simply the standard deviation of the residuals.
It tells you how spread out the errors are and how concentrated the data is around the line of best fit.
It indicates the absolute fit of the model to the data and the average model prediction error in units of the dependent variable.
The lower the RMSE, the better the predictive accuracy.

## Log-loss

Log-loss is another important metric in assessing the performance of machine learning algorithms, especially classifiers.
It indicates how close the prediction probability is to the true value, and the more the predicted probability is different from the actual value, the higher the log-loss [@marsland2011machine; @alpaydin2020introduction].

## Kappa Statistic

Kappa statistic, also called Cohen's kappa, is a metric used to assess the performance of a classification model.
It is the same metric to compute inter-rater agreement, but in the context of machine learning, it can be used to compare the algorithm's predictions on different classes or labels of a dependent variable [@marsland2011machine; @alpaydin2020introduction].

## Geometric mean score (G-mean)

The geometric mean is another useful metric in assessing accuracy of performance.
It is the root of the product of a class-wise sensitivity.
It tried to maximize the accuracy on each of the classes and keep them balanced at the same time.
Mathematically, it is basically the square root of the product of sensitivity and specificity [@kubat1997addressing; @barandela2003new].

# Rationale

Predictive modeling has gained significant attention in recent years across various disciplines due to its potential to make data-driven decisions.
One of the fundamental questions that researchers and practitioners encounter in predictive modeling is whether traditional statistical regression models or machine learning (ML) algorithms are more effective in producing accurate predictions.
This debate is crucial as it guides the selection of appropriate modeling techniques for specific datasets, which can vary in terms of sample size and the number of variables.
In this section, we will review the literature and rationale leading to the hypotheses for Study 1 of the current dissertation proposal.

## Sample Size and Predictive Modeling

Predictive modeling techniques, such as regression models and ML algorithms, differ in their ability to handle different sample sizes effectively.
Owing to its flexibility, ML methods are claimed to have better performance over traditional statistical modeling, and to better handle a larger number of potential predictors [@vijayakumar2019replicability].
Many of these methods have the ability to capture non-linear associations without a theoretical specification by the researcher, as has been shown by simulation studies done by @miller2016finding, in which they simulated datasets with prescribed multivariate dependencies and non-linear effects, and they found out that random forest and boosting performed better than regression in selecting variables relevant to fit the model.
Multiple linear regression models also have the inconvenience that they use assumptions when fitting the data.
Namely, that there is statistical independence of the errors, a linear relationship between dependent and independent variables, homoscedasticity of the error, and normality of the error distribution [@myers2013research].
This is refered to as inconvenient because these assumptions may not always be met, which can lead to biased results that jeopardize predictive accuracy.
Machine learning methods, on the other hand, are quite effective at handling non-linear and complex data without having to make any assumptions, even if datasets are noisy [@craninx2008artificial].
@alzate2022predictions did a study that showed evidence of classification trees outperforming logistic regression when predicting attrition in the U.S.
Marine Corps.
@froud2021relative found that machine learning outperformed regression for simulated non-linear heteroscedastic variables when predicting quality of life and academic performance of school children in Norway.
@luu2020machine found that machine learning (specifically, gradient boosting trees) outperforms logistic regression in predicting next-season NHL player injury.

It is worth noting, however, that there are cases where regression models outperform machine learning in predictive accuracy [@sanchez2018big; @sanchez2018comparison]. It has been shown that machine learning algorithms also require more data than traditional data modeling, since their data hungriness is so demanding [@van2014modern].
Furthermore, machine learning models are typically assessed in terms of discrimination performance (e.g., accuracy, area under the receiver operating characteristic [ROC] curve [AUC]), while the reliability predictions (calibration) are often not assessed (Van Calster et al., 2016).

In a preliminary systematic literature review conducted by the author of this dissertation, when looking at ML-based predictive models built to predict turnover, among other relevant dependent variables for personnel selection, only two articles out of the twenty-eight reviewed had data with sample sizes above 269,999, which is the sample size in @sanchez2018comparison's study at which machine learning algorithms outperform traditional data modeling such as regression.
Without meeting the data hungriness of machine learning algorithms, not only is the accuracy of the models jeopardized, but the outcomes might not be any better than regression, raising the question of why should the parsimony of these traditional models be sacrificed for more complex models if the results might be similar? Knowing the threshold at which regression-based methods are a better alternative is of interest when creating predictive models of turnover intention. 

In the same literature review, when it comes to parsimony, the article with the lowest events per variable (EPV) was 1.62 and the highest was 22267.42.
The median EPV was 44, which is small and more appropriate for predictive studies using regression, as stated by @sanchez2018comparison, since their research shows that machine learning algorithms perform better than regression at a EPV above 200.
Only ten out of the twenty-eight articles in this literature review had a EPV above 87.
Using machine learning algorithms with such small number of variables could compromise the bias-variance tradeoff, overfitting the data.

# Hypotheses

## Study 1: Sample Size and Number of Variables

This study is based on prior research [@sanchez2018comparison; @kirasich2018random], which suggests that ML algorithms outperform regression models consistently when the sample size is large.
In their study, they compared machine learning methods with regression methods for variable selection for two differently sized datasets, focusing on parsimony in predictor selection, which balances predictive accuracy with the sparsity of predictors.
Their findings were that for the smaller dataset (N=6,565), regression achieved the best predictive accuracy, while machine learning methods faring better only in the larger dataset (N=269,999).
Since there is some recognition that regression models perform well with smaller datasets, where the number of observations is limited, and that ML algorithms tend to outperform when the dataset is very large, the following hypotheses for Study 1 are proposed:

Hypothesis 1: As sample size increases, the relative advantage in predictive accuracy of turnover intention between ML and regression-based algorithms will increase.

In addition to sample size, the number of variables in a dataset is another critical factor affecting the choice between regression models and ML algorithms.
Going back to the study done by @sanchez2018comparison, predictive accuracy decreased for machine learning when the dataset had fewer variables compared to regression models, whose predictive accuracy remained the same or improved.
Specifically, this happened when the EPV were below 200.
This leads to the proposal of Hypothesis 2.

Hypothesis 2: As the EPV decreases, the relative advantage in predictive accuracy of turnover intention betweel ML and regression-based algorithms will decrease.

In conclusion, Study 1 aims to investigate the interplay between sample size and the number of variables in predictive modeling by testing the hypotheses outlined above.
Through a systematic exploration of these factors, we hope to contribute to the understanding of when regression models or ML algorithms are more suitable for predictive tasks in real-world scenarios.
Both simulated and real-world data will be used to test these two hypotheses.

## Study 2: Incremental Validity

The literature review done on 28 articles that studied the predictive accuracy of machine learning models when predicting relevant variables from a job selection context, such as turnover or job satisfaction, noted that only 9% of the studies looked at independent variables that I/O psychologists typically use in predicting job performance in selection contexts, and that the literature has proved are relevant in predicting job performance/turnover, such as job satisfaction, commitment, conscientiousness, organizational identification, perceived organization support and justice, involvement, or engagement [@ertas2015turnover; @kim2017employee; @pitts2011so; @liss2015loving; @maertz2007effects; @smith2005achieving; @liss2015loving; @choi2011organizational; @schaufeli2017ultra; @reeve2001refining; @meyer1991three; @shamir2004single; @shore1991construct; @franzen2022developing]. None of the articles that were predicting turnover or turnover intention used any measure of any of these psychological constructs. 
The literature in the field of I/O psychology has much to contribute in using the appropriate predictors for building machine learning algorithms that want to predict job performance and turnover.
This would help the "garbage in garbage out" criterion problem that is so common with ML.

The application of work-related psychological antecedent variables in machine learning-based models holds promise for improving turnover prediction accuracy.
These variables capture psychological and organizational aspects that directly impact employees' turnover intentions and behaviors [@lee2021overlooked; @meyer2004employee]. However, when building machine learning models to predict turnover, most studies use data that could better be described as biodata. Biodata in the context of job selection refers to a comprehensive collection of an individual's personal, educational, and professional information used for employment assessment and decision-making processes. This information typically includes details such as educational qualifications, work experience, skills, achievements, and other relevant data that assists employers in evaluating a candidate's suitability for a specific position.
Assessing whether work-related psychological contructs show incremental validity over biodata when establishing criterion-related validity with turnover intention is of interest in the study of machine learning in the I/O psychology field.
The proposed hypotheses are the following:

Hypothesis 3a: machine learning models trained with work-related psychological constructs will show better predictive accuracy metrics when predicting turnover intention than models trained using biodata.

Hypothesis 3b: the predictive accuracy of machine learning models will increase when work-related psychological constructs are added to the model.

# Study 1

## Methods

## Procedure and Participants

The data that will be used for this study are responses to the Federal Employee Viewpoint Survey (FEVS) publicly available at the website of the Office of Personnel Management.
The dataset consists of 107 variables and a sample size of 557,779 federal employees that took the survey in 2022. Different samples will be randomly taken from this dataset to test for hypothesis 1. These will each have a sample of 50, 500, 5,000, 50,000, and the full 557,779.  
Additionally, to test for hypothesis 2, samples with varying EPV will be randomly selected from the FAVS. These will be with 10, 50, 100, 200, and 500 EPV. 


## Data

The variables used in the FEVS are the following:

## Independent variables

### Individual characteristics

Several factors consistently linked to turnover intention, such as gender [@ertas2015turnover ; @ko2014impacts; @wynen2014impact], minority status [@pink2017examining; @weaver2015intent], educational achievement [@joo2012model; @liss2015loving], supervisory position [@ertas2015turnover; @pink2017examining], and organizational tenure [@lewis1992men; @pitts2011so; @wynen2014impact].
Examples of survey items assessing these variables include "How long have you been with the Federal Government (excluding military service)?" and "What is your supervisory status?"

### Job characteristics

Seven factors pertaining to job characteristics were chosen: job satisfaction [@ertas2015turnover; @kim2017employee; @pitts2011so], personal accomplishment [@ertas2015turnover; @wynen2014impact], workload [@wynen2014impact; @wynen2014impact], meaningfulness [@ertas2015turnover; @kim2017employee; @weaver2015intent], and goal clarity [@ertas2015turnover; @ko2014impacts; @weaver2015intent].
Sample survey items gauging these variables include "Considering everything, how satisfied are you with your job?"; "My work gives me a feeling of personal accomplishment"; and "My workload is reasonable."

### Organizational characteristics

Eleven factors linked to organizational policies were included: compensation [@wynen2014impact; @wynen2014impact), family-friendly policies [@durst1999assessing; @joo2012model], training and skill development [@curry2005training], diversity management [@choi2009diversity], and promotion and advancement policy [@cotton1986employee; @johnson2000factors; @porter1974organizational].
Sample survey items evaluating these variables include "Considering everything, how satisfied are you with your pay?"; "My supervisor supports my need to balance work and other life issues"; and "How satisfied are you with your opportunity to get a better job in your organization?"

### Workplace environment

Thirty-six factors pertaining to organizational environments were selected: organizational satisfaction [@liss2015loving], coworker relations [@cotton1986employee], supervisory support [@maertz2007effects; @smith2005achieving], supervisory communication [@joo2012model], managerial trustworthiness [@ko2014impacts], organizational procedural justice [@choi2011organizational], creativity [@ertas2015turnover], employee empowerment [@kim2017employee; @pink2017examining; @pitts2011so], performance-oriented culture [@pitts2011so], loyalty [@weaver2015intent], and organizational support [@liss2015loving].
Sample survey items assessing these variables include "Considering everything, how satisfied are you with your organization?"; "The people I work with cooperate to get the job done"; "My supervisor provides me with constructive suggestions to improve my job performance"; "My performance appraisal is a fair reflection of my performance"; and "I recommend my organization as a good place to work."

## Dependent variable

### Turnover Intention

Some scholars advise against using turnover intention as a proxy for actual turnover in federal government agencies.
Analyzing age and experience as the unit of analysis revealed strong correlations between turnover intention and actual turnover.
Conversely, at the organizational level, turnover intention exhibited a negative correlation with actual turnover [@cho2012turnover].
@cohen2016does also observed similar findings, noting that the turnover intention rate is not significantly associated with actual turnover at the organizational level.
Despite this caution, organizational-level research on turnover is recommended to utilize actual turnover, while individual-level studies should employ turnover intention at the employee level.
Given that this study investigates employees' perceptions of organizational workplace environments and their relationship to turnover intention at the individual level, the use of turnover intention is deemed appropriate for the study's objectives.

Turnover intention is assessed using a single survey item: "Are you contemplating leaving your organization within the next year, and if so, why?" Response options include "No," "Yes, to take another job within the Federal Government," "Yes, to take another job outside the Federal Government," and "Yes, other." Turnover intention was recoded as a binary outcome, with "0" indicating "No" and "1" indicating "Yes, to take another job within the Federal Government," "Yes, to take another job outside the Federal Government," and "Yes, other."

## Data analysis

Several machine learning models were trained, and the predictive accuracy measures of Turnover intention were estimated for each combination of data, sample sizes, number of variables, and model.
The algorithms being used were determined by a prior literature review conducted by the author in which the most frequent and accurate algorithms used in predicting turnover were the following: Gradient Boosting Trees (GBT), Random Forest (RF), Neural Networks (NN), and Support Vector Machines (SVM).
Additionally, a logistic regression was used as a comparison.

For all of the analyses, the scikit-learn library in Python was used to train the models and optimize performance. Whenever hyperparameter tunning was being done, each configuration was assessed using a 10-fold cross-validation scheme. 

For each XGB model, the xgboost package in Python was used and hyperparameter tuning was conducted using the GridSearchCV method from the scikit-learn library. This tuning process explored a combination of three hyperparameters: max_depth, with values ranging from 1 to 4, and learning_rate, tested at three levels (0.005, 0.05, 0.5). 

Each neural network was developed and evaluated using TensorFlow's Keras API in Python. The architecture of the networks consisted of an input layer with 128 neurons, a hidden layer with 64 neurons, and an output layer with a single neuron using a sigmoid activation function. The models were compiled with the Adam optimizer and the training process was conducted over 10 epochs with a batch size of 32, ensuring that the model had multiple iterations to learn from the dataset.

For each SVM, hyperparameter tuning was executed through RandomizedSearchCV. The norm ('l1', 'l2') was used in the error term, the maximum number of iterations were (1000, 2000, 3000), and the factor to multiply the intercept were (1, 10, 100). 

For each random forest, the grid search focused on two key parameters: the number of trees in the forest, with tested values of 10, 50, 100, and 200, and the maximum depth of each tree, with values ranging from 1 to 4.

```{r}
# library(tidyverse)
# data<-read.csv("2022_OPM_FEVS_PRDF.csv")
# data<- mutate_all(data[3:104], function(x) as.numeric(as.character(x)))
# data<-na.omit(data)
# library(psych)
# fa.parallel(data)

```



# Results

## Sample Size 100,000 

```{r logitable100k, message=FALSE, warning=FALSE}
library(tidyverse)
# Load necessary packages or install if not already installed
if (!require(knitr)) {
    install.packages("knitr")
    library(knitr)
}
library(kableExtra)
# Create a data frame with the classification report metrics
# metrics <- data.frame(
#   Class = c('0', '1', 'Accuracy', 'Macro Avg', 'Weighted Avg'),
#   Precision = c(0.71, 0.78, NA, 0.74, 0.75),
#   Recall = c(0.45, 0.91, NA, 0.68, 0.76),
#   `F1-Score` = c(0.55, 0.84, 0.76, 0.69, 0.75),
#   Support = c(11812, 24862, 36674, 36674, 36674)
# )

metrics<- read.csv("logi100k.csv")
logi100k<-data.frame(
  Algorithm = "logi100k",
  Model = "Logistic Regression",
  `Sample Size`= 100000,
  `Number of Variables` = 105,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsLOGI <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsLOGI[2:7]<- mutate_all(metricsLOGI[2:7], function(x) as.numeric(as.character(x)))
metricsLOGI <- data.frame(lapply(metricsLOGI, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metrics)[1] <- ""

# Use kable to create a nicer table
apa_table(
  metrics
  , caption = "Logistic Regression Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  , longtable = TRUE
  , landscape = FALSE
  , escape = FALSE
)


# kable(metrics, caption = "Logistic Regression Predictive Metrics", 
#       col.names = c('Class', 'Precision', 'Recall', 'F1-Score', 'Support'), 
#       align = 'c') %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
#                 full_width = F, font_size = 12)


```

```{r xgbtable100k, message=FALSE, warning=FALSE}

# Load necessary packages or install if not already installed
if (!require(knitr)) {
    install.packages("knitr")
    library(knitr)
}
library(kableExtra)
# Create a data frame with the classification report metrics
# metrics <- data.frame(
#   Class = c('0', '1', 'Accuracy', 'Macro Avg', 'Weighted Avg'),
#   Precision = c(0.70, 0.78, NA, 0.74, 0.76),
#   Recall = c(0.48, 0.90, NA, 0.69, 0.77),
#   `F1-Score` = c(0.57, 0.84, 0.77, 0.70, 0.75),
#   Support = c(11812, 24862, 36674, 36674, 36674)
# )

metrics<- read.csv("xgb100k.csv")
xgb100k<-data.frame(
  Algorithm = "xgb100k",
  Model = "xgboosting",
  `Sample Size`= 100000,
  `Number of Variables` = 105,
  `AUC Score` = round(metrics$AUC[1], 3)
)


# Convert numeric values to text to handle NA values
metricsXGB <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsXGB[2:7]<- mutate_all(metricsXGB[2:7], function(x) as.numeric(as.character(x)))
metricsXGB <- data.frame(lapply(metricsXGB, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsXGB)[1] <- ""

apa_table(
  metricsXGB
  , caption = "xgboosting Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  , longtable = TRUE
  , landscape = FALSE
  , escape = FALSE
)

# Use kable to create a nicer table
# kable(metricsXGB, caption = "xgboosting Predictive Metrics", 
#       col.names = c('Class', 'Precision', 'Recall', 'F1-Score', 'Support'), 
#       align = 'c') %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
#                 full_width = F, font_size = 12)


```

```{r nn100k, message=FALSE, warning=FALSE}

# Load necessary packages or install if not already installed
if (!require(knitr)) {
    install.packages("knitr")
    library(knitr)
}
library(kableExtra)
# Create a data frame with the classification report metrics
# metrics <- data.frame(
#   Class = c('0', '1', 'Accuracy', 'Macro Avg', 'Weighted Avg'),
#   Precision = c(0.69, 0.79, NA, 0.74, 0.76),
#   Recall = c(0.49, 0.90, NA, 0.69, 0.76),
#   `F1-Score` = c(0.57, 0.84, 0.76, 0.71, 0.75),
#   Support = c(11812, 24862, 36674, 36674, 36674)
# )

metrics<- read.csv("nn100k.csv")
nn100k<-data.frame(
  Algorithm = "nn100k",
  Model = "Neural Network",
  `Sample Size`= 100000,
  `Number of Variables` = 105,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsNN <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsNN[2:7]<- mutate_all(metricsNN[2:7], function(x) as.numeric(as.character(x)))
metricsNN <- data.frame(lapply(metricsNN, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsNN)[1] <- ""

apa_table(
  metricsNN
  , caption = "Neural Network Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  , longtable = TRUE
  , landscape = FALSE
  , escape = FALSE
)
# Use kable to create a nicer table
# kable(metrics, caption = "Neural Network Predictive Metrics", 
#       col.names = c('Class', 'Precision', 'Recall', 'F1-Score', 'Support'), 
#       align = 'c') %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
#                 full_width = F, font_size = 12)


```

```{r svm100k, message=FALSE, warning=FALSE}

# Load necessary packages or install if not already installed
if (!require(knitr)) {
    install.packages("knitr")
    library(knitr)
}
library(kableExtra)
# Create a data frame with the classification report metrics
# metrics <- data.frame(
#   Class = c('0', '1', 'Accuracy', 'Macro Avg', 'Weighted Avg'),
#   Precision = c(0.71, 0.77, NA, 0.74, 0.75),
#   Recall = c(0.44, 0.92, NA, 0.68, 0.76),
#   `F1-Score` = c(0.54, 0.84, 0.76, 0.69, 0.74),
#   Support = c(11812, 24862, 36674, 36674, 36674)
# )

metrics<- read.csv("svm100k.csv")
svm100k<-data.frame(
  Algorithm = "svm100k",
  Model = "Support Vector Machines",
  `Sample Size`= 100000,
  `Number of Variables` = 105,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsSVM <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsSVM[2:7]<- mutate_all(metricsSVM[2:7], function(x) as.numeric(as.character(x)))
metricsSVM <- data.frame(lapply(metricsSVM, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsSVM)[1] <- ""

apa_table(
  metricsSVM
  , caption = "SVM Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  , longtable = TRUE
  , landscape = FALSE
  , escape = FALSE
)

# Use kable to create a nicer table
# kable(metrics, caption = "SVM Predictive Metrics", 
#       col.names = c('Class', 'Precision', 'Recall', 'F1-Score', 'Support'), 
#       align = 'c') %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
#                 full_width = F, font_size = 12)


```

```{r rf100k, message=FALSE, warning=FALSE}

# Load necessary packages or install if not already installed
if (!require(knitr)) {
    install.packages("knitr")
    library(knitr)
}
library(kableExtra)
# Create a data frame with the classification report metrics
# metrics <- data.frame(
#   Class = c('0', '1', 'Accuracy', 'Macro Avg', 'Weighted Avg'),
#   Precision = c(0.70, 0.77, NA, 0.73, 0.75),
#   Recall = c(0.43, 0.91, NA, 0.67, 0.76),
#   `F1-Score` = c(0.53, 0.83, 0.76, 0.68, 0.74),
#   Support = c(11812, 24862, 36674, 36674, 36674)
# )

metrics<- read.csv("rf100k.csv")
rf100k<-data.frame(
  Algorithm = "rf100k",
  Model = "Random Forest",
  `Sample Size`= 100000,
  `Number of Variables` = 105,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsRF <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsRF[2:7]<- mutate_all(metricsRF[2:7], function(x) as.numeric(as.character(x)))
metricsRF <- data.frame(lapply(metricsRF, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsRF)[1] <- ""

apa_table(
  metricsRF
  , caption = "Random Forest Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  , longtable = TRUE
  , landscape = FALSE
  , escape = FALSE
)
# Use kable to create a nicer table
# kable(metrics, caption = "Random Forest Predictive Metrics", 
#       col.names = c('Class', 'Precision', 'Recall', 'F1-Score', 'Support'), 
#       align = 'c') %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
#                 full_width = F, font_size = 12)


```

For all of these analyses, the dataset was split into a training dataset of 80,000 and a testing dataset of 20,000. All predictive performance metrics were obtained through cross-validation on the testing dataset. A logistic regression analysis was conducted to predict turnover intention based on the set of 104 FEVS predictors. Overall accuracy across categories was `r metricsLOGI$Accuracy[3]`, as can be seen in table \@ref(tab:logitable100k). The weighted average precision was `r metricsLOGI$Precision[3]`, with a recall of `r metricsLOGI$Recall[3]` and an F1 score of `r metricsLOGI$F1.score[3]`, suggesting a moderate level of prediction consistency across categories. 
The performance of a xgboosting model across categories was `r metricsXGB$Accuracy[3]`, as can be seen in table \@ref(tab:xgbtable100k). The weighted average precision was `r metricsXGB$Precision[4]`, with a recall of `r metricsXGB$Recall[4]` and an F1 score of `r metricsXGB$F1.score[4]`, suggesting moderate levels of predictive accuracy. 
The performance of a neural network model across categories was `r metricsNN$F1.score[3]`, as can be seen in table \@ref(tab:nn100k). The weighted average precision was `r metricsNN$Precision[4]`, with a recall of `r metricsNN$Recall[4]` and an F1 score of `r metricsNN$F1.score[4]`, suggesting moderate levels of predictive accuracy. 
The performance of a SVM model across categories was `r metricsSVM$F1.score[3]`, as can be seen in table \@ref(tab:svm100k). The weighted average precision was `r metricsSVM$Precision[4]`, with a recall of `r metricsSVM$Recall[4]` and an F1 score of `r metricsSVM$F1.score[4]`, suggesting moderate levels of predictive accuracy.
The performance of a random forest model across categories was `r metricsRF$F1.score[3]`, as can be seen in table \@ref(tab:rf100k). The weighted average precision was `r metricsRF$Precision[4]`, with a recall of `r metricsRF$Recall[4]` and an F1 score of `r metricsRF$F1.score[4]`, suggesting moderate levels of predictive accuracy. 

## Sample Size 10,000

```{r logitable10k, message=FALSE, warning=FALSE}
# Load necessary packages or install if not already installed
if (!require(knitr)) {
    install.packages("knitr")
    library(knitr)
}
library(kableExtra)
# Create a data frame with the classification report metrics
# metrics <- data.frame(
#   Class = c('0', '1', 'Accuracy', 'Macro Avg', 'Weighted Avg'),
#   Precision = c(0.72, 0.77, NA, 0.75, 0.76),
#   Recall = c(0.45, 0.91, NA, 0.68, 0.76),
#   `F1-Score` = c(0.55, 0.84, 0.76, 0.70, 0.74),
#   Support = c(1079, 2221, 3300, 3300, 3300)
# )

metrics<- read.csv("logi10k.csv")
logi10k<-data.frame(
  Algorithm = "logi10k",
  Model = "Logistic Regression",
  `Sample Size`= 10000,
  `Number of Variables` = 105,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsLOGI <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsLOGI[2:7]<- mutate_all(metricsLOGI[2:7], function(x) as.numeric(as.character(x)))
metricsLOGI <- data.frame(lapply(metricsLOGI, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsLOGI)[1] <- ""

apa_table(
  metricsLOGI
  , caption = "Logistic Regression Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  , longtable = TRUE
  , landscape = FALSE
  , escape = FALSE
)

# Use kable to create a nicer table
# kable(metrics, caption = "Logistic Regression Predictive Metrics", 
#       col.names = c('Class', 'Precision', 'Recall', 'F1-Score', 'Support'), 
#       align = 'c') %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
#                 full_width = F, font_size = 12)


```


```{r xgbtable10k, message=FALSE, warning=FALSE}

# Load necessary packages or install if not already installed
if (!require(knitr)) {
    install.packages("knitr")
    library(knitr)
}
library(kableExtra)
# Create a data frame with the classification report metrics
# metrics <- data.frame(
#   Class = c('0', '1', 'Accuracy', 'Macro Avg', 'Weighted Avg'),
#   Precision = c(0.71, 0.78, NA, 0.74, 0.76),
#   Recall = c(0.46, 0.91, NA, 0.68, 0.76),
#   `F1-Score` = c(0.56, 0.84, 0.76, 0.70, 0.75),
#   Support = c(1079, 2221, 3300, 3300, 3300)
# )

metrics<- read.csv("xgb10k.csv")
xgb10k<-data.frame(
  Algorithm = "xgb10k",
  Model = "xgboosting",
  `Sample Size`= 10000,
  `Number of Variables` = 105,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsXGB <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsXGB[2:7]<- mutate_all(metricsXGB[2:7], function(x) as.numeric(as.character(x)))
metricsXGB <- data.frame(lapply(metricsXGB, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsXGB)[1] <- ""

apa_table(
  metricsXGB
  , caption = "xgboosting Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  , longtable = TRUE
  , landscape = FALSE
  , escape = FALSE
)
# Use kable to create a nicer table
# kable(metrics, caption = "xgboosting Predictive Metrics", 
#       col.names = c('Class', 'Precision', 'Recall', 'F1-Score', 'Support'), 
#       align = 'c') %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
#                 full_width = F, font_size = 12)


```


```{r nn10k, message=FALSE, warning=FALSE}

# Load necessary packages or install if not already installed
if (!require(knitr)) {
    install.packages("knitr")
    library(knitr)
}
library(kableExtra)
# Create a data frame with the classification report metrics
# metrics <- data.frame(
#   Class = c('0', '1', 'Accuracy', 'Macro Avg', 'Weighted Avg'),
#   Precision = c(0.77, 0.75, NA, 0.76, 0.76),
#   Recall = c(0.35, 0.95, NA, 0.65, 0.75),
#   `F1-Score` = c(0.49, 0.84, 0.75, 0.66, 0.72),
#   Support = c(1079, 2221, 3300, 3300, 3300)
# )

metrics<- read.csv("nn10k.csv")
nn10k<-data.frame(
  Algorithm = "nn10k",
  Model = "Neural Network",
  `Sample Size`= 10000,
  `Number of Variables` = 105,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsNN <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsNN[2:7]<- mutate_all(metricsNN[2:7], function(x) as.numeric(as.character(x)))
metricsNN <- data.frame(lapply(metricsNN, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsNN)[1] <- ""

apa_table(
  metricsNN
  , caption = "Neural Network Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  , longtable = TRUE
  , landscape = FALSE
  , escape = FALSE
)

# # Use kable to create a nicer table
# kable(metrics, caption = "Neural Network Predictive Metrics", 
#       col.names = c('Class', 'Precision', 'Recall', 'F1-Score', 'Support'), 
#       align = 'c') %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
#                 full_width = F, font_size = 12)


```

Note: nn10k was a bit different from nn100k

```{r svm10k, message=FALSE, warning=FALSE}

# Load necessary packages or install if not already installed
if (!require(knitr)) {
    install.packages("knitr")
    library(knitr)
}
library(kableExtra)
# Create a data frame with the classification report metrics
# metrics <- data.frame(
#   Class = c('0', '1', 'Accuracy', 'Macro Avg', 'Weighted Avg'),
#   Precision = c(0.72, 0.77, NA, 0.75, 0.75),
#   Recall = c(0.44, 0.92, NA, 0.68, 0.76),
#   `F1-Score` = c(0.54, 0.84, 0.76, 0.69, 0.74),
#   Support = c(1079, 2221, 3300, 3300, 3300)
# )

metrics<- read.csv("svm10k.csv")
svm10k<-data.frame(
  Algorithm = "svm10k",
  Model = "Support Vector Machines",
  `Sample Size`= 10000,
  `Number of Variables` = 105,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsSVM <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsSVM[2:7]<- mutate_all(metricsSVM[2:7], function(x) as.numeric(as.character(x)))
metricsSVM <- data.frame(lapply(metricsSVM, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsSVM)[1] <- ""

apa_table(
  metricsSVM
  , caption = "SVM Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  , longtable = TRUE
  , landscape = FALSE
  , escape = FALSE
)
# Use kable to create a nicer table
# kable(metrics, caption = "SVM Predictive Metrics", 
#       col.names = c('Class', 'Precision', 'Recall', 'F1-Score', 'Support'), 
#       align = 'c') %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
#                 full_width = F, font_size = 12)


```

```{r rf10k, message=FALSE, warning=FALSE}

# Load necessary packages or install if not already installed
if (!require(knitr)) {
    install.packages("knitr")
    library(knitr)
}
library(kableExtra)
# Create a data frame with the classification report metrics
# metrics <- data.frame(
#   Class = c('0', '1', 'Accuracy', 'Macro Avg', 'Weighted Avg'),
#   Precision = c(0.70, 0.76, NA, 0.73, 0.74),
#   Recall = c(0.41, 0.91, NA, 0.66, 0.75),
#   `F1-Score` = c(0.52, 0.83, 0.75, 0.68, 0.73),
#   Support = c(1079, 2221, 3300, 3300, 3300)
# )

metrics<- read.csv("rf10k.csv")
rf10k<-data.frame(
  Algorithm = "rf10k",
  Model = "Random Forest",
  `Sample Size`= 10000,
  `Number of Variables` = 105,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsRF <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsRF[2:7]<- mutate_all(metricsRF[2:7], function(x) as.numeric(as.character(x)))
metricsRF <- data.frame(lapply(metricsRF, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsRF)[1] <- ""

apa_table(
  metricsRF
  , caption = "Random Forest Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  , longtable = TRUE
  , landscape = FALSE
  , escape = FALSE
)

# Use kable to create a nicer table
# kable(metrics, caption = "Random Forest Predictive Metrics", 
#       col.names = c('Class', 'Precision', 'Recall', 'F1-Score', 'Support'), 
#       align = 'c') %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
#                 full_width = F, font_size = 12)


```
For all of these analyses, the dataset was split into a training dataset of 8,000 and a testing dataset of 2,000. All predictive performance metrics were obtained through cross-validation on the testing dataset. A logistic regression analysis was conducted to predict turnover intention based on the set of 104 FEVS predictors. Overall accuracy across categories was `r metricsLOGI$Accuracy[3]`, as can be seen in Table \@ref(tab:logitable10k). The weighted average precision was `r metricsLOGI$Precision[3]`, with a recall of `r metricsLOGI$Recall[3]` and an F1 score of `r metricsLOGI$F1.score[3]`, suggesting a moderate level of prediction consistency across categories. 
The performance of a xgboosting model across categories was `r metricsXGB$Accuracy[3]`, as can be seen in table \@ref(tab:xgbtable10k). The weighted average precision was `r metricsXGB$Precision[4]`, with a recall of `r metricsXGB$Recall[4]` and an F1 score of `r metricsXGB$F1.score[4]`, suggesting moderate levels of predictive accuracy. 
The performance of a neural network model across categories was `r metricsNN$F1.score[3]`, as can be seen in table \@ref(tab:nn10k). The weighted average precision was `r metricsNN$Precision[4]`, with a recall of `r metricsNN$Recall[4]` and an F1 score of `r metricsNN$F1.score[4]`, suggesting moderate levels of predictive accuracy. 
The performance of a SVM model across categories was `r metricsSVM$F1.score[3]`, as can be seen in table \@ref(tab:svm10k). The weighted average precision was `r metricsSVM$Precision[4]`, with a recall of `r metricsSVM$Recall[4]` and an F1 score of `r metricsSVM$F1.score[4]`, suggesting moderate levels of predictive accuracy.
The performance of a random forest model across categories was `r metricsRF$F1.score[3]`, as can be seen in table \@ref(tab:rf10k). The weighted average precision was `r metricsRF$Precision[4]`, with a recall of `r metricsRF$Recall[4]` and an F1 score of `r metricsRF$F1.score[4]`, suggesting moderate levels of predictive accuracy.


## Sample Size 1,000

```{r logitable1k, message=FALSE, warning=FALSE}
# Load necessary packages or install if not already installed
if (!require(knitr)) {
    install.packages("knitr")
    library(knitr)
}
library(kableExtra)
# Create a data frame with the classification report metrics
# metrics <- data.frame(
#   Class = c('0', '1', 'Accuracy', 'Macro Avg', 'Weighted Avg'),
#   Precision = c(0.70, 0.77, NA, 0.74, 0.75),
#   Recall = c(0.45, 0.90, NA, 0.68, 0.75),
#   `F1-Score` = c(0.55, 0.83, 0.75, 0.69, 0.74),
#   Support = c(110, 220, 330, 330, 330)
# )

metrics<- read.csv("logi1k.csv")
logi1k<-data.frame(
  Algorithm = "logi1k",
  Model = "Logistic Regression",
  `Sample Size`= 1000,
  `Number of Variables` = 105,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsLOGI <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsLOGI[2:7]<- mutate_all(metricsLOGI[2:7], function(x) as.numeric(as.character(x)))
metricsLOGI <- data.frame(lapply(metricsLOGI, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsLOGI)[1] <- ""

apa_table(
  metricsLOGI
  , caption = "Logistic Regression Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  , longtable = TRUE
  , landscape = FALSE
  , escape = FALSE
)

# Use kable to create a nicer table
# kable(metrics, caption = "Logistic Regression Predictive Metrics", 
#       col.names = c('Class', 'Precision', 'Recall', 'F1-Score', 'Support'), 
#       align = 'c') %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
#                 full_width = F, font_size = 12)


```

```{r xgbtable1k, message=FALSE, warning=FALSE}

# Load necessary packages or install if not already installed
if (!require(knitr)) {
    install.packages("knitr")
    library(knitr)
}
library(kableExtra)
# Create a data frame with the classification report metrics
# metrics <- data.frame(
#   Class = c('0', '1', 'Accuracy', 'Macro Avg', 'Weighted Avg'),
#   Precision = c(0.72, 0.76, NA, 0.74, 0.75),
#   Recall = c(0.44, 0.91, NA, 0.68, 0.75),
#   `F1-Score` = c(0.54, 0.83, 0.75, 0.69, 0.74),
#   Support = c(110, 220, 330, 330, 330)
# )

metrics<- read.csv("xgb1k.csv")
xgb1k<-data.frame(
  Algorithm = "xgb1k",
  Model = "xgboosting",
  `Sample Size`= 1000,
  `Number of Variables` = 105,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsXGB <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsXGB[2:7]<- mutate_all(metricsXGB[2:7], function(x) as.numeric(as.character(x)))
metricsXGB <- data.frame(lapply(metricsXGB, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsXGB)[1] <- ""

apa_table(
  metricsXGB
  , caption = "xgboosting Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  , longtable = TRUE
  , landscape = FALSE
  , escape = FALSE
)

# Use kable to create a nicer table
# kable(metrics, caption = "xgboosting Predictive Metrics", 
#       col.names = c('Class', 'Precision', 'Recall', 'F1-Score', 'Support'), 
#       align = 'c') %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
#                 full_width = F, font_size = 12)


```

```{r nn1k, message=FALSE, warning=FALSE}

# Load necessary packages or install if not already installed
if (!require(knitr)) {
    install.packages("knitr")
    library(knitr)
}
library(kableExtra)
# Create a data frame with the classification report metrics
# metrics <- data.frame(
#   Class = c('0', '1', 'Accuracy', 'Macro Avg', 'Weighted Avg'),
#   Precision = c(0.66, 0.77, NA, 0.71, 0.73),
#   Recall = c(0.46, 0.88, NA, 0.67, 0.74),
#   `F1-Score` = c(0.55, 0.82, 0.74, 0.68, 0.73),
#   Support = c(110, 220, 330, 330, 330)
# )

metrics<- read.csv("nn1k.csv")
nn1k<-data.frame(
  Algorithm = "nn1k",
  Model = "Neural Network",
  `Sample Size`= 1000,
  `Number of Variables` = 105,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsNN <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsNN[2:7]<- mutate_all(metricsNN[2:7], function(x) as.numeric(as.character(x)))
metricsNN <- data.frame(lapply(metricsNN, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsNN)[1] <- ""

apa_table(
  metricsNN
  , caption = "Neural Network Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  , longtable = TRUE
  , landscape = FALSE
  , escape = FALSE
)

# Use kable to create a nicer table
# kable(metrics, caption = "Neural Network Predictive Metrics", 
#       col.names = c('Class', 'Precision', 'Recall', 'F1-Score', 'Support'), 
#       align = 'c') %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
#                 full_width = F, font_size = 12)


```

```{r svm1k, message=FALSE, warning=FALSE}

# Load necessary packages or install if not already installed
if (!require(knitr)) {
    install.packages("knitr")
    library(knitr)
}
library(kableExtra)
# Create a data frame with the classification report metrics
# metrics <- data.frame(
#   Class = c('0', '1', 'Accuracy', 'Macro Avg', 'Weighted Avg'),
#   Precision = c(0.71, 0.77, NA, 0.74, 0.75),
#   Recall = c(0.45, 0.91, NA, 0.68, 0.76),
#   `F1-Score` = c(0.56, 0.83, 0.76, 0.69, 0.74),
#   Support = c(110, 220, 330, 330, 330)
# )

metrics<- read.csv("svm1k.csv")
svm1k<-data.frame(
  Algorithm = "svm1k",
  Model = "Support Vector Machines",
  `Sample Size`= 1000,
  `Number of Variables` = 105,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsSVM <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsSVM[2:7]<- mutate_all(metricsSVM[2:7], function(x) as.numeric(as.character(x)))
metricsSVM <- data.frame(lapply(metricsSVM, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsSVM)[1] <- ""

apa_table(
  metricsSVM
  , caption = "SVM Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  , longtable = TRUE
  , landscape = FALSE
  , escape = FALSE
)

# Use kable to create a nicer table
# kable(metrics, caption = "SVM Predictive Metrics", 
#       col.names = c('Class', 'Precision', 'Recall', 'F1-Score', 'Support'), 
#       align = 'c') %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
#                 full_width = F, font_size = 12)


```

```{r rf1k, message=FALSE, warning=FALSE}

# Load necessary packages or install if not already installed
if (!require(knitr)) {
    install.packages("knitr")
    library(knitr)
}
library(kableExtra)
# Create a data frame with the classification report metrics
# metrics <- data.frame(
#   Class = c('0', '1', 'Accuracy', 'Macro Avg', 'Weighted Avg'),
#   Precision = c(0.69, 0.76, NA, 0.73, 0.74),
#   Recall = c(0.43, 0.90, NA, 0.67, 0.75),
#   `F1-Score` = c(0.53, 0.83, 0.75, 0.68, 0.73),
#   Support = c(110, 220, 330, 330, 330)
# )

metrics<- read.csv("rf1k.csv")
rf1k<-data.frame(
  Algorithm = "rf1k",
  Model = "Random Forest",
  `Sample Size`= 1000,
  `Number of Variables` = 105,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsNN <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsNN[2:7]<- mutate_all(metricsNN[2:7], function(x) as.numeric(as.character(x)))
metricsNN <- data.frame(lapply(metricsNN, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsRF)[1] <- ""

apa_table(
  metricsRF
  , caption = "Random Forest Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  , longtable = TRUE
  , landscape = FALSE
  , escape = FALSE
)

# Use kable to create a nicer table
# kable(metrics, caption = "Random Forest Predictive Metrics", 
#       col.names = c('Class', 'Precision', 'Recall', 'F1-Score', 'Support'), 
#       align = 'c') %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
#                 full_width = F, font_size = 12)


```


For all of these analyses, the dataset was split into a training dataset of 800 and a testing dataset of 200. All predictive performance metrics were obtained through cross-validation on the testing dataset. A logistic regression analysis was conducted to predict turnover intention based on the set of 104 FEVS predictors. Overall accuracy across categories was `r metricsLOGI$Accuracy[3]`, as can be seen in Table \@ref(tab:logitable1k). The weighted average precision was `r metricsLOGI$Precision[3]`, with a recall of `r metricsLOGI$Recall[3]` and an F1 score of `r metricsLOGI$F1.score[3]`, suggesting a moderate level of prediction consistency across categories. 
The performance of a xgboosting model across categories was `r metricsXGB$Accuracy[3]`, as can be seen in table \@ref(tab:xgbtable1k). The weighted average precision was `r metricsXGB$Precision[4]`, with a recall of `r metricsXGB$Recall[4]` and an F1 score of `r metricsXGB$F1.score[4]`, suggesting moderate levels of predictive accuracy. 
The performance of a neural network model across categories was `r metricsNN$F1.score[3]`, as can be seen in table \@ref(tab:nn1k). The weighted average precision was `r metricsNN$Precision[4]`, with a recall of `r metricsNN$Recall[4]` and an F1 score of `r metricsNN$F1.score[4]`, suggesting moderate levels of predictive accuracy. 
The performance of a SVM model across categories was `r metricsSVM$F1.score[3]`, as can be seen in table \@ref(tab:svm1k). The weighted average precision was `r metricsSVM$Precision[4]`, with a recall of `r metricsSVM$Recall[4]` and an F1 score of `r metricsSVM$F1.score[4]`, suggesting moderate levels of predictive accuracy.
The performance of a random forest model across categories was `r metricsRF$F1.score[3]`, as can be seen in table \@ref(tab:rf1k). The weighted average precision was `r metricsRF$Precision[4]`, with a recall of `r metricsRF$Recall[4]` and an F1 score of `r metricsRF$F1.score[4]`, suggesting moderate levels of predictive accuracy.


## Sample Size 100


```{r logitable200, message=FALSE, warning=FALSE}
# Load necessary packages or install if not already installed
if (!require(knitr)) {
    install.packages("knitr")
    library(knitr)
}
library(kableExtra)
# Create a data frame with the classification report metrics
# metrics <- data.frame(
#   Class = c('0', '1', 'Accuracy', 'Macro Avg', 'Weighted Avg'),
#   Precision = c(0.59, 0.84, NA, 0.71, 0.77),
#   Recall = c(0.56, 0.85, NA, 0.70, 0.77),
#   `F1-Score` = c(0.57, 0.85, 0.77, 0.71, 0.77),
#   Support = c(18, 48, 66, 66, 66)
# )

metrics<- read.csv("logi100.csv")
logi100<-data.frame(
  Algorithm = "logi100",
  Model = "Logistic Regression",
  `Sample Size`= 100,
  `Number of Variables` = 105,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsLOGI <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsLOGI[2:7]<- mutate_all(metricsLOGI[2:7], function(x) as.numeric(as.character(x)))
metricsLOGI <- data.frame(lapply(metricsLOGI, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsLOGI)[1] <- ""

apa_table(
  metricsLOGI
  , caption = "Logistic Regression Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  , longtable = TRUE
  , landscape = FALSE
  , escape = FALSE
)

# Use kable to create a nicer table
# kable(metrics, caption = "Logistic Regression Predictive Metrics", 
#       col.names = c('Class', 'Precision', 'Recall', 'F1-Score', 'Support'), 
#       align = 'c') %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
#                 full_width = F, font_size = 12)


```

```{r xgbtable200, message=FALSE, warning=FALSE}

# Load necessary packages or install if not already installed
if (!require(knitr)) {
    install.packages("knitr")
    library(knitr)
}
library(kableExtra)
# Create a data frame with the classification report metrics
# metrics <- data.frame(
#   Class = c('0', '1', 'Accuracy', 'Macro Avg', 'Weighted Avg'),
#   Precision = c(0.67, 0.79, NA, 0.73, 0.76),
#   Recall = c(0.33, 0.94, NA, 0.64, 0.77),
#   `F1-Score` = c(0.44, 0.86, 0.77, 0.65, 0.74),
#   Support = c(18, 48, 66, 66, 66)
# )

metrics<- read.csv("xgb100.csv")
xgb100<-data.frame(
  Algorithm = "xgb100",
  Model = "xgboosting",
  `Sample Size`= 100,
  `Number of Variables` = 105,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsXGB <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsXGB[2:7]<- mutate_all(metricsXGB[2:7], function(x) as.numeric(as.character(x)))
metricsXGB <- data.frame(lapply(metricsXGB, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsXGB)[1] <- ""

apa_table(
  metricsXGB
  , caption = "xgboosting Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  , longtable = TRUE
  , landscape = FALSE
  , escape = FALSE
)

# Use kable to create a nicer table
# kable(metrics, caption = "xgboosting Predictive Metrics", 
#       col.names = c('Class', 'Precision', 'Recall', 'F1-Score', 'Support'), 
#       align = 'c') %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
#                 full_width = F, font_size = 12)


```

```{r nn200, message=FALSE, warning=FALSE}

# Load necessary packages or install if not already installed
if (!require(knitr)) {
    install.packages("knitr")
    library(knitr)
}
library(kableExtra)
# Create a data frame with the classification report metrics
# metrics <- data.frame(
#   Class = c('0', '1', 'Accuracy', 'Macro Avg', 'Weighted Avg'),
#   Precision = c(0.47, 0.81, NA, 0.64, 0.72),
#   Recall = c(0.50, 0.79, NA, 0.65, 0.71),
#   `F1-Score` = c(0.49, 0.80, 0.71, 0.64, 0.71),
#   Support = c(18, 48, 66, 66, 66)
# )

metrics<- read.csv("nn100.csv")
nn100<-data.frame(
  Algorithm = "nn100",
  Model = "Neural Network",
  `Sample Size`= 100,
  `Number of Variables` = 105,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsNN <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsNN[2:7]<- mutate_all(metricsNN[2:7], function(x) as.numeric(as.character(x)))
metricsNN <- data.frame(lapply(metricsNN, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsNN)[1] <- ""

apa_table(
  metricsNN
  , caption = "Neural Network Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  , longtable = TRUE
  , landscape = FALSE
  , escape = FALSE
)

# Use kable to create a nicer table
# kable(metrics, caption = "Neural Network Predictive Metrics", 
#       col.names = c('Class', 'Precision', 'Recall', 'F1-Score', 'Support'), 
#       align = 'c') %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
#                 full_width = F, font_size = 12)


```

```{r svm200, message=FALSE, warning=FALSE}

# Load necessary packages or install if not already installed
if (!require(knitr)) {
    install.packages("knitr")
    library(knitr)
}
library(kableExtra)
# Create a data frame with the classification report metrics
# metrics <- data.frame(
#   Class = c('0', '1', 'Accuracy', 'Macro Avg', 'Weighted Avg'),
#   Precision = c(0.58, 0.85, NA, 0.72, 0.78),
#   Recall = c(0.61, 0.83, NA, 0.72, 0.77),
#   `F1-Score` = c(0.59, 0.84, 0.77, 0.72, 0.77),
#   Support = c(18, 48, 66, 66, 66)
# )

metrics<- read.csv("svm100.csv")
svm100<-data.frame(
  Algorithm = "svm100",
  Model = "Support Vector Machines",
  `Sample Size`= 100,
  `Number of Variables` = 105,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsSVM <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsSVM[2:7]<- mutate_all(metricsSVM[2:7], function(x) as.numeric(as.character(x)))
metricsSVM <- data.frame(lapply(metricsSVM, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsSVM)[1] <- ""

apa_table(
  metricsSVM
  , caption = "SVM Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  , longtable = TRUE
  , landscape = FALSE
  , escape = FALSE
)

# Use kable to create a nicer table
# kable(metrics, caption = "SVM Predictive Metrics", 
#       col.names = c('Class', 'Precision', 'Recall', 'F1-Score', 'Support'), 
#       align = 'c') %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
#                 full_width = F, font_size = 12)


```

```{r rf200, message=FALSE, warning=FALSE}

# Load necessary packages or install if not already installed
if (!require(knitr)) {
    install.packages("knitr")
    library(knitr)
}
library(kableExtra)
# Create a data frame with the classification report metrics
# metrics <- data.frame(
#   Class = c('0', '1', 'Accuracy', 'Macro Avg', 'Weighted Avg'),
#   Precision = c(0.59, 0.84, NA, 0.71, 0.77),
#   Recall = c(0.56, 0.85, NA, 0.70, 0.77),
#   `F1-Score` = c(0.57, 0.85, 0.77, 0.71, 0.77),
#   Support = c(18, 48, 66, 66, 66)
# )

metrics<- read.csv("rf100.csv")
rf100<-data.frame(
  Algorithm = "rf100",
  Model = "Random Forest",
  `Sample Size`= 100,
  `Number of Variables` = 105,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsRF <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsRF[2:7]<- mutate_all(metricsRF[2:7], function(x) as.numeric(as.character(x)))
metricsRF <- data.frame(lapply(metricsRF, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsRF)[1] <- ""


apa_table(
  metricsRF
  , caption = "Random Forest Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  , longtable = TRUE
  , landscape = FALSE
  , escape = FALSE
)

# Use kable to create a nicer table
# kable(metrics, caption = "Random Forest Predictive Metrics", 
#       col.names = c('Class', 'Precision', 'Recall', 'F1-Score', 'Support'), 
#       align = 'c') %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
#                 full_width = F, font_size = 12)


```


For all of these analyses, the dataset was split into a training dataset of 80 and a testing dataset of 20. All predictive performance metrics were obtained through cross-validation on the testing dataset. A logistic regression analysis was conducted to predict turnover intention based on the set of 104 FEVS predictors. Overall accuracy across categories was `r metricsLOGI$Accuracy[3]`, as can be seen in Table \@ref(tab:logitable200). The weighted average precision was `r metricsLOGI$Precision[3]`, with a recall of `r metricsLOGI$Recall[3]` and an F1 score of `r metricsLOGI$F1.score[3]`, suggesting a moderate level of prediction consistency across categories. 
The performance of a xgboosting model across categories was `r metricsXGB$Accuracy[3]`, as can be seen in table \@ref(tab:xgbtable200). The weighted average precision was `r metricsXGB$Precision[4]`, with a recall of `r metricsXGB$Recall[4]` and an F1 score of `r metricsXGB$F1.score[4]`, suggesting moderate levels of predictive accuracy. 
The performance of a neural network model across categories was `r metricsNN$F1.score[3]`, as can be seen in table \@ref(tab:nn200). The weighted average precision was `r metricsNN$Precision[4]`, with a recall of `r metricsNN$Recall[4]` and an F1 score of `r metricsNN$F1.score[4]`, suggesting moderate levels of predictive accuracy. 
The performance of a SVM model across categories was `r metricsSVM$F1.score[3]`, as can be seen in table \@ref(tab:svm200). The weighted average precision was `r metricsSVM$Precision[4]`, with a recall of `r metricsSVM$Recall[4]` and an F1 score of `r metricsSVM$F1.score[4]`, suggesting moderate levels of predictive accuracy.
The performance of a random forest model across categories was `r metricsRF$F1.score[3]`, as can be seen in table \@ref(tab:rf200). The weighted average precision was `r metricsRF$Precision[4]`, with a recall of `r metricsRF$Recall[4]` and an F1 score of `r metricsRF$F1.score[4]`, suggesting moderate levels of predictive accuracy.




## Number of Variables 75

```{r logitable75, message=FALSE, warning=FALSE}
# Load necessary packages or install if not already installed
if (!require(knitr)) {
    install.packages("knitr")
    library(knitr)
}
library(kableExtra)
# Create a data frame with the classification report metrics
# metrics <- data.frame(
#   Class = c('0', '1', 'Accuracy', 'Macro Avg', 'Weighted Avg'),
#   Precision = c(0.59, 0.84, NA, 0.71, 0.77),
#   Recall = c(0.56, 0.85, NA, 0.70, 0.77),
#   `F1-Score` = c(0.57, 0.85, 0.77, 0.71, 0.77),
#   Support = c(18, 48, 66, 66, 66)
# )

metrics<- read.csv("logi75.csv")
logi75<-data.frame(
  Algorithm = "logi75",
  Model = "Logistic Regression",
  `Sample Size`= 100000,
  `Number of Variables` = 75,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsLOGI <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsLOGI[2:7]<- mutate_all(metricsLOGI[2:7], function(x) as.numeric(as.character(x)))
metricsLOGI <- data.frame(lapply(metricsLOGI, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsLOGI)[1] <- ""

apa_table(
  metricsLOGI
  , caption = "Logistic Regression Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  , longtable = TRUE
  , landscape = FALSE
  , escape = FALSE
)

# Use kable to create a nicer table
# kable(metrics, caption = "Logistic Regression Predictive Metrics", 
#       col.names = c('Class', 'Precision', 'Recall', 'F1-Score', 'Support'), 
#       align = 'c') %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
#                 full_width = F, font_size = 12)


```

```{r xgbtable75, message=FALSE, warning=FALSE}

# Load necessary packages or install if not already installed
if (!require(knitr)) {
    install.packages("knitr")
    library(knitr)
}
library(kableExtra)
# Create a data frame with the classification report metrics
# metrics <- data.frame(
#   Class = c('0', '1', 'Accuracy', 'Macro Avg', 'Weighted Avg'),
#   Precision = c(0.67, 0.79, NA, 0.73, 0.76),
#   Recall = c(0.33, 0.94, NA, 0.64, 0.77),
#   `F1-Score` = c(0.44, 0.86, 0.77, 0.65, 0.74),
#   Support = c(18, 48, 66, 66, 66)
# )

metrics<- read.csv("xgb75.csv")
xgb75<-data.frame(
  Algorithm = "xgb75",
  Model = "xgboosting",
  `Sample Size`= 100000,
  `Number of Variables` = 75,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsXGB <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsXGB[2:7]<- mutate_all(metricsXGB[2:7], function(x) as.numeric(as.character(x)))
metricsXGB <- data.frame(lapply(metricsXGB, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsXGB)[1] <- ""

apa_table(
  metricsXGB
  , caption = "xgboosting Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  , longtable = TRUE
  , landscape = FALSE
  , escape = FALSE
)

# Use kable to create a nicer table
# kable(metrics, caption = "xgboosting Predictive Metrics", 
#       col.names = c('Class', 'Precision', 'Recall', 'F1-Score', 'Support'), 
#       align = 'c') %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
#                 full_width = F, font_size = 12)


```

```{r nn75, message=FALSE, warning=FALSE}

# Load necessary packages or install if not already installed
if (!require(knitr)) {
    install.packages("knitr")
    library(knitr)
}
library(kableExtra)
# Create a data frame with the classification report metrics
# metrics <- data.frame(
#   Class = c('0', '1', 'Accuracy', 'Macro Avg', 'Weighted Avg'),
#   Precision = c(0.47, 0.81, NA, 0.64, 0.72),
#   Recall = c(0.50, 0.79, NA, 0.65, 0.71),
#   `F1-Score` = c(0.49, 0.80, 0.71, 0.64, 0.71),
#   Support = c(18, 48, 66, 66, 66)
# )

metrics<- read.csv("nn75.csv")
nn75<-data.frame(
  Algorithm = "nn75",
  Model = "Neural Network",
  `Sample Size`= 100000,
  `Number of Variables` = 75,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsNN <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsNN[2:7]<- mutate_all(metricsNN[2:7], function(x) as.numeric(as.character(x)))
metricsNN <- data.frame(lapply(metricsNN, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsNN)[1] <- ""

apa_table(
  metricsNN
  , caption = "Neural Network Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  , longtable = TRUE
  , landscape = FALSE
  , escape = FALSE
)

# Use kable to create a nicer table
# kable(metrics, caption = "Neural Network Predictive Metrics", 
#       col.names = c('Class', 'Precision', 'Recall', 'F1-Score', 'Support'), 
#       align = 'c') %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
#                 full_width = F, font_size = 12)


```

```{r svm75, message=FALSE, warning=FALSE}

# Load necessary packages or install if not already installed
if (!require(knitr)) {
    install.packages("knitr")
    library(knitr)
}
library(kableExtra)
# Create a data frame with the classification report metrics
# metrics <- data.frame(
#   Class = c('0', '1', 'Accuracy', 'Macro Avg', 'Weighted Avg'),
#   Precision = c(0.58, 0.85, NA, 0.72, 0.78),
#   Recall = c(0.61, 0.83, NA, 0.72, 0.77),
#   `F1-Score` = c(0.59, 0.84, 0.77, 0.72, 0.77),
#   Support = c(18, 48, 66, 66, 66)
# )

metrics<- read.csv("svm75.csv")
svm75<-data.frame(
  Algorithm = "svm75",
  Model = "Support Vector Machines",
  `Sample Size`= 100000,
  `Number of Variables` = 75,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsSVM <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsSVM[2:7]<- mutate_all(metricsSVM[2:7], function(x) as.numeric(as.character(x)))
metricsSVM <- data.frame(lapply(metricsSVM, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsSVM)[1] <- ""

apa_table(
  metricsSVM
  , caption = "SVM Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  , longtable = TRUE
  , landscape = FALSE
  , escape = FALSE
)

# Use kable to create a nicer table
# kable(metrics, caption = "SVM Predictive Metrics", 
#       col.names = c('Class', 'Precision', 'Recall', 'F1-Score', 'Support'), 
#       align = 'c') %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
#                 full_width = F, font_size = 12)


```

```{r rf75, message=FALSE, warning=FALSE}

# Load necessary packages or install if not already installed
if (!require(knitr)) {
    install.packages("knitr")
    library(knitr)
}
library(kableExtra)
# Create a data frame with the classification report metrics
# metrics <- data.frame(
#   Class = c('0', '1', 'Accuracy', 'Macro Avg', 'Weighted Avg'),
#   Precision = c(0.59, 0.84, NA, 0.71, 0.77),
#   Recall = c(0.56, 0.85, NA, 0.70, 0.77),
#   `F1-Score` = c(0.57, 0.85, 0.77, 0.71, 0.77),
#   Support = c(18, 48, 66, 66, 66)
# )

metrics<- read.csv("rf75.csv")
rf75<-data.frame(
  Algorithm = "rf75",
  Model = "Random Forest",
  `Sample Size`= 100000,
  `Number of Variables` = 75,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsRF <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsRF[2:7]<- mutate_all(metricsRF[2:7], function(x) as.numeric(as.character(x)))
metricsRF <- data.frame(lapply(metricsRF, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsRF)[1] <- ""


apa_table(
  metricsRF
  , caption = "Random Forest Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  , longtable = TRUE
  , landscape = FALSE
  , escape = FALSE
)

# Use kable to create a nicer table
# kable(metrics, caption = "Random Forest Predictive Metrics", 
#       col.names = c('Class', 'Precision', 'Recall', 'F1-Score', 'Support'), 
#       align = 'c') %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
#                 full_width = F, font_size = 12)


```


For all of these analyses, the dataset was split into a training dataset of 80,000 and a testing dataset of 20,000. All predictive performance metrics were obtained through cross-validation on the testing dataset. A logistic regression analysis was conducted to predict turnover intention based on the set of FEVS predictors. From these predictors, 75 were randomly chosen to conduct the analyses. Overall accuracy across categories was `r metricsLOGI$Accuracy[3]`, as can be seen in Table \@ref(tab:logitable75). The weighted average precision was `r metricsLOGI$Precision[3]`, with a recall of `r metricsLOGI$Recall[3]` and an F1 score of `r metricsLOGI$F1.score[3]`, suggesting a moderate level of prediction consistency across categories. 
The performance of a xgboosting model across categories was `r metricsXGB$Accuracy[3]`, as can be seen in table \@ref(tab:xgbtable75). The weighted average precision was `r metricsXGB$Precision[4]`, with a recall of `r metricsXGB$Recall[4]` and an F1 score of `r metricsXGB$F1.score[4]`, suggesting moderate levels of predictive accuracy. 
The performance of a neural network model across categories was `r metricsNN$F1.score[3]`, as can be seen in table \@ref(tab:nn75). The weighted average precision was `r metricsNN$Precision[4]`, with a recall of `r metricsNN$Recall[4]` and an F1 score of `r metricsNN$F1.score[4]`, suggesting moderate levels of predictive accuracy. 
The performance of a SVM model across categories was `r metricsSVM$F1.score[3]`, as can be seen in table \@ref(tab:svm75). The weighted average precision was `r metricsSVM$Precision[4]`, with a recall of `r metricsSVM$Recall[4]` and an F1 score of `r metricsSVM$F1.score[4]`, suggesting moderate levels of predictive accuracy.
The performance of a random forest model across categories was `r metricsRF$F1.score[3]`, as can be seen in table \@ref(tab:rf75). The weighted average precision was `r metricsRF$Precision[4]`, with a recall of `r metricsRF$Recall[4]` and an F1 score of `r metricsRF$F1.score[4]`, suggesting moderate levels of predictive accuracy.




## Number of Variables 40

```{r logitable40, message=FALSE, warning=FALSE}
# Load necessary packages or install if not already installed
if (!require(knitr)) {
    install.packages("knitr")
    library(knitr)
}
library(kableExtra)
# Create a data frame with the classification report metrics
# metrics <- data.frame(
#   Class = c('0', '1', 'Accuracy', 'Macro Avg', 'Weighted Avg'),
#   Precision = c(0.59, 0.84, NA, 0.71, 0.77),
#   Recall = c(0.56, 0.85, NA, 0.70, 0.77),
#   `F1-Score` = c(0.57, 0.85, 0.77, 0.71, 0.77),
#   Support = c(18, 48, 66, 66, 66)
# )

metrics<- read.csv("logi40.csv")
logi40<-data.frame(
  Algorithm = "logi40",
  Model = "Logistic Regression",
  `Sample Size`= 100000,
  `Number of Variables` = 40,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsLOGI <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsLOGI[2:7]<- mutate_all(metricsLOGI[2:7], function(x) as.numeric(as.character(x)))
metricsLOGI <- data.frame(lapply(metricsLOGI, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsLOGI)[1] <- ""

apa_table(
  metricsLOGI
  , caption = "Logistic Regression Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  , longtable = TRUE
  , landscape = FALSE
  , escape = FALSE
)

# Use kable to create a nicer table
# kable(metrics, caption = "Logistic Regression Predictive Metrics", 
#       col.names = c('Class', 'Precision', 'Recall', 'F1-Score', 'Support'), 
#       align = 'c') %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
#                 full_width = F, font_size = 12)


```

```{r xgbtable40, message=FALSE, warning=FALSE}

# Load necessary packages or install if not already installed
if (!require(knitr)) {
    install.packages("knitr")
    library(knitr)
}
library(kableExtra)
# Create a data frame with the classification report metrics
# metrics <- data.frame(
#   Class = c('0', '1', 'Accuracy', 'Macro Avg', 'Weighted Avg'),
#   Precision = c(0.67, 0.79, NA, 0.73, 0.76),
#   Recall = c(0.33, 0.94, NA, 0.64, 0.77),
#   `F1-Score` = c(0.44, 0.86, 0.77, 0.65, 0.74),
#   Support = c(18, 48, 66, 66, 66)
# )

metrics<- read.csv("xgb40.csv")
xgb40<-data.frame(
  Algorithm = "xgb40",
  Model = "xgboosting",
  `Sample Size`= 100000,
  `Number of Variables` = 40,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsXGB <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsXGB[2:7]<- mutate_all(metricsXGB[2:7], function(x) as.numeric(as.character(x)))
metricsXGB <- data.frame(lapply(metricsXGB, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsXGB)[1] <- ""

apa_table(
  metricsXGB
  , caption = "xgboosting Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  , longtable = TRUE
  , landscape = FALSE
  , escape = FALSE
)

# Use kable to create a nicer table
# kable(metrics, caption = "xgboosting Predictive Metrics", 
#       col.names = c('Class', 'Precision', 'Recall', 'F1-Score', 'Support'), 
#       align = 'c') %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
#                 full_width = F, font_size = 12)


```

```{r nn40, message=FALSE, warning=FALSE}

# Load necessary packages or install if not already installed
if (!require(knitr)) {
    install.packages("knitr")
    library(knitr)
}
library(kableExtra)
# Create a data frame with the classification report metrics
# metrics <- data.frame(
#   Class = c('0', '1', 'Accuracy', 'Macro Avg', 'Weighted Avg'),
#   Precision = c(0.47, 0.81, NA, 0.64, 0.72),
#   Recall = c(0.50, 0.79, NA, 0.65, 0.71),
#   `F1-Score` = c(0.49, 0.80, 0.71, 0.64, 0.71),
#   Support = c(18, 48, 66, 66, 66)
# )

metrics<- read.csv("nn40.csv")
nn40<-data.frame(
  Algorithm = "nn40",
  Model = "Neural Network",
  `Sample Size`= 100000,
  `Number of Variables` = 40,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsNN <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsNN[2:7]<- mutate_all(metricsNN[2:7], function(x) as.numeric(as.character(x)))
metricsNN <- data.frame(lapply(metricsNN, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsNN)[1] <- ""

apa_table(
  metricsNN
  , caption = "Neural Network Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  , longtable = TRUE
  , landscape = FALSE
  , escape = FALSE
)

# Use kable to create a nicer table
# kable(metrics, caption = "Neural Network Predictive Metrics", 
#       col.names = c('Class', 'Precision', 'Recall', 'F1-Score', 'Support'), 
#       align = 'c') %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
#                 full_width = F, font_size = 12)


```

```{r svm40, message=FALSE, warning=FALSE}

# Load necessary packages or install if not already installed
if (!require(knitr)) {
    install.packages("knitr")
    library(knitr)
}
library(kableExtra)
# Create a data frame with the classification report metrics
# metrics <- data.frame(
#   Class = c('0', '1', 'Accuracy', 'Macro Avg', 'Weighted Avg'),
#   Precision = c(0.58, 0.85, NA, 0.72, 0.78),
#   Recall = c(0.61, 0.83, NA, 0.72, 0.77),
#   `F1-Score` = c(0.59, 0.84, 0.77, 0.72, 0.77),
#   Support = c(18, 48, 66, 66, 66)
# )

metrics<- read.csv("svm40.csv")
svm40<-data.frame(
  Algorithm = "svm40",
  Model = "Support Vector Machines",
  `Sample Size`= 100000,
  `Number of Variables` = 40,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsSVM <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsSVM[2:7]<- mutate_all(metricsSVM[2:7], function(x) as.numeric(as.character(x)))
metricsSVM <- data.frame(lapply(metricsSVM, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsSVM)[1] <- ""

apa_table(
  metricsSVM
  , caption = "SVM Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  , longtable = TRUE
  , landscape = FALSE
  , escape = FALSE
)

# Use kable to create a nicer table
# kable(metrics, caption = "SVM Predictive Metrics", 
#       col.names = c('Class', 'Precision', 'Recall', 'F1-Score', 'Support'), 
#       align = 'c') %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
#                 full_width = F, font_size = 12)


```

```{r rf40, message=FALSE, warning=FALSE}

# Load necessary packages or install if not already installed
if (!require(knitr)) {
    install.packages("knitr")
    library(knitr)
}
library(kableExtra)
# Create a data frame with the classification report metrics
# metrics <- data.frame(
#   Class = c('0', '1', 'Accuracy', 'Macro Avg', 'Weighted Avg'),
#   Precision = c(0.59, 0.84, NA, 0.71, 0.77),
#   Recall = c(0.56, 0.85, NA, 0.70, 0.77),
#   `F1-Score` = c(0.57, 0.85, 0.77, 0.71, 0.77),
#   Support = c(18, 48, 66, 66, 66)
# )

metrics<- read.csv("rf40.csv")
rf40<-data.frame(
  Algorithm = "rf40",
  Model = "Random Forest",
  `Sample Size`= 100000,
  `Number of Variables` = 40,
  `AUC Score` = round(metrics$AUC[1], 3)
)

# Convert numeric values to text to handle NA values
metricsRF <- data.frame(lapply(metrics, function(x) ifelse(is.na(x), "", x)), stringsAsFactors = FALSE)
metricsRF[2:7]<- mutate_all(metricsRF[2:7], function(x) as.numeric(as.character(x)))
metricsRF <- data.frame(lapply(metricsRF, function(x) {
  if(is.numeric(x)) round(x, 3) else x
}))

# Adjust column names for the table
colnames(metricsRF)[1] <- ""


apa_table(
  metricsRF
  , caption = "Random Forest Predictive Metrics"
  , note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  , longtable = TRUE
  , landscape = FALSE
  , escape = FALSE
)

# Use kable to create a nicer table
# kable(metrics, caption = "Random Forest Predictive Metrics", 
#       col.names = c('Class', 'Precision', 'Recall', 'F1-Score', 'Support'), 
#       align = 'c') %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
#                 full_width = F, font_size = 12)


```



For all of these analyses, the dataset was split into a training dataset of 80,000 and a testing dataset of 20,000. All predictive performance metrics were obtained through cross-validation on the testing dataset. A logistic regression analysis was conducted to predict turnover intention based on the set of FEVS predictors. From these predictors, 40 were randomly chosen to conduct the analyses. Overall accuracy across categories was `r metricsLOGI$Accuracy[3]`, as can be seen in Table \@ref(tab:logitable40). The weighted average precision was `r metricsLOGI$Precision[3]`, with a recall of `r metricsLOGI$Recall[3]` and an F1 score of `r metricsLOGI$F1.score[3]`, suggesting a moderate level of prediction consistency across categories. 
The performance of a xgboosting model across categories was `r metricsXGB$Accuracy[3]`, as can be seen in table \@ref(tab:xgbtable40). The weighted average precision was `r metricsXGB$Precision[4]`, with a recall of `r metricsXGB$Recall[4]` and an F1 score of `r metricsXGB$F1.score[4]`, suggesting moderate levels of predictive accuracy. 
The performance of a neural network model across categories was `r metricsNN$F1.score[3]`, as can be seen in table \@ref(tab:nn40). The weighted average precision was `r metricsNN$Precision[4]`, with a recall of `r metricsNN$Recall[4]` and an F1 score of `r metricsNN$F1.score[4]`, suggesting moderate levels of predictive accuracy. 
The performance of a SVM model across categories was `r metricsSVM$F1.score[3]`, as can be seen in table \@ref(tab:svm40). The weighted average precision was `r metricsSVM$Precision[4]`, with a recall of `r metricsSVM$Recall[4]` and an F1 score of `r metricsSVM$F1.score[4]`, suggesting moderate levels of predictive accuracy.
The performance of a random forest model across categories was `r metricsRF$F1.score[3]`, as can be seen in table \@ref(tab:rf40). The weighted average precision was `r metricsRF$Precision[4]`, with a recall of `r metricsRF$Recall[4]` and an F1 score of `r metricsRF$F1.score[4]`, suggesting moderate levels of predictive accuracy.


## Summary Results

```{r summarytable}
options(scipen = 999)
data_sum<- rbind(
  logi100k,
  logi10k,
  logi1k,
  logi100,
  logi75,
  logi40,
  xgb100k,
  xgb10k,
  xgb1k,
  xgb100,
  xgb75,
  xgb40,
  nn100k, 
  nn10k,
  nn1k,
  nn100,
  nn75,
  nn40,
  svm100k,
  svm10k,
  svm1k,
  svm100,
  svm75,
  svm40,
  rf100k, 
  rf10k,
  rf1k,
  rf100, 
  rf75,
  rf40
  
)

data_sum <- data_sum[order(data_sum$AUC.Score, decreasing = TRUE), ]
row.names(data_sum) <- NULL

apa_table(
  data_sum
  , caption = "AUC scores for all models"
  #, note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  , longtable = TRUE
  , landscape = FALSE
  , escape = FALSE
)


```

The AUC score for all combination of models, sample sizes and number of variables has been summarized in table \@ref(tab:summarytable). 


# Study 2

## Methods

## Procedure and Participants

A Qualtrics survey including  will be sent out via Amazon's MTurk. Participants will be sourced to create a sample that closely mirrors the diverse working population across various industries. Quality assurance measures, including attention check items, tracking response times, and synonymous/antonymous items will be used. The R careless package will be used to control for careless responding, with participants that are responding inconsistently in a significant way with the synonymous/antonymous items being excluded from the study. The Cloud Research platform will be used to to help ensure the quality of the data. The following MTurk and Cloud Research inclusion criteria will be used for the online survey: approved 95% HIT (task) rate on MTurk, 100+ HITs approved, geographical locations restricted to the United States, Cloud Research approved participants, duplicate IP addresses blocked, and suspicious geocodes blocked. Additionally, participants will be compensated for their participation. The target sample size is 500 respondents. 

## Data

### Work-related psychological constructs

The work-related psychological constructs will include the following FAVS measures: job satisfaction [@ertas2015turnover; @kim2017employee; @pitts2011so], organizational satisfaction [@liss2015loving], supervisory support [@maertz2007effects; @smith2005achieving], managerial trustworthiness [@ko2014impacts], employee empowerment [@kim2017employee; @pink2017examining; @pitts2011so], organizational support [@liss2015loving], and organizational procedural justice [@choi2011organizational].

Additionally, the following publicly available instruments will be added as independent variables: the 3-items Utrecht Work Engagement Scale (UWES-3) [@schaufeli2017ultra], the refined 9-items Lodahl and Kejner's Job Involvement Scale (JI) [@reeve2001refining], the 18-items revised Three-Component Model (TCM) Employee Commitment Survey [@meyer1991three], the 1-item Graphic Scale of Organizational Identification (GSOI) [@shamir2004single], the 8-item Survey of Perceived Organizational Support (SPOS) [@shore1991construct], and the 28-items Concise Conscientiousness Measure (CCM-S) [@franzen2022developing].       

### Biodata

When measuring biodata, the following FAVS measures will be used: gender [@ertas2015turnover ; @ko2014impacts; @wynen2014impact], minority status [@pink2017examining; @weaver2015intent], educational achievement [@joo2012model; @liss2015loving], supervisory position [@ertas2015turnover; @pink2017examining], organizational tenure [@lewis1992men; @pitts2011so; @wynen2014impact],
compensation [@wynen2014impact; @wynen2014impact), family-friendly policies [@durst1999assessing; @joo2012model], training and skill development [@curry2005training], diversity management [@choi2009diversity], promotion and advancement policy [@cotton1986employee; @johnson2000factors; @porter1974organizational], and coworker relations [@cotton1986employee]. 

## Data Analysis

Several machine learning algorithms will be used to train a series of models using biodata features only. These models includes GBT, RF, NN, SVM, and multiple regression. A second series of models that include work-related psychological constructs only in the data will be trained. Predictive accuracy metrics will be compared for each dataset to test hypothesis 3a. 
In addition, a baseline series of models will be trained using biodata features only. A second series of models that include work-related psychological constructs only in the data will be added to the models. Performance metrics of each model will be evaluated using accuracy, precision, recall, F1-scores, and AUC-ROC. Bootstrap resampling will be used as a statistical test to assess whether the improvement in performance when adding psychological constructs is statistically significant. An F-test will be used to determine if the change in R-squared is statistically significant for multiple regression. This is done to test for hypothesis 3b.

## Data Description

```{r study2descr}
data_descr_study1<-read.csv("descriptives_study2.csv")

apa_table(
  data_descr_study1
  , caption = "Descriptive Statistics"
  #, note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  , longtable = TRUE
  , landscape = FALSE
  , escape = FALSE
)

```

The dataset used in this study consists of 241 observations, each represented by 22 variables. These variables encompass a mixture of continuous and categorical data types. Turnover Intention is the target variable, measured as a binary indicator (0 or 1). Job satisfaction, measures by the MOAQ, is also a target variable, and unlike Turnover Intention it is a continuous Likert-scale ranging from "Strongly agree" to "Strongly disagree". The work-related psychological constructs predictor variables were also continuous Likert-scales ranging from "Strongly agree" to "Strongly disagree". Biodata variables were continuous and varied in scale. Summary descriptive statistics for each variable in the dataset can be found in table \@ref(tab:study2descr). 

# Results

## Predicting Turnover Intention

```{r biotable, message=FALSE, warning=FALSE}

data_bio<-read.csv("model_auc_scores.csv")

colnames(data_bio)<- c("Model", "AUC Score Biodata")

apa_table(
  data_bio
  , caption = "Biodata only model"
  
  #, note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  , longtable = TRUE
  , landscape = FALSE
  , escape = FALSE
)

```


```{r wrctable, message=FALSE, warning=FALSE}

data_wrc<-read.csv("addedWRC_model_auc_scores.csv")
colnames(data_wrc)<- c("Model", "AUC Score WRPC")

apa_table(
  data_wrc
  , caption = "Work-related psychological constructs only model"
  
  #, note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  , longtable = TRUE
  , landscape = FALSE
  , escape = FALSE
)

```


```{r deltas}
data_ALL<-read.csv("ALL_model_auc_scores.csv")
colnames(data_ALL)<- c("Model", "AUC Score when adding WRPC")
data_del<-left_join(data_bio, data_ALL, by="Model")

data_del$Delta <- data_del$`AUC Score when adding WRPC` - data_del$`AUC Score Biodata`

apa_table(
  data_del
  , caption = "Work-related psychological constructs added to the models"
  
  #, note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  , longtable = TRUE
  , landscape = FALSE
  , escape = FALSE
)

```



The objective of this study was to evaluate the impact of incorporating work-related psychological constructs into models trained with biodata features for predicting turnover intention. To this end, machine learning algorithms including Gradient Boosting Trees (GBT), Random Forest (RF), Neural Networks (NN) and Support Vector Machines (SVM) were employed. A logistic regression was also employed. Models were initially trained using only biodata features. Subsequently, a second series of models was developed, integrating work-related psychological constructs.

The analysis revealed a consistent enhancement in model performance upon the addition of work-related psychological constructs across all algorithms. Specifically, models incorporating work-related psychological constructs demonstrated a notable improvement in AUC-ROC scores (table \@ref(tab:wrctable)) compared to those based solely on biodata features (table \@ref(tab:biotable)). As can be seen in table \@ref(tab:deltas), the deltas between the AUC scores with Biodata and the AUC scores when adding WRPC to the models are on average `r mean(data_del$Delta)`.  

The results substantiate hypothesis 3a, demonstrating that models trained with both biodata features and work-related psychological constructs outperform those trained with biodata features alone. These findings underscore the value of incorporating psychological constructs into predictive models within the domain of work-related outcomes.

note: include deltas


## Predicting Job Satisfaction


```{r biotable2, message=FALSE, warning=FALSE}

data_bio2<-read.csv("base_model_rmse_scores.csv")

colnames(data_bio2)<- c("Model", "RMSE Biodata")

apa_table(
  data_bio2
  , caption = "Biodata only model"
  
  #, note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  , longtable = TRUE
  , landscape = FALSE
  , escape = FALSE
)

```


```{r wrctable2, message=FALSE, warning=FALSE}

data_wrc2<-read.csv("addedWRC_model_rmse_scores.csv")
colnames(data_wrc2)<- c("Model", "RMSE WRPC")

apa_table(
  data_wrc2
  , caption = "Work-related psychological constructs only model"
  
  #, note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  , longtable = TRUE
  , landscape = FALSE
  , escape = FALSE
)

```


```{r deltas2}
data_ALL2<-read.csv("ALL_model_rmse_scores.csv")
colnames(data_ALL2)<- c("Model", "RMSE when adding WRPC")
data_del2<-left_join(data_bio2, data_ALL2, by="Model")

data_del2$Delta <- data_del2$`RMSE when adding WRPC` - data_del2$`RMSE Biodata`

apa_table(
  data_del2
  , caption = "Work-related psychological constructs added to the models"
  
  #, note = "1= turnover intention, 0= no turnover intention."
  #, align = c("m{2cm}", "m{12cm}")
  , longtable = TRUE
  , landscape = FALSE
  , escape = FALSE
)

```


The analysis revealed a consistent enhancement in model performance upon the addition of work-related psychological constructs across all algorithms. Specifically, models incorporating work-related psychological constructs demonstrated a notable improvement in RMSE scores (table \@ref(tab:wrctable2)) compared to those based solely on biodata features (table \@ref(tab:biotable2)). As can be seen in table \@ref(tab:deltas2), the deltas between the RMSE scores with Biodata and the RMSE scores when adding WRPC to the models are on average `r mean(data_del2$Delta, na.rm=TRUE)`.  

The results substantiate hypothesis 3a, demonstrating that models trained with both biodata features and work-related psychological constructs outperform those trained with biodata features alone. These findings underscore the value of incorporating psychological constructs into predictive models within the domain of work-related outcomes.

# Discussion

The findings of the present study do not support the hypotheses that were based on the prior research by Sanchez (2018) and Kirasich et al. (2018). The hypotheses posited were centered around the performance of machine learning (ML) algorithms compared to regression models, with a specific focus on the effects of sample size and the number of variables on predictive accuracy. Contrary to expectations, the results from this study did not demonstrate a significant relative advantage in predictive accuracy for ML algorithms as sample size increased, nor did they confirm the hypothesized impact of the number of variables in datasets on the choice between ML and regression models.

Hypothesis 1 anticipated that as sample size increases, ML algorithms would exhibit a growing superiority over regression models in terms of predictive accuracy for turnover intentions. This hypothesis was predicated on the assertion from previous studies that ML algorithms tend to outperform regression models in larger datasets due to their ability to handle high-dimensional data and complex model interactions (Sanchez, 2018; Kirasich et al., 2018). However, the current study's findings suggest that the predictive accuracy of ML algorithms does not significantly outpace that of regression models as sample sizes increase. This discrepancy might be attributed to several factors including the nature of the data, the specific algorithms employed, or the methodologies used in the current study compared to those in the cited research.

Hypothesis 2 was based on the observation that ML algorithms perform less effectively in terms of predictive accuracy when the number of predictors in a dataset is limited, particularly when the events per variable (EPV) are below 200. This hypothesis also did not find support in the current study’s results, suggesting that the number of variables alone may not be a determinant factor in the efficacy of ML algorithms compared to regression models. This finding challenges the conclusions drawn by Sanchez (2018), where it was noted that regression models held a consistent or improved predictive accuracy under similar conditions. It is possible that advancements in ML techniques or variations in data preprocessing and feature engineering might have influenced the outcomes observed in the current study.

The absence of expected findings calls for a careful consideration of the conditions under which previous research asserted the superiority of ML over traditional statistical methods. It may be necessary to conduct further investigations into how specific characteristics of data, such as the structure and quality of the dataset, influence the performance of different analytical approaches. Additionally, the replication of this study across various contexts and with different types of data might help in understanding the nuanced dynamics between ML algorithms and regression models.

In conclusion, while the current study does not support the hypothesized advantages of ML algorithms over regression models with respect to sample size and the number of predictors, these results contribute to the ongoing dialogue and exploration into the most effective analytical techniques in predictive modeling. Future research should continue to explore these dynamics, perhaps focusing on different types of outcomes, varying conditions of data complexity, and the integration of novel machine learning methodologies.


As Richard Landers said in an interview with SIOP, "Let's blend I-O psychology's tried-and-true practices where we know what we're measuring and we're very confident in the kinds of recommendations we're giving, and let's figure out where the intersections are with some of the new stuff coming out, to figure out what is truly new and useful and what is just a faddish waste of time." (Landers, 2019).
Many of the new technological developments in the fields of data science and computers science are not backed by the expertise that I/O psychology has been developing over the years, yet they appear more attractive for many HR practices.
It is important that we blend with these communities and contribute with our knowledge, otherwise, as Landers puts it, "they're going to run away with the farm," (Landers, 2019).
A way in which the field of I/O psychology could contribute in the development of machine learning solutions for HR practices would be to build algorithms that use antecedents of job performance commonly studied by I/O psychology researchers as input variables.
These could be variables such as organizational commitment, organizational engagement, organizational identification, perceived organizational support, perceived organizational justice, etc.
Showing evidence of incremental predictive accuracy over algorithms built with other types of input would encourage better practices.
Although machine learning algorithms have the advantage that they can learn from the data and look at associations between variables that would need to be specified if a regression technique was used instead, they are also very data hungry.
This makes them viable for fortune 500 companies, for instance, but in most contexts there's the possibility that regression methods will produce the same predictive accuracy (or perhaps even better) without having to sacrifice parsimony and sparsity.
It is also important to be selective on the variables that are being fed to the algorithms, since using all and any data that gives good predictive performance could lead to the aforementioned GIGO issue ("garbage in, garbage out").
Input may be subjective (and/or biased), particularly if there are IVs such as "previous performance".
This GIGO issue might be exacerbated by peoples' awe and wonder of ML.
Using variables supported by previous research would be best practice in building these ML solutions.

\newpage

# References
```{r}
r_refs("r-references.bib", append=FALSE)
```

```{=tex}
\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

```
::: {#refs custom-style="Bibliography"}
:::

```{=tex}
\endgroup
```

\newpage

# Appendix